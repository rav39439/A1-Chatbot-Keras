# from autogen_ext.models.ollama import OllamaChatCompletionClient
# ollamamodel_client = OllamaChatCompletionClient(model="llama3.2")

# from autogen_agentchat.messages import TextMessage
# message = TextMessage(content="I'd like to go to London", source="user")
# from autogen_agentchat.messages import TextMessage, MultiModalMessage


# from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
# from autogen_core import CancellationToken

# from autogen_ext.models.ollama import OllamaChatCompletionClient
# from autogen_core.models import UserMessage
# from autogen_agentchat.conditions import  TextMentionTermination

# from langchain_community.utilities import GoogleSerperAPIWrapper
# from langchain_core.tools import Tool
# from autogen_ext.tools.langchain import LangChainToolAdapter
# from dotenv import load_dotenv
# from autogen_agentchat.teams import RoundRobinGroupChat
# import os
# load_dotenv(override=True)

# print("SERPER_API_KEY (raw):", os.getenv("SERPER_API_KEY"))

# os.environ["SERPER_API_KEY"]= os.getenv("SERPER_API_KEY")
# serper = GoogleSerperAPIWrapper()
# langchain_serper =Tool(name="internet_search", func=serper.run, description="useful for when you need to search the internet")
# autogen_serper = LangChainToolAdapter(langchain_serper)
# prompt = """Find a one-way non-stop flight from Bangalore to Mumbai in November 2025."""


# ollama_client = OllamaChatCompletionClient(
#         model="llama3.2",  # Specify the model you pulled with Ollama
# )

# primary_agent = AssistantAgent(
#     "primary",
#     model_client=ollama_client,
#     tools=[autogen_serper],
#     system_message="You are a helpful AI research assistant who looks for promising deals on flights. Incorporate any feedback you receive.",
# )

# evaluation_agent = AssistantAgent(
#     "evaluator",
#     model_client=ollama_client,
#     system_message="Provide constructive feedback. Respond with 'APPROVE' when your feedback is addressed.",
# )

# text_termination = TextMentionTermination("APPROVE")
#     # Example usage
# team = RoundRobinGroupChat([primary_agent, evaluation_agent], termination_condition=text_termination, max_turns=20)

#     # Run the async function (e.g., in an async context or with asyncio.run())

# # Agent 1: Parse ONTAP logs
# LogParserAgent = AssistantAgent(
#     name="LogParserAgent",
#     system_message=(
#         "You are an expert in parsing error logs. Extract relevant information from ONTAP log for classifying the defect and convert it into a single paragraph "
    
#     ),
#     model_client=ollama_client,
# )

# # Agent 2: Classify defect type
# DefectClassifierAgent = AssistantAgent(
#     name="DefectClassifierAgent",
#     system_message=(
#         "You are a defect classifier. Given parsed ONTAP log data, classify the "
#         "defect into one of: cluster-config defect, client-side defect, test-script-defect, System defect."
#         "If defect does not come under any of the above then use your own logic to classify"
#     ),
#     model_client=ollama_client,
# )

# # Agent 3: Summarize
# SummaryAgent = AssistantAgent(
#     name="SummaryAgent",
#     system_message=(
#         "You are a summarizer. Combine parsed log data and defect classification "
#         "into a concise, readable report for the user."
#     ),
#     model_client=ollama_client,

# )

# # User proxy
# user = UserProxyAgent(name="User")

# # --------------------------
# # Message Passing Workflow
# # --------------------------
# async def process_log(log_text):
#     # Step 1: LogParserAgent parses
#     # multi_modal_message = MultiModalMessage(content=[f"Parse this ONTAP log:\n{log_text}"], source="User")
#     # response = await LogParserAgent.on_messages([multi_modal_message], cancellation_token=CancellationToken())
#     # reply = response.chat_message.content

#     # print(reply)  # type: ignore
#     # multi_modal_message = MultiModalMessage(content=[f"Classify this defect:\n{reply}"], source="User")

#     # # Step 2: DefectClassifierAgent classifies
#     # classified = await DefectClassifierAgent.on_messages([multi_modal_message], cancellation_token=CancellationToken())
#     # resultr = classified.chat_message.content

#     # # Step 3: SummaryAgent summarizes
#     # multi_modal_message = MultiModalMessage(content=[f"Summarize this:\nParsed Info: {reply}\nClassification: {resultr}"], source="User")

#     # summarized = await SummaryAgent.on_messages([multi_modal_message], cancellation_token=CancellationToken())
#     # info = summarized.chat_message.content
#     result = await team.run(task=prompt)
#     for message in result.messages:
#        print(f"{message.source}:\n{message.content}\n\n")

    

#     # Step 4: Send to user
# # Example usage:
# ontap_log = """
# [2025-11-04 10:45:21] INFO  :: NATE::TestManager -> Initializing test session for ONTAP cluster 10.25.44.16
# [2025-11-04 10:45:21] DEBUG :: Loading test configuration from /etc/nate/config/ontap_cluster.conf
# [2025-11-04 10:45:21] INFO  :: Connecting to ONTAP management LIF 10.25.44.16 via HTTPS (port 443)
# [2025-11-04 10:45:22] DEBUG :: Using Perl SDK module NaServer.pm for API calls
# [2025-11-04 10:45:22] INFO  :: Preparing test harness for test cases: aggregate_resize_validation, qtree_clone_validation
# [2025-11-04 10:45:22] INFO  :: Starting test case 'aggregate_resize_validation'
# [2025-11-04 10:45:22] DEBUG :: Constructing XML request for aggregate_resize_validation
# <netapp xmlns="http://www.netapp.com/filer/admin" version="1.140">
#    <aggr-resize></aggr-resize>
# </netapp>
# [2025-11-04 10:45:22] ERROR :: NATE::TestManager -> Test script error: Missing mandatory parameter 'target_aggregate' in 'aggregate_resize_validation'
# [2025-11-04 10:45:23] WARN  :: Execution halted for 'aggregate_resize_validation' due to missing input parameter
# [2025-11-04 10:45:23] INFO  :: Logging defect: test-script-defect, subtype: missing-parameter
# [2025-11-04 10:45:23] DEBUG :: Skipping dependent subtests as aggregate_resize_validation failed
# [2025-11-04 10:45:24] INFO  :: Starting subtest 'qtree_clone_validation'
# [2025-11-04 10:45:24] DEBUG :: Validating test configuration for qtree_clone_validation
# [2025-11-04 10:45:25] ERROR :: NATE::TestManager -> Subtest failed: Missing 'source_qtree' parameter in test script
# [2025-11-04 10:45:25] WARN  :: Subtest 'qtree_clone_validation' skipped due to configuration error
# [2025-11-04 10:45:25] INFO  :: Logging defect: test-script-defect, subtype: configuration-error
# [2025-11-04 10:45:26] DEBUG :: Capturing XML request for qtree_clone_validation
# <netapp xmlns="http://www.netapp.com/filer/admin" version="1.140">
#    <qtree-clone-create></qtree-clone-create>
# </netapp>
# [2025-11-04 10:45:26] DEBUG :: XML Response indicates test skipped: status="failed" reason="Configuration error"
# [2025-11-04 10:45:27] INFO  :: Test session summary:
#    Total Test Cases: 2
#    Passed: 0
#    Failed: 2
#    Skipped: 1
# [2025-11-04 10:45:27] INFO  :: Defects recorded:
#    1. aggregate_resize_validation - Missing mandatory parameter 'target_aggregate'
#    2. qtree_clone_validation - Configuration error (missing 'source_qtree' parameter)
# [2025-11-04 10:45:28] INFO  :: NATE Test session completed successfully
# [2025-11-04 10:45:28] INFO  :: Session log saved to /var/log/nate/session_20251104_testscript_2.log
# """

# import asyncio
# asyncio.run(process_log(ontap_log))

#---------------------------------playwright and beautiful soup-------------------

from autogen_ext.models.ollama import OllamaChatCompletionClient
ollamamodel_client = OllamaChatCompletionClient(model="llama3.2")

from autogen_agentchat.messages import TextMessage
message = TextMessage(content="I'd like to go to London", source="user")
from autogen_agentchat.messages import TextMessage, MultiModalMessage


from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_core import CancellationToken

from autogen_ext.models.ollama import OllamaChatCompletionClient
from autogen_core.models import UserMessage
from autogen_agentchat.conditions import  TextMentionTermination

from langchain_community.utilities import GoogleSerperAPIWrapper
from langchain_core.tools import Tool
from autogen_ext.tools.langchain import LangChainToolAdapter
from dotenv import load_dotenv
from autogen_agentchat.teams import RoundRobinGroupChat
import os
load_dotenv(override=True)

from playwright.sync_api import sync_playwright
# from langchain.agents import tool
from langchain_core.tools import Tool



# ollama_client = OllamaChatCompletionClient(
#         model="llama3.2",  # Specify the model you pulled with Ollama
# )
# def navigate_and_extract(url: str, selector: str = None) -> str:
#     """Fetch page content or text of an element without showing the browser."""
#     with sync_playwright() as p:
#         browser = p.chromium.launch(headless=True)  # headless=True hides the browser
#         page = browser.new_page()
#         page.goto(url, wait_until="domcontentloaded", timeout=15000)

#         if selector:
#             try:
#                 content = page.locator(selector).inner_text()
#             except Exception:
#                 content = f"Selector '{selector}' not found."
#         else:
#             content = page.inner_text()

#         browser.close()
#         return content

import requests
from bs4 import BeautifulSoup

def navigate_and_extract(url:str):
    html = requests.get(url, timeout=10).text
    soup = BeautifulSoup(html, "html.parser")
    return soup.get_text()

isearch =Tool(name="internet_search", func=navigate_and_extract, description="useful for when you need to search the internet")
isearchtool = LangChainToolAdapter(isearch)



# agen = AssistantAgent(
#     name="agent",
#     system_message=(
#         "You are an expert in parsing error logs. Extract relevant information from ONTAP log for classifying the defect and convert it into a single paragraph "
    
#     ),
#     model_client=ollamamodel_client,
#     tools=[isearchtool]
# )

# result = agen.run("Get the title of https://example.com")
# print(result)

agent = AssistantAgent(
    name="agent",
    system_message=(
        "You are an expert in parsing error logs. Extract relevant information "
        "from ONTAP log for classifying the defect and convert it into a single paragraph."
    ),
    model_client=ollamamodel_client,
    tools=[isearchtool]
)

# Now call run() on the instance
import asyncio

# ---- your agent setup ----
# agent = AssistantAgent(...)

async def run_agent_task():
    """Run an async agent call properly."""
    result = await agent.run(task="Extract the content of https://getbootstrap.com/docs/5.3/getting-started/introduction/")
    print(result)
    return result

if __name__ == "__main__":
    
    result = asyncio.run(run_agent_task())

# If result contains messages (like in your log)
    if hasattr(result, "messages"):
        # Get the last message
        final_message = result.messages[-1]
        print("\nFinal message type:", final_message.type)
        print("Final content:\n", final_message.content)
    else:
        # Some versions return content directly
        print(result)

import inspect
print(inspect.iscoroutinefunction(agent.run))
#------------------------------------------------------------------------------------------
#-----------------------------interacting agents----------------


import asyncio
from dataclasses import dataclass
from typing import List

from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler
from autogen_core.models import SystemMessage, UserMessage
# from autogen_ext.models.openai import OpenAIChatCompletionClient

from autogen_ext.models.ollama import OllamaChatCompletionClient
ollamamodel_client = OllamaChatCompletionClient(model="llama3.2")

@dataclass
class WorkerTask:
    task: str
    previous_results: List[str]


@dataclass
class WorkerTaskResult:
    result: str


@dataclass
class UserTask:
    task: str


@dataclass
class FinalResult:
    result: str


class WorkerAgent(RoutedAgent):
    def __init__(
        self,
        model_client: OllamaChatCompletionClient,
    ) -> None:
        super().__init__(description="Worker Agent")
        self._model_client = ollamamodel_client

    @message_handler
    async def handle_task(self, message: WorkerTask, ctx: MessageContext) -> WorkerTaskResult:
        if message.previous_results:
            # If previous results are provided, we need to synthesize them to create a single prompt.
            system_prompt = "You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\n\nResponses from models:"
            system_prompt += "\n" + "\n\n".join([f"{i+1}. {r}" for i, r in enumerate(message.previous_results)])
            model_result = await self._model_client.create(
                [SystemMessage(content=system_prompt), UserMessage(content=message.task, source="user")]
            )
        else:
            # If no previous results are provided, we can simply pass the user query to the model.
            model_result = await self._model_client.create([UserMessage(content=message.task, source="user")])
        assert isinstance(model_result.content, str)
        print(f"{'-'*80}\nWorker-{self.id}:\n{model_result.content}")
        return WorkerTaskResult(result=model_result.content)


class OrchestratorAgent(RoutedAgent):
    def __init__(
        self,
        model_client: OllamaChatCompletionClient,
        worker_agent_types: List[str],
        num_layers: int,
    ) -> None:
        super().__init__(description="Aggregator Agent")
        self._model_client = ollamamodel_client
        self._worker_agent_types = worker_agent_types
        self._num_layers = num_layers

    @message_handler
    async def handle_task(self, message: UserTask, ctx: MessageContext) -> FinalResult:
        print(f"{'-'*80}\nOrchestrator-{self.id}:\nReceived task: {message.task}")
        # Create task for the first layer.
        worker_task = WorkerTask(task=message.task, previous_results=[])
        # Iterate over layers.
        for i in range(self._num_layers - 1):
            # Assign workers for this layer.
            worker_ids = [
                AgentId(worker_type, f"{self.id.key}/layer_{i}/worker_{j}")
                for j, worker_type in enumerate(self._worker_agent_types)
            ]
            # Dispatch tasks to workers.
            print(f"{'-'*80}\nOrchestrator-{self.id}:\nDispatch to workers at layer {i}")
            results = await asyncio.gather(*[self.send_message(worker_task, worker_id) for worker_id in worker_ids])
            print(f"{'-'*80}\nOrchestrator-{self.id}:\nReceived results from workers at layer {i}")
            # Prepare task for the next layer.
            worker_task = WorkerTask(task=message.task, previous_results=[r.result for r in results])
        # Perform final aggregation.
        print(f"{'-'*80}\nOrchestrator-{self.id}:\nPerforming final aggregation")
        system_prompt = "You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\n\nResponses from models:"
        system_prompt += "\n" + "\n\n".join([f"{i+1}. {r}" for i, r in enumerate(worker_task.previous_results)])
        model_result = await self._model_client.create(
            [SystemMessage(content=system_prompt), UserMessage(content=message.task, source="user")]
        )
        assert isinstance(model_result.content, str)
        return FinalResult(result=model_result.content)



task = (
    "Ravi borrows $5,000 from a bank at an annual simple interest rate of 8%. How much interest will he have to pay after 3 years?"
)

# runtime = SingleThreadedAgentRuntime()
# model_client = OpenAIChatCompletionClient(model="llama3.2")
# await WorkerAgent.register(runtime, "worker", lambda: WorkerAgent(model_client=ollamamodel_client))
# await OrchestratorAgent.register(
#     runtime,
#     "orchestrator",
#     lambda: OrchestratorAgent(model_client=ollamamodel_client, worker_agent_types=["worker"] * 3, num_layers=3),
# )

# runtime.start()
# result = await runtime.send_message(UserTask(task=task), AgentId("orchestrator", "default"))

# await runtime.stop_when_idle()
# await model_client.close()

# print(f"{'-'*80}\nFinal result:\n{result.result}")


import asyncio
# from autogen_core import SingleThreadedAgentRuntime, AgentId, UserTask
# from ollama import Ollama  # your model client

# Assuming WorkerAgent and OrchestratorAgent are already defined and compatible with asyncio

async def main(task: str):
    # Create runtime
    runtime = SingleThreadedAgentRuntime()

    # Initialize model client

    # Register worker agents
    await WorkerAgent.register(runtime, "worker", lambda: WorkerAgent(model_client=ollamamodel_client))

    # Register orchestrator agent
    await OrchestratorAgent.register(
        runtime,
        "orchestrator",
        lambda: OrchestratorAgent(
            model_client=ollamamodel_client, 
            worker_agent_types=["worker"] * 3, 
            num_layers=3
        ),
    )

    # Start the runtime
    runtime.start()

    # Send user task to orchestrator agent
    result = await runtime.send_message(
        UserTask(task=task),
        AgentId("orchestrator", "default")
    )

    # Stop the runtime once idle
    await runtime.stop_when_idle()

    # Close the model client if it has an async close method
    if hasattr(model_client, "close"):
        await model_client.close()

    print(f"{'-'*80}\nFinal result:\n{result.result}")


if __name__ == "__main__":
    taskt = task
    asyncio.run(main(taskt))

