

# #--------------------------------third way------------------------------------------------------
#
#
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# import pandas as pd
# from collections import defaultdict
# import string
# import tensorflow as tf
# import re
# import os
# import time
# from tensorflow import keras
# from tensorflow.keras.layers import Dense, Input
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.models import Model
# from tensorflow.keras.callbacks import ModelCheckpoint
# from sklearn.preprocessing import OneHotEncoder
# from sklearn.model_selection import train_test_split
# ENCODER_LEN = 200
# DECODER_LEN = 100
# BATCH_SIZE = 64
# BUFFER_SIZE = BATCH_SIZE*8
# # news = pd.read_csv("samples1.csv")
# #----------------------------------one way--------------------------------------------------
# # article = news['Question']
# # summary = news['Answer']
# # # news_subset = news.head(20)
# # #
# # # # Access the 'Headline' and 'Content' columns from the subset
# # # article = news_subset['Headline']
# # # summary = news_subset['Content']
# # article = article.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')
# # summary = summary.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')
# # # def preprocess(text):
# # #     text = re.sub(r"&.[1-9]+;₹"," ",text)
# # #     return text
# #
# # def preprocess(text):
# #     # Remove HTML entities like "&1234;", and the symbol ₹
# #     text = re.sub(r"&[1-9]+;|₹", " ", text)  # Updated regex for both patterns
# #     # Remove single quotes (')
# #     # text = re.sub(r"'", "", text)
# #     text = re.sub(r"'", "", text)
# #     # Remove single double quotes (") within words
# #     text = re.sub(r'(\w)"(\w)', r'\1\2', text)
# #     return text
# #
# # # def preprocess_text(text):
# # #     # Remove special characters (e.g., ₹) from words
# # #     return re.sub(r"[₹]", "", text)
# #
# # article = article.apply(lambda x: preprocess(x))
# # summary = summary.apply(lambda x: preprocess(x))
# #
# #
# #
# #
# # filters = '₹!"#$%&()*+,-./:;=?@[\\]^_`{|}~\t\n'
# # oov_token = '<unk>'
# # article_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)
# # summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)
#
#
# #--------------------------------------proccessing numbers-----------------------------------
#
#
# news = pd.read_csv("average_samples_final_rounded_updated.csv")  # Replace with your file path
# article = news['Input']
# summary = news['Output']
#
# # Add SOS and EOS tokens
# article = article.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')
# summary = summary.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')
# #
# #
# # # Preprocessing function to separate numbers and operators
# # def preprocess(text):
# #     # Add spaces around operators for proper tokenization
# #     text = re.sub(r"(\d+)([+])(\d+)", r"\1 \2 \3", text)  # e.g., "12+14" → "12 + 14"
# #     return text
#
#
# def preprocess(text):
#     # Tokenize numbers and separate them into digits
#     text = re.sub(r'(\d+)', lambda match: ' '.join(list(match.group(0))), text)  # e.g., "64" → "6 4"
#
#     # Add spaces around operators for proper tokenization
#     text = re.sub(r"(\+|\-|\*|/|=)", r" \1 ", text)  # Add spaces around operators
#
#     # Tokenize spaces between words
#     text = re.sub(r'\s+', ' ', text).strip()  # Normalize spaces
#
#     # Make text lowercase
#     text = text.lower()
#
#     return text
# # Apply preprocessing
# article = article.apply(preprocess)
# summary = summary.apply(preprocess)
# filters = '₹!"#$%&()-.:;?@[\\]^_`{|}~\t\n,'  # Ensure ',' is inside the quotes
#
# # filters = '₹!"#$%&(),-./:;=?@[\\]^_`{|}~\t\n'
#
# oov_token = '<unk>'
#
# article_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)
# summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)
#
# # Fit tokenizers on preprocessed data
# article_tokenizer.fit_on_texts(article)
# summary_tokenizer.fit_on_texts(summary)
# if ',' not in article_tokenizer.word_index:
#     article_tokenizer.word_index[','] = len(article_tokenizer.word_index) + 1
#
# if ',' not in summary_tokenizer.word_index:
#     summary_tokenizer.word_index[','] = len(summary_tokenizer.word_index) + 1
# # Convert text to sequences
# inputs = article_tokenizer.texts_to_sequences(article)
# targets = summary_tokenizer.texts_to_sequences(summary)
# # print(inputs)
#
# # Vocabulary sizes
# ENCODER_VOCAB = len(article_tokenizer.word_index) + 1
# DECODER_VOCAB = len(summary_tokenizer.word_index) + 1
#
# # Pad sequences
# ENCODER_LEN = 70  # Maximum length of input sequences
# DECODER_LEN = 40   # Maximum length of output sequences
#
# inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')
# targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')
# input_texts_padded = [' '.join([article_tokenizer.index_word.get(token, '') for token in seq]) for seq in inputs]
# target_texts_padded = [' '.join([summary_tokenizer.index_word.get(token, '') for token in seq]) for seq in targets]
#
# # Print tokenizers' word index
# print("Article Tokenizer Word Index (Word to Token):")
# print(article_tokenizer.word_index)
# print("\nSummary Tokenizer Word Index (Word to Token):")
# print(summary_tokenizer.word_index)
# word_43 = article_tokenizer.index_word.get(43, "Not found")
# word_35 = article_tokenizer.index_word.get(35, "Not found")
# word_72 = summary_tokenizer.index_word.get(35, "Not found")
# inputs = tf.cast(inputs, dtype=tf.int64)
# targets = tf.cast(targets, dtype=tf.int64)
# dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
#
#
# #-------------------------------------------------------------------------------------------
#
#
# #--------------------------transformer---------------------------------------
#
#
# def get_angles(position, i, d_model):
#     angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
#     return position * angle_rates
#
# def positional_encoding(position, d_model):
#     angle_rads = get_angles(
#         np.arange(position)[:, np.newaxis],
#         np.arange(d_model)[np.newaxis, :],
#         d_model
#     )
#
#     angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
#
#     angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
#
#     pos_encoding = angle_rads[np.newaxis, ...]
#
#     return tf.cast(pos_encoding, dtype=tf.float32)
#
# def create_padding_mask(seq):
#     seq = tf.cast(tf.math.equal(seq, 0), tf.float32)
#     return seq[:, tf.newaxis, tf.newaxis, :]
#
# def create_look_ahead_mask(size):
#     mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
#     return mask
#
# def scaled_dot_product_attention(q, k, v, mask):
#     matmul_qk = tf.matmul(q, k, transpose_b=True)
#
#     dk = tf.cast(tf.shape(k)[-1], tf.float32)
#     scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
#     if mask is not None:
#         scaled_attention_logits += (mask * -1e9)
#
#     attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
#
#     output = tf.matmul(attention_weights, v)
#     return output, attention_weights
#
#
# class MultiHeadAttention(tf.keras.layers.Layer):
#     def __init__(self, d_model, num_heads):
#         super(MultiHeadAttention, self).__init__()
#         self.num_heads = num_heads
#         self.d_model = d_model
#
#         assert d_model % self.num_heads == 0
#
#         self.depth = d_model // self.num_heads
#
#         self.wq = tf.keras.layers.Dense(d_model)
#         self.wk = tf.keras.layers.Dense(d_model)
#         self.wv = tf.keras.layers.Dense(d_model)
#
#         self.dense = tf.keras.layers.Dense(d_model)
#
#     def split_heads(self, x, batch_size):
#         x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
#         return tf.transpose(x, perm=[0, 2, 1, 3])
#
#     def call(self, v, k, q, mask):
#         batch_size = tf.shape(q)[0]
#
#         q = self.wq(q)
#         k = self.wk(k)
#         v = self.wv(v)
#
#         q = self.split_heads(q, batch_size)
#         k = self.split_heads(k, batch_size)
#         v = self.split_heads(v, batch_size)
#
#         scaled_attention, attention_weights = scaled_dot_product_attention(
#             q, k, v, mask)
#
#         # tf.print("Attention weights matrix structure (example 1, head 1):", attention_weights[2,3], summarize=-1)
#
#         scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])
#
#         concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))
#         output = self.dense(concat_attention)
#         # print('output')
#         # print(output)
#         # tf.print("ouuput matrix structure (example 1, head 1):", output)
#
#
#         return output, attention_weights
#
#
# def point_wise_feed_forward_network(d_model, dff):
#     return tf.keras.Sequential([
#         tf.keras.layers.Dense(dff, activation='relu'),
#         tf.keras.layers.Dense(d_model)
#     ])
#
#
# class EncoderLayer(tf.keras.layers.Layer):
#     def __init__(self, d_model, num_heads, dff, rate=0.1):
#         super(EncoderLayer, self).__init__()
#
#         self.mha = MultiHeadAttention(d_model, num_heads)
#         self.ffn = point_wise_feed_forward_network(d_model, dff)
#
#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#         self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#
#         self.dropout1 = tf.keras.layers.Dropout(rate)
#         self.dropout2 = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, training, mask):
#         attn_output, _ = self.mha(x, x, x, mask)
#         attn_output = self.dropout1(attn_output, training=training)
#         out1 = self.layernorm1(x + attn_output)
#
#         ffn_output = self.ffn(out1)
#         ffn_output = self.dropout2(ffn_output, training=training)
#         out2 = self.layernorm2(out1 + ffn_output)
#
#         return out2
#
#
# class DecoderLayer(tf.keras.layers.Layer):
#     def __init__(self, d_model, num_heads, dff, rate=0.1):
#         super(DecoderLayer, self).__init__()
#
#         self.mha1 = MultiHeadAttention(d_model, num_heads)
#         self.mha2 = MultiHeadAttention(d_model, num_heads)
#
#         self.ffn = point_wise_feed_forward_network(d_model, dff)
#
#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#         self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#         self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#
#         self.dropout1 = tf.keras.layers.Dropout(rate)
#         self.dropout2 = tf.keras.layers.Dropout(rate)
#         self.dropout3 = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
#         attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)
#         attn1 = self.dropout1(attn1, training=training)
#         out1 = self.layernorm1(attn1 + x)
#
#         attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)
#         attn2 = self.dropout2(attn2, training=training)
#         out2 = self.layernorm2(attn2 + out1)
#
#         ffn_output = self.ffn(out2)
#         ffn_output = self.dropout3(ffn_output, training=training)
#         out3 = self.layernorm3(ffn_output + out2)
#
#         return out3, attn_weights_block1, attn_weights_block2
#
#
# class Encoder(tf.keras.layers.Layer):
#     def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):
#         super(Encoder, self).__init__()
#
#         self.d_model = d_model
#         self.num_layers = num_layers
#
#         self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
#         self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)
#
#         self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]
#
#         self.dropout = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, training, mask):
#         seq_len = tf.shape(x)[1]
#
#         x = self.embedding(x)
#         x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
#         x += self.pos_encoding[:, :seq_len, :]
#
#         x = self.dropout(x, training=training)
#
#         for i in range(self.num_layers):
#             x = self.enc_layers[i](x, training=training, mask=mask)
#
#         return x
#
#
# class Decoder(tf.keras.layers.Layer):
#
#     def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):
#         super(Decoder, self).__init__()
#
#         self.d_model = d_model
#         self.num_layers = num_layers
#
#         self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)
#         self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)
#
#         self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]
#         self.dropout = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
#         seq_len = tf.shape(x)[1]
#         attention_weights = {}
#
#         x = self.embedding(x)
#         x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
#         x += self.pos_encoding[:, :seq_len, :]
#
#         x = self.dropout(x, training=training)
#
#         for i in range(self.num_layers):
#             x, block1, block2 = self.dec_layers[i](x, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)
#
#             attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1
#             attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2
#
#         return x, attention_weights
#
# class Transformer(tf.keras.Model):
#     def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input,
#                      pe_target, rate=0.1):
#         super(Transformer, self).__init__()
#
#         self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)
#
#         self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)
#
#         self.final_layer = tf.keras.layers.Dense(target_vocab_size)
#
#     def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):
#         enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)
#
#         dec_output, attention_weights = self.decoder(tar, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)
#
#         final_output = self.final_layer(dec_output)
#
#         return final_output, attention_weights
#
# # num_layers = 6
# # d_model = 128
# # dff = 512
# # num_heads = 8
# # dropout_rate = 0.3
# # EPOCHS = 90
#
# num_layers = 3
# # d_model = 128
# d_model=128
# dff = 256
# num_heads = 4
# dropout_rate = 0.4
# EPOCHS = 10
#
# class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
#     def __init__(self, d_model, warmup_steps=4000):
#         super(CustomSchedule, self).__init__()
#
#         self.d_model = d_model
#         self.d_model = tf.cast(self.d_model, tf.float32)
#
#         self.warmup_steps = warmup_steps
#
#
#     def __call__(self, step):
#         step = tf.cast(step, dtype=tf.float32)
#         arg1 = tf.math.rsqrt(step)
#         arg2 = step * (self.warmup_steps ** -1.5)
#
#         return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)
#
#
# learning_rate = CustomSchedule(d_model)
# # learning_rate=.01
#
# optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)
# temp_learning_rate_schedule = CustomSchedule(d_model)
#
# plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))
# plt.ylabel("Learning Rate")
# plt.xlabel("Train Step")
#
#
# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
# def loss_function(real, pred):
#     mask = tf.math.logical_not(tf.math.equal(real, 0))
#     loss_ = loss_object(real, pred)
#
#     mask = tf.cast(mask, dtype=loss_.dtype)
#     loss_ *= mask
#
#     return tf.reduce_sum(loss_)/tf.reduce_sum(mask)
#
#
# def accuracy_function(real, pred):
#     accuracies = tf.equal(real, tf.argmax(pred, axis=2))
#     accuracies = tf.cast(accuracies, dtype= tf.float32)
#     mask = tf.math.logical_not(tf.math.equal(real, 0))
#     # accuracies = tf.math.logical_and(mask, accuracies)
#     accuracies = tf.cast(accuracies, dtype=tf.float32)
#     mask = tf.cast(mask, dtype=tf.float32)
#     return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)
#
# train_loss = tf.keras.metrics.Mean(name='train_loss')
# train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')
#
#
#
# transformer = Transformer(
#     num_layers=num_layers,
#     d_model=d_model,
#     num_heads=num_heads,
#     dff=dff,
#     input_vocab_size=ENCODER_VOCAB,
#     target_vocab_size=DECODER_VOCAB,
#     pe_input=1000,
#     pe_target=1000,
#     rate=dropout_rate)
#
#
# def create_masks(inp, tar):
#     enc_padding_mask = create_padding_mask(inp)
#     dec_padding_mask = create_padding_mask(inp)
#
#     look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])
#     dec_target_padding_mask = create_padding_mask(tar)
#     combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)
#
#     return enc_padding_mask, combined_mask, dec_padding_mask
#
# # checkpoint_path = "/content/drive/MyDrive/Dataset/checkpoints"
# checkpoint_path = "checkpoints"
# ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)
# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)
#
# if ckpt_manager.latest_checkpoint:
#     ckpt.restore(ckpt_manager.latest_checkpoint)
#     for i, layer in enumerate(transformer.layers):
#         print(f"Layer {i}: Name = {layer.name}, Trainable = {layer.trainable}, Type = {type(layer)}")
#     print ('Latest checkpoint restored!!')
#
# @tf.function
# def train_step(inp, tar):
#     tar_inp = tar[:, :-1]
#     tar_real = tar[:, 1:]
#
#     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)
#
#     with tf.GradientTape() as tape:
#         predictions, _ = transformer(
#             inp,
#             tar_inp,
#             training=True,
#             enc_padding_mask=enc_padding_mask,
#             look_ahead_mask=combined_mask,
#             dec_padding_mask=dec_padding_mask
#         )
#         loss = loss_function(tar_real, predictions)
#
#     gradients = tape.gradient(loss, transformer.trainable_variables)
#     optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
#     train_loss(loss)
#     train_accuracy(accuracy_function(tar_real, predictions))
#
#
# for epoch in range(EPOCHS):
#     start = time.time()
#
#     train_loss.reset_states()
#
#     for (batch, (inp, tar)) in enumerate(dataset):
#         train_step(inp, tar)
#
#         if batch % 100 == 0:
#             print(
#                 f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')
#
#     if (epoch + 1) % 5 == 0:
#         ckpt_save_path = ckpt_manager.save()
#         print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))
#
#     print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')
#     print('Time taken for 1 epoch: {} secs\n'.format(time.time() - start))
# # creating embedding solving the problem of unseen number token
#
# # class NumericEmbedding(nn.Module):
# #     def __init__(self, embedding_dim=32):
# #         super().__init__()
# #         self.embedding = nn.Embedding(10, embedding_dim)  # 0-9 digits
# #         self.linear = nn.Linear(embedding_dim * 5, embedding_dim)  # 5-digit numbers
# #
# #     def forward(self, number):
# #         digits = [int(d) for d in str(number).zfill(5)]  # Pad number to 5 digits
# #         embeddings = self.embedding(torch.tensor(digits))  # Get embeddings
# #         return self.linear(embeddings.view(1, -1))  # Flatten & process
# #
# # num_encoder = NumericEmbedding()
# # print(num_encoder(34))  # Outputs a co
#
# def evaluate(input_article):
#     # Tokenize and pad the input article
#     print(input_article)
#     input_article = re.sub(r'(\d)', r'\1 ', input_article)  # Adds space after each digit
#     input_article = input_article.replace("  ", " ").strip()
#
#     input_article = tf.convert_to_tensor(input_article, dtype=tf.string)  # Ensure it's a TensorFlow string
#     input_article = input_article.numpy().decode("utf-8")
#         # inputs=inputArr.apply(preprocess(input_article))
#         # inputs = input_series.apply(lambda x: preprocess(x).split())
#         #
#     input_article = article_tokenizer.texts_to_sequences([input_article])
#
#
#     # inputs=input_article.apply(preprocess_and_split_numbers(input_article))
#
#     # input_article = article_tokenizer.texts_to_sequences([inputs])
#     # input_article=[[3,4,5,6,7,8,9,10,14,15,18,19,11,15,16,17,12]]
#     # input_article = [[2,3,4,9,10,5,13,14,6]]
#     print("tokens")
#     print(input_article)
#
#     input_article = tf.keras.preprocessing.sequence.pad_sequences(
#         input_article, maxlen=ENCODER_LEN, padding='post', truncating='post'
#     )
#     print("Inputs")
#     print(input_article)
#     encoder_input = tf.expand_dims(input_article[0], 0)
#     print (encoder_input)
#     decoder_input = [summary_tokenizer.word_index['<sos>']]
#     output = tf.expand_dims(decoder_input, 0)
#
#     for i in range(DECODER_LEN):
#         # Create masks
#         enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)
#
#         # Get predictions from the transformer
#         predictions, attention_weights = transformer(
#             encoder_input,
#             output,
#             training=False,
#             enc_padding_mask=enc_padding_mask,
#             look_ahead_mask=combined_mask,
#             dec_padding_mask=dec_padding_mask
#         )
#         predictions = predictions[:, -1:, :]  # Take the last predicted token
#         predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)
#         if predicted_id == summary_tokenizer.word_index['<eos>']:
#             return tf.squeeze(output, axis=0), attention_weights
#         # Append the predicted token to the output
#         output = tf.concat([output, predicted_id], axis=-1)
#     return tf.squeeze(output, axis=0), attention_weights
#
# def summarize(input_article):
#     summarized = evaluate(input_article=input_article)[0].numpy()
#     summarized = np.expand_dims(summarized[1:], 0)  # Remove the <sos> token
#     return summary_tokenizer.sequences_to_texts(summarized)[0]
#
# # Test the function
# print('Output:')
# print(summarize(
#     '<SOS> Calculate the average of 565 and 408 and 673 <EOS>'))
# def preprocess_and_split_numbers(input_text):
#     # Split numbers into their individual digits
#     # This regular expression finds numbers and splits them into individual characters
#     input_text = re.sub(r'(\d)', r' \1 ', input_text)  # Add space around each digit
#     return input_text



#------------------------------------embedding  -----========================================

# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# import pandas as pd
# from collections import defaultdict
# import string
# import tensorflow as tf
# import re
# import os
# import time
# from tensorflow import keras
# from tensorflow.keras.layers import Dense, Input
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.models import Model
# from tensorflow.keras.callbacks import ModelCheckpoint
# from sklearn.preprocessing import OneHotEncoder
# from sklearn.model_selection import train_test_split
#
# # ENCODER_LEN = 200
# # DECODER_LEN = 70
# # BATCH_SIZE = 36
# # BUFFER_SIZE = BATCH_SIZE*8
# ENCODER_LEN = 200
# DECODER_LEN = 100
# BATCH_SIZE = 64
# BUFFER_SIZE = BATCH_SIZE*8
# # news = pd.read_csv("samples1.csv")
# # #----------------------------------one way--------------------------------------------------
# #
#
# news = pd.read_csv("modified_dataset_5000_samples.csv")  # Replace with yfiltered_positive_average_samplesour file path
# article = news['Input']
# summary = news['Output']
#
# # Add SOS and EOS tokens
#
# #---------------------------------------invalid for addition-----------------------------
# article = article.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')
# summary = summary.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')
#
#
# def preprocess(text):
#     # Add spaces around operators for correct tokenization
#     text = re.sub(r"(\d+|\+|\-|\*|/|=)", r" \1 ", text)
#     text = re.sub(r'\s+', ' ', text).strip()  # Normalize spaces
#     return text
#
# # Example usage:
# # text = "90+88"
# # tokens = preprocess(text).split()  # Tokenize by splitting on spaces
# # Apply preprocessing
# article = article.apply(lambda x: preprocess(x).split())
# summary = summary.apply(lambda x: preprocess(x).split())
#
# print("Processed Article:", article.head())
# print("Processed Summary:", summary.head())
#
# # Tokenization
# filters = '₹!"#$%&()*,-.:;?@[\\]^_`{|}~\t\n'
# oov_token = '<unk>'
#
# article_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)
# summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)
#
# # Fit tokenizers on preprocessed data
# article_tokenizer.fit_on_texts(article)
# summary_tokenizer.fit_on_texts(summary)
#
# # Convert text to sequences
# inputs = article_tokenizer.texts_to_sequences(article)
# targets = summary_tokenizer.texts_to_sequences(summary)
# # print(inputs)
#
# # Vocabulary sizes
# ENCODER_VOCAB = len(article_tokenizer.word_index) + 1
# DECODER_VOCAB = len(summary_tokenizer.word_index) + 1
#
# # Pad sequences
# ENCODER_LEN = 70  # Maximum length of input sequences
# DECODER_LEN = 40   # Maximum length of output sequences
#
# inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')
# targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')
# input_texts_padded = [' '.join([article_tokenizer.index_word.get(token, '') for token in seq]) for seq in inputs]
# target_texts_padded = [' '.join([summary_tokenizer.index_word.get(token, '') for token in seq]) for seq in targets]
#
# # Print tokenizers' word index
# print("Article Tokenizer Word Index (Word to Token):")
# print(article_tokenizer.word_index)
#
# print("\nSummary Tokenizer Word Index (Word to Token):")
# print(summary_tokenizer.word_index)
#
# word_43 = article_tokenizer.index_word.get(43, "Not found")
# word_35 = article_tokenizer.index_word.get(35, "Not found")
# word_72 = summary_tokenizer.index_word.get(35, "Not found")
#
#
# inputs = tf.cast(inputs, dtype=tf.int64)
#
# targets = tf.cast(targets, dtype=tf.int64)
# #---------------------------------------------------------------------------------------------
#
# dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
#
# #--------------------------transformer---------------------------------------
#
#
# def get_angles(position, i, d_model):
#     angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
#     return position * angle_rates
#
# def positional_encoding(position, d_model):
#     angle_rads = get_angles(
#         np.arange(position)[:, np.newaxis],
#         np.arange(d_model)[np.newaxis, :],
#         d_model
#     )
#
#     angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
#
#     angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
#
#     pos_encoding = angle_rads[np.newaxis, ...]
#
#     return tf.cast(pos_encoding, dtype=tf.float32)
#
# def create_padding_mask(seq):
#     seq = tf.cast(tf.math.equal(seq, 0), tf.float32)
#     return seq[:, tf.newaxis, tf.newaxis, :]
#
# def create_look_ahead_mask(size):
#     mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
#     return mask
#
# def scaled_dot_product_attention(q, k, v, mask):
#     matmul_qk = tf.matmul(q, k, transpose_b=True)
#
#     dk = tf.cast(tf.shape(k)[-1], tf.float32)
#     scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
#     # print('loggits')
#     # tf.print(scaled_attention_logits)
#
#     if mask is not None:
#         scaled_attention_logits += (mask * -1e9)
#
#     attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
#
#     output = tf.matmul(attention_weights, v)
#     return output, attention_weights
#
#
# class MultiHeadAttention(tf.keras.layers.Layer):
#     def __init__(self, d_model, num_heads):
#         super(MultiHeadAttention, self).__init__()
#         self.num_heads = num_heads
#         self.d_model = d_model
#
#         assert d_model % self.num_heads == 0
#
#         self.depth = d_model // self.num_heads
#
#         self.wq = tf.keras.layers.Dense(d_model)
#         self.wk = tf.keras.layers.Dense(d_model)
#         self.wv = tf.keras.layers.Dense(d_model)
#
#         self.dense = tf.keras.layers.Dense(d_model)
#
#     def split_heads(self, x, batch_size):
#         x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
#         return tf.transpose(x, perm=[0, 2, 1, 3])
#
#     def call(self, v, k, q, mask):
#         batch_size = tf.shape(q)[0]
#
#         q = self.wq(q)
#         k = self.wk(k)
#         v = self.wv(v)
#
#         q = self.split_heads(q, batch_size)
#         k = self.split_heads(k, batch_size)
#         v = self.split_heads(v, batch_size)
#
#         scaled_attention, attention_weights = scaled_dot_product_attention(
#             q, k, v, mask)
#
#         # tf.print("Attention weights matrix structure (example 1, head 1):", attention_weights[2,3], summarize=-1)
#
#         scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])
#
#         concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))
#         output = self.dense(concat_attention)
#         return output, attention_weights
#
#
# def point_wise_feed_forward_network(d_model, dff):
#     return tf.keras.Sequential([
#         tf.keras.layers.Dense(dff, activation='relu'),
#         tf.keras.layers.Dense(d_model)
#     ])
#
#
# class EncoderLayer(tf.keras.layers.Layer):
#     def __init__(self, d_model, num_heads, dff, rate=0.1):
#         super(EncoderLayer, self).__init__()
#
#         self.mha = MultiHeadAttention(d_model, num_heads)
#         self.ffn = point_wise_feed_forward_network(d_model, dff)
#
#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#         self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#
#         self.dropout1 = tf.keras.layers.Dropout(rate)
#         self.dropout2 = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, training, mask):
#         attn_output, _ = self.mha(x, x, x, mask)
#         attn_output = self.dropout1(attn_output, training=training)
#         out1 = self.layernorm1(x + attn_output)
#
#         ffn_output = self.ffn(out1)
#         ffn_output = self.dropout2(ffn_output, training=training)
#         out2 = self.layernorm2(out1 + ffn_output)
#
#         return out2
#
# # class PositionalLogEmbedding(tf.keras.layers.Layer):
# #     def __init__(self, vocab_size, embedding_dim, max_length=100):
# #         super(PositionalLogEmbedding, self).__init__()
# #         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
# #         self.positional_encoding = self.add_weight(
# #             shape=(max_length, embedding_dim),
# #             initializer="random_normal",
# #             trainable=True
# #         )
# #         self.norm = tf.keras.layers.LayerNormalization()
# #
# #     def call(self, inputs):
# #         embedded = self.embedding(inputs)
# #         log_embedded = tf.math.log1p(embedded)
# #
# #         positions = tf.range(tf.shape(inputs)[-1])  # Token positions
# #         position_encoding = tf.gather(self.positional_encoding, positions)
# #
# #         return self.norm(log_embedded + position_encoding)
#
# # class ExpEmbedding(tf.keras.layers.Layer):
# #     def __init__(self, vocab_size, embedding_dim):
# #         super(ExpEmbedding, self).__init__()
# #         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
# #         self.norm = tf.keras.layers.LayerNormalization()
# #
# #     def call(self, inputs):
# #         embedded = self.embedding(inputs)
# #         log_embedded = tf.math.log1p(embedded)  # Apply log(1 + x) to prevent log(0) issues
# #         return self.norm(log_embedded)
#
# #
#     # class ExpEmbedding(tf.keras.layers.Layer):
#     #     def __init__(self, vocab_size, embedding_dim, token_to_word_map):
#     #         super(ExpEmbedding, self).__init__()
#     #
#     #         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
#     #         self.norm = tf.keras.layers.LayerNormalization()
#     #         self.token_to_word_map = token_to_word_map  # Dictionary mapping tokens to words
#     #
#     #     def call(self, inputs):
#     #         # Convert token IDs back to words
#     #         words = tf.gather(self.token_to_word_map, inputs)
#     #
#     #         # Identify numeric tokens
#     #         is_numeric = tf.map_fn(lambda x: tf.strings.regex_full_match(x, "\\d+"), words, dtype=tf.bool)
#     #
#     #         # Standard embedding for words
#     #         word_embedded = self.embedding(inputs)
#     #
#     #         # Exponential embedding for numbers
#     #         number_embedded = tf.exp(self.embedding(inputs))
#
#             # Select appropriate embeddings
#             # embedded = tf.where(tf.expand_dims(is_numeric, -1), number_embedded, word_embedded)
#             #
#             # return self.norm(embedded)
#
#
# class ExpEmbedding(tf.keras.layers.Layer):
#     def __init__(self, vocab_size, embedding_dim, token_to_word_map):
#         super(ExpEmbedding, self).__init__()
#         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
#         self.norm = tf.keras.layers.LayerNormalization()
#
#         # Convert token_to_word_map dictionary to a TensorFlow lookup table
#         # self.token_lookup = tf.lookup.StaticHashTable(
#         #     tf.lookup.KeyValueTensorInitializer(
#         #         keys=tf.constant(list(token_to_word_map.keys()), dtype=tf.int32),
#         #         values=tf.constant(list(token_to_word_map.values()), dtype=tf.string)
#         #     ),
#         self.token_lookup = tf.lookup.StaticHashTable(
#             tf.lookup.KeyValueTensorInitializer(
#                 keys=tf.constant(list(token_to_word_map.keys()), dtype=tf.int64),  # Changed to int64
#                 values=tf.constant(list(token_to_word_map.values()), dtype=tf.string)
#             ),
#
#             default_value=tf.constant("", dtype=tf.string)  # Default empty string if key is not found
#         )
#
#     def call(self, inputs):
#         # Convert token IDs back to words
#         print('Inputtttttttttttttt')
#         tensor_int64 = tf.cast(inputs, tf.int64)
#
#         words = self.token_lookup.lookup(tensor_int64)
#         # print(words)
#
#         # Identify numeric tokens (only numbers should get the exponential embedding)
#         # is_numeric = tf.strings.regex_full_match(words, r"\d+")
#         #
#         # # Standard embedding for word tokens
#         # word_embedded = self.embedding(inputs)
#         #
#         # # Exponential embedding for number tokens
#         # number_embedded = tf.where(is_numeric, tf.exp(word_embedded), word_embedded)
#         #
#         # return self.norm(number_embedded)
#         is_numeric = tf.strings.regex_full_match(words, r"\d+")
#         # print(is_numeric)
#
#         # Expand dims for broadcasting
#         is_numeric = tf.expand_dims(is_numeric, axis=-1)  # Shape: [batch_size, seq_len, 1]
#
#         # Standard embedding for word tokens
#         word_embedded = self.embedding(tensor_int64)  # Shape: [batch_size, seq_len, embedding_dim]
#         #         log_embedded = tf.math.log1p(embedded)  # Apply log(1 + x) to prevent log(0) issues
#
#         # Exponential embedding for number tokens
#         # number_embedded = tf.where(is_numeric, tf.exp(word_embedded), word_embedded)
#         number_embedded = tf.where(is_numeric,tf.exp(word_embedded), word_embedded)
#
#         return self.norm(number_embedded)
#
# # class ExpEmbedding(tf.keras.layers.Layer):
# #     def __init__(self, vocab_size, embedding_dim, token_to_word_map, **kwargs):
# #         super(ExpEmbedding, self).__init__(**kwargs)
# #         self.vocab_size = vocab_size
# #         self.embedding_dim = embedding_dim
# #         self.token_to_word_map = token_to_word_map  # Store for reconstruction
# #         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
# #         self.norm = tf.keras.layers.LayerNormalization()
# #
# #         self.token_lookup = tf.lookup.StaticHashTable(
# #             tf.lookup.KeyValueTensorInitializer(
# #                 keys=tf.constant(list(token_to_word_map.keys()), dtype=tf.int64),
# #                 values=tf.constant(list(token_to_word_map.values()), dtype=tf.string)
# #             ),
# #             default_value=tf.constant("", dtype=tf.string)
# #         )
# #
# #     def call(self, inputs):
# #         tensor_int64 = tf.cast(inputs, tf.int64)
# #         words = self.token_lookup.lookup(tensor_int64)
# #         # print(words)
# #
# #
# #         is_numeric = tf.strings.regex_full_match(words, r"\d+")
# #         is_numeric = tf.expand_dims(is_numeric, axis=-1)
# #         word_embedded = self.embedding(tensor_int64)
# #         number_embedded = tf.where(is_numeric, tf.math.log1p(word_embedded), word_embedded)
# #         return self.norm(number_embedded)
# #
# #     def get_config(self):
# #         config = super().get_config()
# #         config.update({
# #             "vocab_size": self.vocab_size,
# #             "embedding_dim": self.embedding_dim,
# #             "token_to_word_map": self.token_to_word_map  # Include for reconstruction
# #         })
# #         return config
# #
# #     @classmethod
# #     def from_config(cls, config):
# #         return cls(**config)
#
# class DecoderLayer(tf.keras.layers.Layer):
#     def __init__(self, d_model, num_heads, dff, rate=0.1):
#         super(DecoderLayer, self).__init__()
#
#         self.mha1 = MultiHeadAttention(d_model, num_heads)
#         self.mha2 = MultiHeadAttention(d_model, num_heads)
#
#         self.ffn = point_wise_feed_forward_network(d_model, dff)
#
#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#         self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#         self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#
#         self.dropout1 = tf.keras.layers.Dropout(rate)
#         self.dropout2 = tf.keras.layers.Dropout(rate)
#         self.dropout3 = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
#         attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)
#         attn1 = self.dropout1(attn1, training=training)
#         out1 = self.layernorm1(attn1 + x)
#
#         attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)
#         attn2 = self.dropout2(attn2, training=training)
#         out2 = self.layernorm2(attn2 + out1)
#
#         ffn_output = self.ffn(out2)
#         ffn_output = self.dropout3(ffn_output, training=training)
#         out3 = self.layernorm3(ffn_output + out2)
#
#         return out3, attn_weights_block1, attn_weights_block2
#
#
# class Encoder(tf.keras.layers.Layer):
#     def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):
#         super(Encoder, self).__init__()
#         self.d_model = d_model
#         self.num_layers = num_layers
#
#         # self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
#         self.embedding = ExpEmbedding(input_vocab_size, d_model,article_tokenizer.index_word)  # Pass vocab_size and d_model
#
#         self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)
#
#         self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]
#
#         self.dropout = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, training, mask):
#         seq_len = tf.shape(x)[1]
#
#         x = self.embedding(x)
#         # print("encoder embedding")
#         # print(x)
#         x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
#         x += self.pos_encoding[:, :seq_len, :]
#
#         x = self.dropout(x, training=training)
#
#         for i in range(self.num_layers):
#             x = self.enc_layers[i](x, training=training, mask=mask)
#
#         return x
#
#
# class Decoder(tf.keras.layers.Layer):
#
#     def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):
#         super(Decoder, self).__init__()
#         self.d_model = d_model
#         self.num_layers = num_layers
#         # self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)
#         self.embedding = ExpEmbedding(target_vocab_size, d_model,summary_tokenizer.index_word)  # Pass vocab_size and d_model
#
#         self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)
#         self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]
#         self.dropout = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
#         seq_len = tf.shape(x)[1]
#         attention_weights = {}
#         x = self.embedding(x)
#         # print("deocder embedding")
#         # print(x)
#         x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
#         x += self.pos_encoding[:, :seq_len, :]
#
#         x = self.dropout(x, training=training)
#
#         for i in range(self.num_layers):
#             x, block1, block2 = self.dec_layers[i](x, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)
#
#             attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1
#             attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2
#
#         return x, attention_weights
#
# class Transformer(tf.keras.Model):
#     def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input,
#                      pe_target, rate=0.1):
#         super(Transformer, self).__init__()
#
#         self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)
#
#         self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)
#
#         self.final_layer = tf.keras.layers.Dense(target_vocab_size)
#
#     def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):
#         # encoder_weights = self.encoder.embedding.embedding.get_weights()
#         # if encoder_weights:
#         #     print("Encoder Embedding Weights Shape:", encoder_weights[0].shape)
#         #     print("Encoder Embedding Weights Sample:\n", encoder_weights[0][:5])  # First 5 embeddings
#         # else:
#         #     print("Encoder embedding weights not initialized.")
#
#         # Print Decoder Embedding Weights
#         # decoder_weights = self.decoder.embedding.embedding.get_weights()
#         # if decoder_weights:
#         #     print("Decoder Embedding Weights Shape:", decoder_weights[0].shape)
#         #     print("Decoder Embedding Weights Sample:\n", decoder_weights[0][:5])  # First 5 embeddings
#         # else:
#         #     print("Decoder embedding weights not initialized.")
#         enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)
#
#         dec_output, attention_weights = self.decoder(tar, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)
#
#         final_output = self.final_layer(dec_output)
#
#         return final_output, attention_weights
#
# # num_layers = 6
# # d_model = 128
# # dff = 512
# # num_heads = 8
# # dropout_rate = 0.3
# # EPOCHS = 90
#
# num_layers = 3
# d_model = 128
# # d_model=32
# dff = 256
# num_heads = 16
# dropout_rate = 0.3
# EPOCHS =20
#
# class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
#     def __init__(self, d_model, warmup_steps=4000):
#         super(CustomSchedule, self).__init__()
#
#         self.d_model = d_model
#         self.d_model = tf.cast(self.d_model, tf.float32)
#
#         self.warmup_steps = warmup_steps
#
#
#     def __call__(self, step):
#         step = tf.cast(step, dtype=tf.float32)
#         arg1 = tf.math.rsqrt(step)
#         arg2 = step * (self.warmup_steps ** -1.5)
#         return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)
#
# learning_rate = CustomSchedule(d_model)
# # learning_rate=.001
#
# optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)
# temp_learning_rate_schedule = CustomSchedule(d_model)
#
# plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))
# plt.ylabel("Learning Rate")
# plt.xlabel("Train Step")
#
#
# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
# def loss_function(real, pred):
#     # mask = tf.math.logical_not(tf.math.equal(real, 0))
#     # loss_ = loss_object(real, pred)
#     #
#     # mask = tf.cast(mask, dtype=loss_.dtype)
#     # loss_ *= mask
#     #
#     # return tf.reduce_sum(loss_)/tf.reduce_sum(mask)
#     loss_ = loss_object(real, pred)  # Computes loss for all tokens
#     mask = tf.math.not_equal(real, 0)  # Mask out padding tokens (0)
#     mask = tf.cast(mask, dtype=loss_.dtype)  # Convert mask to correct type
#     loss_ *= mask  # Apply the mask to ignore padding losses
#     return tf.reduce_sum(loss_) / tf.reduce_sum(mask)  # Normalize
#
#
# def accuracy_function(real, pred):
#
#     accuracies = tf.equal(real, tf.argmax(pred, axis=2))
#     accuracies = tf.cast(accuracies, dtype= tf.float32)
#     mask = tf.math.logical_not(tf.math.equal(real, 0))
#     # accuracies = tf.math.logical_and(mask, accuracies)
#     accuracies = tf.cast(accuracies, dtype=tf.float32)
#     print('real')
#     print(real)
#     print('predicted')
#     print(accuracies)
#     mask = tf.cast(mask, dtype=tf.float32)
#     return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)
#
# train_loss = tf.keras.metrics.Mean(name='train_loss')
# train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')
#
# transformer = Transformer(
#     num_layers=num_layers,
#     d_model=d_model,
#     num_heads=num_heads,
#     dff=dff,
#     input_vocab_size=ENCODER_VOCAB,
#     target_vocab_size=DECODER_VOCAB,
#     pe_input=1000,
#     pe_target=1000,
#     rate=dropout_rate)
#
#
#
#
# def create_masks(inp, tar):
#     enc_padding_mask = create_padding_mask(inp)
#     dec_padding_mask = create_padding_mask(inp)
#     look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])
#     dec_target_padding_mask = create_padding_mask(tar)
#     combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)
#
#     return enc_padding_mask, combined_mask, dec_padding_mask
#
# # checkpoint_path = "/content/drive/MyDrive/Dataset/checkpoints"
# checkpoint_path = "checkpoints"
# ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)
# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)
#
# if ckpt_manager.latest_checkpoint:
#     ckpt.restore(ckpt_manager.latest_checkpoint)
#
# @tf.function
# def train_step(inp, tar):
#     tar_inp = tar[:, :-1]
#     tar_real = tar[:, 1:]
#
#     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)
#
#     with tf.GradientTape() as tape:
#         predictions, _ = transformer(
#             inp,
#             tar_inp,
#             training=True,
#             enc_padding_mask=enc_padding_mask,
#             look_ahead_mask=combined_mask,
#             dec_padding_mask=dec_padding_mask
#         )
#
#         loss = loss_function(tar_real, predictions)
#
#     gradients = tape.gradient(loss, transformer.trainable_variables)
#     optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
#     train_loss(loss)
#     train_accuracy(accuracy_function(tar_real, predictions))
#
#
# for epoch in range(EPOCHS):
#     start = time.time()
#
#     train_loss.reset_states()
#
#     for (batch, (inp, tar)) in enumerate(dataset):
#         train_step(inp, tar)
#
#         if batch % 100 == 0:
#             print(
#                 f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')
#
#     if (epoch + 1) % 5 == 0:
#         ckpt_save_path = ckpt_manager.save()
#         print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))
#
#     print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')
#     print('Time taken for 1 epoch: {} secs\n'.format(time.time() - start))
# # creating embedding solving the problem of unseen number token
#
# def evaluate(input_article):
#     # Tokenize and pad the input article
#     print(input_article)
#     # if ckpt_manager.latest_checkpoint:
#     #     print(f"Restoring from checkpoint: {ckpt_manager.latest_checkpoint}")
#     #     ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()
#     #     # pretrained_embedding_weights = transformer.encoder.embedding.embedding.get_weights()[0]
#     #     # print("Shape of Encoder Embedding Weights:", pretrained_embedding_weights.shape)
#     #     #
#     #     # pretrained_embedding_weights_decoder = transformer.decoder.embedding.embedding.get_weights()[0]
#     #     # print("Shape of Decoder Embedding Weights:", pretrained_embedding_weights_decoder.shape)
#     # else:
#     #     print("No checkpoint found. Using randomly initialized weights.")
#
#     # input_series = pd.Series([input_article])  # Wrap it in a list to create a Series
#     input_article = tf.convert_to_tensor(input_article, dtype=tf.string)  # Ensure it's a TensorFlow string
#     input_article = input_article.numpy().decode("utf-8")
#     # inputs=inputArr.apply(preprocess(input_article))
#     # inputs = input_series.apply(lambda x: preprocess(x).split())
#     #
#     input_article = article_tokenizer.texts_to_sequences([input_article])
#
#     # input_article = article_tokenizer.texts_to_sequences([inputs])
#     # input_article=[[3,4,5,6,7,8,9,10,14,15,18,19,11,15,16,17,12]]
#     # input_article = [[4,5,6,7,8,11,12,9]]
#     input_article = tf.cast(input_article, dtype=tf.int64)
#
#     print("tokens")
#     print(input_article)
#
#     input_article = tf.keras.preprocessing.sequence.pad_sequences(
#         input_article, maxlen=ENCODER_LEN, padding='post', truncating='post'
#     )
#
#     encoder_input = tf.expand_dims(input_article[0], 0)
#     print (encoder_input)
#     decoder_input = [summary_tokenizer.word_index['<sos>']]
#     output = tf.expand_dims(decoder_input, 0)
#     # print("99999999999999999")
#     # print(output)
#
#     for i in range(DECODER_LEN):
#         # Create masks
#         enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)
#
#         # Get predictions from the transformer
#         predictions, attention_weights = transformer(
#             encoder_input,
#             output,
#             training=False,
#             enc_padding_mask=enc_padding_mask,
#             look_ahead_mask=combined_mask,
#             dec_padding_mask=dec_padding_mask
#         )
#         # print(transformer.summary())
#         # for layer in transformer.layers:
#         #     print(layer.name)
#         # for var in transformer.variables:
#         #     print(var.name)
#         # for var in transformer.encoder.trainable_variables:
#         #     print(var.name, var.shape)
#
#
#         # transformer.encoder.embedding.embedding.set_weights(pretrained_embedding_weights)
#         # transformer.decoder.embedding.embedding.set_weights(pretrained_embedding_weights)
#
#         predictions = predictions[:, -1:, :]  # Take the last predicted token
#         predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)
#         if predicted_id == summary_tokenizer.word_index['<eos>']:
#             return tf.squeeze(output, axis=0), attention_weights
#         # Append the predicted token to the output
#         output = tf.concat([output, predicted_id], axis=-1)
#         # print(output)
#     return tf.squeeze(output, axis=0), attention_weights
#
# def summarize(input_article):
#     summarized = evaluate(input_article=input_article)[0].numpy()
#     summarized = np.expand_dims(summarized[1:], 0)  # Remove the <sos> token
#     return summary_tokenizer.sequences_to_texts(summarized)[0]
#
# # Test the function
# print('Output:')
# print(summarize(
#     "<SOS> Find the average of 697 and 666 and 267 and 376 <EOS>"))
#
# def preprocess_and_split_numbers(input_text):
#     # Split numbers into their individual digits
#     # This regular expression finds numbers and splits them into individual characters
#     input_text = re.sub(r'(\d)', r' \1 ', input_text)  # Add space around each digit
#     return input_text