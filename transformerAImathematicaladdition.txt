# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# import pandas as pd
# from collections import defaultdict
# import string
# import tensorflow as tf
# import re
# import os
# import time
# from tensorflow import keras
# from tensorflow.keras.layers import Dense, Input
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.models import Model
# from tensorflow.keras.callbacks import ModelCheckpoint
# from sklearn.preprocessing import OneHotEncoder
# from sklearn.model_selection import train_test_split
#
# # ENCODER_LEN = 200
# # DECODER_LEN = 70
# # BATCH_SIZE = 36
# # BUFFER_SIZE = BATCH_SIZE*8
# ENCODER_LEN = 140
# DECODER_LEN = 40
# BATCH_SIZE = 16
# BUFFER_SIZE = BATCH_SIZE*8
# news = pd.read_csv("english_news_dataset1.csv")
# print(news)
# print(news.columns)
#
#
# article = news['Text']
# summary = news['Summary']
# article = article.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')
# summary = summary.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')
# def preprocess(text):
#     text = re.sub(r"&.[1-9]+;"," ",text)
#     return text
# article = article.apply(lambda x: preprocess(x))
# summary = summary.apply(lambda x: preprocess(x))
#
# filters = '!"#$%&()*+,-./:;=?@[\\]^_`{|}~\t\n'
# oov_token = '<unk>'
# article_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)
# summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)
# article_tokenizer.fit_on_texts(article)
# summary_tokenizer.fit_on_texts(summary)
# inputs = article_tokenizer.texts_to_sequences(article)
# targets = summary_tokenizer.texts_to_sequences(summary)
#
# ENCODER_VOCAB = len(article_tokenizer.word_index) + 1
# DECODER_VOCAB = len(summary_tokenizer.word_index) + 1
# # print(ENCODER_VOCAB, DECODER_VOCAB)
#
# inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')
# targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')
# inputs = tf.cast(inputs, dtype=tf.int64)
# targets = tf.cast(targets, dtype=tf.int64)
#
# dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
#
#
# #--------------------------transformer---------------------------------------
#
#
# def get_angles(position, i, d_model):
#     angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
#     return position * angle_rates
#
# def positional_encoding(position, d_model):
#     angle_rads = get_angles(
#         np.arange(position)[:, np.newaxis],
#         np.arange(d_model)[np.newaxis, :],
#         d_model
#     )
#
#     angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
#
#     angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
#
#     pos_encoding = angle_rads[np.newaxis, ...]
#
#     return tf.cast(pos_encoding, dtype=tf.float32)
#
# def create_padding_mask(seq):
#     seq = tf.cast(tf.math.equal(seq, 0), tf.float32)
#     return seq[:, tf.newaxis, tf.newaxis, :]
#
# def create_look_ahead_mask(size):
#     mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
#     return mask
#
# def scaled_dot_product_attention(q, k, v, mask):
#     matmul_qk = tf.matmul(q, k, transpose_b=True)
#
#     dk = tf.cast(tf.shape(k)[-1], tf.float32)
#     scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
#
#     if mask is not None:
#         scaled_attention_logits += (mask * -1e9)
#
#     attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
#
#     output = tf.matmul(attention_weights, v)
#     return output, attention_weights
#
#
# class MultiHeadAttention(tf.keras.layers.Layer):
#     def __init__(self, d_model, num_heads):
#         super(MultiHeadAttention, self).__init__()
#         self.num_heads = num_heads
#         self.d_model = d_model
#
#         assert d_model % self.num_heads == 0
#
#         self.depth = d_model // self.num_heads
#
#         self.wq = tf.keras.layers.Dense(d_model)
#         self.wk = tf.keras.layers.Dense(d_model)
#         self.wv = tf.keras.layers.Dense(d_model)
#
#         self.dense = tf.keras.layers.Dense(d_model)
#
#     def split_heads(self, x, batch_size):
#         x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
#         return tf.transpose(x, perm=[0, 2, 1, 3])
#
#     def call(self, v, k, q, mask):
#         batch_size = tf.shape(q)[0]
#
#         q = self.wq(q)
#         k = self.wk(k)
#         v = self.wv(v)
#
#         q = self.split_heads(q, batch_size)
#         k = self.split_heads(k, batch_size)
#         v = self.split_heads(v, batch_size)
#
#         scaled_attention, attention_weights = scaled_dot_product_attention(
#             q, k, v, mask)
#
#         scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])
#
#         concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))
#         output = self.dense(concat_attention)
#
#         return output, attention_weights
#
#
# def point_wise_feed_forward_network(d_model, dff):
#     return tf.keras.Sequential([
#         tf.keras.layers.Dense(dff, activation='relu'),
#         tf.keras.layers.Dense(d_model)
#     ])
#
#
# class EncoderLayer(tf.keras.layers.Layer):
#     def __init__(self, d_model, num_heads, dff, rate=0.1):
#         super(EncoderLayer, self).__init__()
#
#         self.mha = MultiHeadAttention(d_model, num_heads)
#         self.ffn = point_wise_feed_forward_network(d_model, dff)
#
#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#         self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#
#         self.dropout1 = tf.keras.layers.Dropout(rate)
#         self.dropout2 = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, training, mask):
#         attn_output, _ = self.mha(x, x, x, mask)
#         attn_output = self.dropout1(attn_output, training=training)
#         out1 = self.layernorm1(x + attn_output)
#
#         ffn_output = self.ffn(out1)
#         ffn_output = self.dropout2(ffn_output, training=training)
#         out2 = self.layernorm2(out1 + ffn_output)
#
#         return out2
#
#
# class DecoderLayer(tf.keras.layers.Layer):
#     def __init__(self, d_model, num_heads, dff, rate=0.1):
#         super(DecoderLayer, self).__init__()
#
#         self.mha1 = MultiHeadAttention(d_model, num_heads)
#         self.mha2 = MultiHeadAttention(d_model, num_heads)
#
#         self.ffn = point_wise_feed_forward_network(d_model, dff)
#
#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#         self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#         self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#
#         self.dropout1 = tf.keras.layers.Dropout(rate)
#         self.dropout2 = tf.keras.layers.Dropout(rate)
#         self.dropout3 = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
#         attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)
#         attn1 = self.dropout1(attn1, training=training)
#         out1 = self.layernorm1(attn1 + x)
#
#         attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)
#         attn2 = self.dropout2(attn2, training=training)
#         out2 = self.layernorm2(attn2 + out1)
#
#         ffn_output = self.ffn(out2)
#         ffn_output = self.dropout3(ffn_output, training=training)
#         out3 = self.layernorm3(ffn_output + out2)
#
#         return out3, attn_weights_block1, attn_weights_block2
#
#
# class Encoder(tf.keras.layers.Layer):
#     def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):
#         super(Encoder, self).__init__()
#
#         self.d_model = d_model
#         self.num_layers = num_layers
#
#         self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
#         self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)
#
#         self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]
#
#         self.dropout = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, training, mask):
#         seq_len = tf.shape(x)[1]
#
#         x = self.embedding(x)
#         x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
#         x += self.pos_encoding[:, :seq_len, :]
#
#         x = self.dropout(x, training=training)
#
#         for i in range(self.num_layers):
#             x = self.enc_layers[i](x, training, mask)
#
#         return x
#
#
# class Decoder(tf.keras.layers.Layer):
#
#     def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):
#         super(Decoder, self).__init__()
#
#         self.d_model = d_model
#         self.num_layers = num_layers
#
#         self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)
#         self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)
#
#         self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]
#         self.dropout = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
#         seq_len = tf.shape(x)[1]
#         attention_weights = {}
#
#         x = self.embedding(x)
#         x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
#         x += self.pos_encoding[:, :seq_len, :]
#
#         x = self.dropout(x, training=training)
#
#         for i in range(self.num_layers):
#             x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)
#
#             attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1
#             attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2
#
#         return x, attention_weights
#
# class Transformer(tf.keras.Model):
#     def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input,
#                      pe_target, rate=0.1):
#         super(Transformer, self).__init__()
#
#         self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)
#
#         self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)
#
#         self.final_layer = tf.keras.layers.Dense(target_vocab_size)
#
#     def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):
#         enc_output = self.encoder(inp, training, enc_padding_mask)
#
#         dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)
#
#         final_output = self.final_layer(dec_output)
#
#         return final_output, attention_weights
#
# # num_layers = 6
# # d_model = 128
# # dff = 512
# # num_heads = 8
# # dropout_rate = 0.3
# # EPOCHS = 90
#
# num_layers = 6
# d_model = 128
# dff = 512
# num_heads = 8
# dropout_rate = 0.3
# EPOCHS = 30
#
# class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
#     def __init__(self, d_model, warmup_steps=4000):
#         super(CustomSchedule, self).__init__()
#
#         self.d_model = d_model
#         self.d_model = tf.cast(self.d_model, tf.float32)
#
#         self.warmup_steps = warmup_steps
#
#     def __call__(self, step):
#         arg1 = tf.math.rsqrt(step)
#         arg2 = step * (self.warmup_steps ** -1.5)
#
#         return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)
#
#
# learning_rate = CustomSchedule(d_model)
# #learning_rate=.006
#
# optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)
# temp_learning_rate_schedule = CustomSchedule(d_model)
#
# plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))
# plt.ylabel("Learning Rate")
# plt.xlabel("Train Step")
#
#
# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
# def loss_function(real, pred):
#     mask = tf.math.logical_not(tf.math.equal(real, 0))
#     loss_ = loss_object(real, pred)
#
#     mask = tf.cast(mask, dtype=loss_.dtype)
#     loss_ *= mask
#
#     return tf.reduce_sum(loss_)/tf.reduce_sum(mask)
#
#
# def accuracy_function(real, pred):
#     accuracies = tf.equal(real, tf.argmax(pred, axis=2))
#     #accuracies = tf.cast(accuracies, dtype= tf.float32)
#     # print('real')
#     # print(real)
#     # print('pred')
#     # print(tf.argmax(pred, axis=2))
#     tf.print("Real Matrix (token IDs):", real, summarize=-1)  # summarize=-1 to print the whole tensor
#     tf.print("Pred Matrix (token IDs):", tf.argmax(pred, axis=-1), summarize=-1)
#     tf.print("---")
#
#     mask = tf.math.logical_not(tf.math.equal(real, 0))
#     accuracies = tf.math.logical_and(mask, accuracies)
#     accuracies = tf.cast(accuracies, dtype=tf.float32)
#     mask = tf.cast(mask, dtype=tf.float32)
#     return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)
#
# train_loss = tf.keras.metrics.Mean(name='train_loss')
# train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')
#
# transformer = Transformer(
#     num_layers=num_layers,
#     d_model=d_model,
#     num_heads=num_heads,
#     dff=dff,
#     input_vocab_size=ENCODER_VOCAB,
#     target_vocab_size=DECODER_VOCAB,
#     pe_input=1000,
#     pe_target=1000,
#     rate=dropout_rate)
#
#
# def create_masks(inp, tar):
#     enc_padding_mask = create_padding_mask(inp)
#     dec_padding_mask = create_padding_mask(inp)
#
#     look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])
#     dec_target_padding_mask = create_padding_mask(tar)
#     combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)
#
#     return enc_padding_mask, combined_mask, dec_padding_mask
#
# checkpoint_path = "checkpoints"
#
# ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)
#
# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)
#
# if ckpt_manager.latest_checkpoint:
#     ckpt.restore(ckpt_manager.latest_checkpoint)
#     print ('Latest checkpoint restored!!')
#
#
# @tf.function
# def train_step(inp, tar):
#     tar_inp = tar[:, :-1]
#     tar_real = tar[:, 1:]
#
#     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)
#
#     with tf.GradientTape() as tape:
#         predictions, _ = transformer(
#             inp, tar_inp,
#             True,
#             enc_padding_mask,
#             combined_mask,
#             dec_padding_mask
#         )
#         loss = loss_function(tar_real, predictions)
#
#     gradients = tape.gradient(loss, transformer.trainable_variables)
#     optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
#
#     train_loss(loss)
#     train_accuracy(accuracy_function(tar_real, predictions))
#
#
# for epoch in range(EPOCHS):
#     start = time.time()
#
#     train_loss.reset_states()
#
#     for (batch, (inp, tar)) in enumerate(dataset):
#         train_step(inp, tar)
#
#         if batch % 100 == 0:
#             print(
#                 f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')
#
#     if (epoch + 1) % 4 == 0:
#         ckpt_save_path = ckpt_manager.save()
#         print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))
#
#     print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')
#     print('Time taken for 1 epoch: {} secs\n'.format(time.time() - start))
#
#
# def evaluate(input_article):
#     # Tokenize and pad the input article
#     input_article = article_tokenizer.texts_to_sequences([input_article])
#     input_article = tf.keras.preprocessing.sequence.pad_sequences(
#         input_article, maxlen=ENCODER_LEN, padding='post', truncating='post'
#     )
#     encoder_input = tf.expand_dims(input_article[0], 0)
#
#     # Initialize decoder input with <sos> token
#     decoder_input = [summary_tokenizer.word_index['<sos>']]
#
#     output = tf.expand_dims(decoder_input, 0)
#
#     for i in range(DECODER_LEN):
#         # Create masks
#         enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)
#
#         # Get predictions from the transformer
#         predictions, attention_weights = transformer(
#             encoder_input,
#             output,
#             training=False,
#             enc_padding_mask=enc_padding_mask,
#             look_ahead_mask=combined_mask,
#             dec_padding_mask=dec_padding_mask
#         )
#
#         # Get the predicted token
#         predictions = predictions[:, -1:, :]  # Take the last predicted token
#         # print('predictions')
#         # print(predictions)
#         predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)
#         # print(predicted_id)
#         # print(summary_tokenizer.word_index)
#
#         # Check for <eos> token to stop generation
#         if predicted_id == summary_tokenizer.word_index['<eos>']:
#             return tf.squeeze(output, axis=0), attention_weights
#
#         # Append the predicted token to the output
#         output = tf.concat([output, predicted_id], axis=-1)
#
#     return tf.squeeze(output, axis=0), attention_weights
#
#
# def summarize(input_article):
#     summarized = evaluate(input_article=input_article)[0].numpy()
#     summarized = np.expand_dims(summarized[1:], 0)  # Remove the <sos> token
#     return summary_tokenizer.sequences_to_texts(summarized)[0]
#
#
# # Test the function
# print('Output:')
# print(summarize(
#     '<SOS>Congress leader Baljinder Singh was shot dead at his house in Punjabs Moga on Monday, a video of which has also surfaced online. According to a report by the Free Press Journal, Singh received a call from someone regarding the signing of documents. In the video, the accused can be seen shooting at Singh while he is walking towards him. <EOS>'
# ))


#---------------------------------------------ssecond way------------------------------------------


# import numpy as np
# import pandas as pd
# import nltk
# nltk.download('stopwords')
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize, sent_tokenize
# import re
# import warnings
# warnings.filterwarnings("ignore")
# from sklearn.model_selection import train_test_split as tts
# import torch
# import collections
# from collections import Counter
# from torch import nn
# from torch.utils.data import Dataset, DataLoader
# from bs4 import BeautifulSoup
# import math
# #----------------------------------data proccessing-----------------------------
# df = pd.read_csv('english_news_dataset1.csv', nrows = 20)
# data = df[['Text', 'Summary']]
# data.drop_duplicates(subset=['Text'], inplace=True) # Drop duplicate rows
# word_mapping = {"ain't": "is not","aint": "is not", "aren't": "are not","arent": "are not","can't": "cannot","cant": "cannot", "'cause": "because", "cause": "because", "could've": "could have", "couldn't": "could not",
#
#                            "didn't": "did not", "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not",
#
#                            "he'd": "he would","he'll": "he will", "he's": "he is", "how'd": "how did", "how'd'y": "how do you", "how'll": "how will", "how's": "how is",
#
#                            "I'd": "I would", "I'd've": "I would have", "I'll": "I will", "I'll've": "I will have","I'm": "I am", "I've": "I have", "i'd": "i would",
#
#                            "i'd've": "i would have", "i'll": "i will",  "i'll've": "i will have","i'm": "i am", "i've": "i have", "isn't": "is not", "it'd": "it would",
#
#                            "it'd've": "it would have", "it'll": "it will", "it'll've": "it will have","it's": "it is", "let's": "let us", "ma'am": "madam",
#
#                            "mayn't": "may not", "might've": "might have","mightn't": "might not","mightn't've": "might not have", "must've": "must have", 'mstake':"mistake",
#
#                            "mustn't": "must not", "mustn't've": "must not have", "needn't": "need not", "needn't've": "need not have","o'clock": "of the clock",
#
#                            "oughtn't": "ought not", "oughtn't've": "ought not have", "shan't": "shall not", "sha'n't": "shall not", "shan't've": "shall not have",
#
#                            "she'd": "she would", "she'd've": "she would have", "she'll": "she will", "she'll've": "she will have", "she's": "she is",
#
#                            "should've": "should have", "shouldn't": "should not", "shouldn't've": "should not have", "so've": "so have","so's": "so as",
#
#                            "this's": "this is","that'd": "that would", "that'd've": "that would have", "that's": "that is", "there'd": "there would",
#
#                            "there'd've": "there would have", "there's": "there is", "here's": "here is","they'd": "they would", "they'd've": "they would have",
#
#                            "they'll": "they will", "they'll've": "they will have", "they're": "they are", "they've": "they have", "to've": "to have",
#
#                            "wasn't": "was not",'wasnt':"was not", "we'd": "we would", "we'd've": "we would have", "we'll": "we will", "we'll've": "we will have", "we're": "we are",
#
#                            "we've": "we have", "weren't": "were not", "what'll": "what will", "what'll've": "what will have", "what're": "what are",
#
#                            "what's": "what is", "what've": "what have", "when's": "when is", "when've": "when have", "where'd": "where did", "where's": "where is",
#
#                            "where've": "where have", "who'll": "who will", "who'll've": "who will have", "who's": "who is", "who've": "who have",
#
#                            "why's": "why is", "why've": "why have", "will've": "will have", "won't": "will not", "won't've": "will not have",
#
#                            "would've": "would have", "wouldn't": "would not", "wouldn't've": "would not have", "y'all": "you all",
#
#                            "y'all'd": "you all would","y'all'd've": "you all would have","y'all're": "you all are","y'all've": "you all have",
#
#                            "you'd": "you would", "you'd've": "you would have", "you'll": "you will", "you'll've": "you will have",
#
#                            "you're": "you are", "you've": "you have", 'youve':"you have", 'goin':"going", '4ward':"forward", "shant":"shall not",'tat':"that", 'u':"you", 'v': "we",'b4':'before', "sayin'":"saying"
#                       }
# stop_words = list(stopwords.words('english'))
# # stop_words = set(stopwords.words('english'))
# #
# #
# def text_cleaner(text):
#     newString = text.lower()
#     newString = BeautifulSoup(newString, "lxml").text
#     newString = re.sub(r'\([^)]*\)', '', newString)
#     newString = re.sub('"', '', newString)
#     newString = ' '.join([word_mapping[t] if t in word_mapping else t for t in newString.split(" ")])
#     newString = re.sub(r"'s\b", "", newString)
#     newString = re.sub("[^a-zA-Z]", " ", newString)
#     tokens = [w for w in newString.split() if not w in stop_words]
#     long_words = []
#
#     tokens = [w for w in newString.split()]
#     long_words = []
#     for i in tokens:
#         if len(i) >= 3:  # removing short word
#             long_words.append(i)
#     text = " ".join(long_words).strip()
#
#     def no_space(word, prev_word):
#         return word in set(',!"";.''?') and prev_word != " "
#
#     text = text.replace('\u202f', ' ').replace('\xa0', ' ').lower()
#     out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)]
#     text = ''.join(out)
#     return text
#
#
# data['cleaned_text'] = data['Text'].apply(text_cleaner)
# data['cleaned_summary'] = data['Summary'].apply(text_cleaner)
# # this step is to remove all rows that have a blank summary
# data["cleaned_summary"].replace('', np.nan, inplace=True)
# data.dropna(axis=0, inplace=True)
#
# max_len_text=10
# max_len_summary=100
#
# x_train,x_test,y_train,y_test = tts(data['cleaned_text'],data['cleaned_summary'],test_size=0.1, shuffle=True, random_state=111)
#
# def tokenize(lines, token='word'):
#     assert token in ('word', 'char'), 'Unknown token type: ' + token
#     return [line.split() if token == 'word' else list(line) for line in lines]
#
# # pading function
# def truncate_pad(line, num_steps, padding_token):
#     if len(line) > num_steps:
#         return line[:num_steps]  # Truncate
#     return line + [padding_token] * (num_steps - len(line))  # Pad
#
# # the vocabulary class
# class Vocab:
#     def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):
#         # Flatten a 2D list if needed
#         if tokens and isinstance(tokens[0], list):
#             tokens = [token for line in tokens for token in line]
#             # print('tokensssssss')
#             # print(tokens)
#         # Count token frequencies
#         counter = collections.Counter(tokens)
#         # print(counter)
#         self.token_freqs = sorted(counter.items(), key=lambda x: x[1],
#                                   reverse=True)
#         # print(self.token_freqs)
#         # The list of unique tokens
#         self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [
#             token for token, freq in self.token_freqs if freq >= min_freq])))
#         # print("idx to token")
#         # print(self.idx_to_token)
#         self.token_to_idx = {token: idx
#                              for idx, token in enumerate(self.idx_to_token)}
#         # print(self.token_to_idx)
#         self.unk = self.token_to_idx['<unk>']
#
#     def __len__(self):
#         return len(self.idx_to_token)
#
#     def __getitem__(self, tokens):
#
#         if not isinstance(tokens, (list, tuple)):
#             # print('tokenssssssssssss')
#             # print(tokens)
#             return self.token_to_idx.get(tokens, self.unk)
#         return [self.__getitem__(token) for token in tokens]
#
#     def to_tokens(self, indices):
#
#         if hasattr(indices, '__len__') and len(indices) > 1:
#             return [self.idx_to_token[int(index)] for index in indices]
#         return self.idx_to_token[indices]
#
#     def unk(self):  # Index for the unknown token
#         # print("not foundddddddddddddd")
#         # print(self.token_to_idx['<unk>'])
#         return self.token_to_idx['<unk>']
# # tokenize
# src_tokens = tokenize(data['cleaned_text'])
# tgt_tokens = tokenize(data['cleaned_summary'])
# # print(tgt_tokens)
#
# # build vocabulary on dataset
# # src_sentence='Technology has revolutionized healthcare by improving diagnosis, treatment, and patient care'
# src_vocab = Vocab(src_tokens, reserved_tokens=['<pad>', '<bos>', '<eos>'])
# # src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]
# # src_tokens = truncate_pad(src_tokens, 10, src_vocab['<pad>'])
# # print('Source Tokens')
# # print(src_tokens)
# tgt_vocab = Vocab(tgt_tokens, reserved_tokens=['<pad>', '<bos>', '<eos>'])
# # print(tgt_vocab)
#
# #-----------------------------------------------------------------------------------
# #-------------------------------dataset creation-----------------------------------
# def build_array_sum(lines, vocab, num_steps):
#     lines = [vocab[l] for l in lines]
#     lines = [l + [vocab['<eos>']] for l in lines]
#     array = torch.tensor([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])
#     valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)
#     return array, valid_len
#
# src_array, src_valid_len = build_array_sum(src_tokens, src_vocab, max_len_text)
# tgt_array, tgt_valid_len = build_array_sum(tgt_tokens, tgt_vocab, max_len_summary)
# data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)
# # create the tensor dataset object
# def load_array(data_arrays, batch_size, is_train=True):
#     dataset = torch.utils.data.TensorDataset(*data_arrays)
#     return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)
# batch_size = 64
# data_iter = load_array(data_arrays, batch_size)
# #---------------------------------------------------------------------------------------
#
# #------------------------------transfomer----------------------------------
# class MultiHeadAttention(nn.Module):
#     def __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=False, **kwargs):
#         super(MultiHeadAttention, self).__init__(**kwargs)
#         self.num_heads = num_heads
#         self.attention = DotProductAttention(dropout)
#         self.w_q = nn.Linear(query_size, num_hiddens, bias=bias)
#         self.w_k = nn.Linear(key_size, num_hiddens, bias=bias)
#         self.w_v = nn.Linear(value_size, num_hiddens, bias=bias)
#         self.w_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)
#     def forward(self, queries, keys, values, valid_lens):
#         queries = transpose_qkv(self.w_q(queries), self.num_heads)
#         keys = transpose_qkv(self.w_k(keys), self.num_heads)
#         values = transpose_qkv(self.w_v(values), self.num_heads)
#         if valid_lens is not None:
#             valid_lens = torch.repeat_interleave(valid_lens, repeats = self.num_heads, dim=0)
#         output = self.attention(queries, keys, values, valid_lens)
#         output_concat = transpose_output(output, self.num_heads)
#         return self.w_o(output_concat)
#
# # Function to transpose the linearly transformed query key and values
# def transpose_qkv(X, num_heads):
#     X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)
#     X = X.permute(0, 2, 1, 3)
#     return X.reshape(-1, X.shape[2], X.shape[3])
#
# # For output formatting
# def transpose_output(X, num_heads):
#     X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])
#     X = X.permute(0, 2, 1, 3)
#     return X.reshape(X.shape[0], X.shape[1], -1)
#
# # The dot product attention scoring function
# class DotProductAttention(nn.Module):
#     def __init__(self, dropout, **kwargs):
#         super(DotProductAttention, self).__init__(**kwargs)
#         self.dropout = nn.Dropout(dropout)
#     def forward(self, queries, keys, values, valid_lens=None):
#         d = queries.shape[-1]
#         scores = torch.bmm(queries, keys.transpose(1, 2))/math.sqrt(d)
#         self.attention_weights = masked_softmax(scores, valid_lens)
#         return torch.bmm(self.dropout(self.attention_weights), values)
# # Here masking is used so that irrelevant padding tokens are not considered
# # while calculations
#
# def sequence_mask(X, valid_len, value=0):
#     maxlen = X.size(1)
#     mask = torch.arange((maxlen), dtype=torch.float32)[None, :] < valid_len[:, None]    #device=X.device
#     X[~mask] = value
#     return X
# # the irrelevant tokens are given a very small negative value which gets
# # ignored in the subsequent calculations
# def masked_softmax(X, valid_lens):
#     if valid_lens is None:
#         return nn.functional.softmax(X, dim=-1)
#     else:
#         shape = X.shape
#         if valid_lens.dim() == 1:
#             valid_lens = torch.repeat_interleave(valid_lens, shape[1])
#         else:
#             valid_lens = valid_lens.reshape(-1)
#         X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)
#         return nn.functional.softmax(X.reshape(shape), dim=-1)
#
#
# class PositionWiseFFN(nn.Module):
#     def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_output, **kwargs):
#         super(PositionWiseFFN, self).__init__(**kwargs)
#         self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)
#         self.relu = nn.ReLU()
#         self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_output)
#     def forward(self, X):
#         return self.dense2(self.relu(self.dense1(X)))
#
#
# class PositionalEncoding(nn.Module):
#     def __init__(self, num_hiddens, dropout, max_len=1000):
#         super(PositionalEncoding, self).__init__()
#         self.dropout = nn.Dropout(dropout)
#         self.P = torch.zeros((1, max_len, num_hiddens))
#         X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(10000, torch.arange(0, num_hiddens, 2,
#                                                                                                       dtype=torch.float32) / num_hiddens)
#         self.P[:, :, 0::2] = torch.sin(X)
#         self.P[:, :, 1::2] = torch.cos(X)
#
#     def forward(self, X):
#         X = X + self.P[:, :X.shape[1], :].to(X.device)
#         return self.dropout(X)
#
#
# class AddNorm(nn.Module):
#     def __init__(self, normalized_shape, dropout):
#         super(AddNorm, self).__init__()
#         self.norm = nn.LayerNorm(normalized_shape)
#         self.dropout = nn.Dropout(dropout)
#
#     def forward(self, x, sublayer_output):
#         return self.norm(x + self.dropout(sublayer_output))
#
#
# # class for the block structure within
# class EncoderBlock(nn.Module):
#     def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input,
#                  ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs):
#         super(EncoderBlock, self).__init__(**kwargs)
#         self.attention = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias)
#         self.addnorm1 = AddNorm(norm_shape, dropout)
#         self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)
#         self.addnorm2 = AddNorm(norm_shape, dropout)
#
#     def forward(self, X, valid_lens):
#         Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
#         return self.addnorm2(Y, self.ffn(Y))
#
#
# # the main encoder class
# class TransformerEncoder(nn.Module):
#     def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input,
#                  ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs):
#         super(TransformerEncoder, self).__init__(**kwargs)
#         self.num_hiddens = num_hiddens
#         self.embedding = nn.Embedding(vocab_size, num_hiddens)
#         self.pos_encoding = PositionalEncoding(num_hiddens, dropout)
#         self.blks = nn.Sequential()
#         for i in range(num_layers):
#             self.blks.add_module("block" + str(i),
#                                  EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input,
#                                               ffn_num_hiddens, num_heads, dropout, use_bias))
#
#     def forward(self, X, valid_lens, *args):
#         X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
#         self.attention_weights = [None] * len(self.blks)
#         for i, blk in enumerate(self.blks):
#             X = blk(X, valid_lens)
#             self.attention_weights[i] = blk.attention.attention.attention_weights
#         return X
#
#
# class DecoderBlock(nn.Module):
#     def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape,
#                  ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs):
#         super(DecoderBlock, self).__init__(**kwargs)
#         self.i = i
#         self.attention1 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)
#         self.addnorm1 = AddNorm(norm_shape, dropout)
#         self.attention2 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)
#         self.addnorm2 = AddNorm(norm_shape, dropout)
#         self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)
#         self.addnorm3 = AddNorm(norm_shape, dropout)
#
#     def forward(self, X, state):
#         enc_outputs, enc_valid_lens = state[0], state[1]
#         if state[2][self.i] is None:  # true when training the model
#             key_values = X
#         else:  # while decoding state[2][self.i] is decoded output of the ith block till the present time-step
#             key_values = torch.cat((state[2][self.i], X), axis=1)
#         state[2][self.i] = key_values
#         if self.training:
#             batch_size, num_steps, _ = X.shape
#             dec_valid_lens = torch.arange(1, num_steps + 1, device=X.device).repeat(batch_size, 1)
#         else:
#             dec_valid_lens = None
#         X2 = self.attention1(X, key_values, key_values, dec_valid_lens)
#         Y = self.addnorm1(X, X2)
#         Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)
#         Z = self.addnorm2(Y, Y2)
#         return self.addnorm3(Z, self.ffn(Z)), state
#
#
# # The main decoder class
# class TransformerDecoder(nn.Module):
#     def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens,
#                  norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs):
#         super(TransformerDecoder, self).__init__(**kwargs)
#         self.num_hiddens = num_hiddens
#         self.num_layers = num_layers
#         self.embedding = nn.Embedding(vocab_size, num_hiddens)
#         self.pos_encoding = PositionalEncoding(num_hiddens, dropout)
#         self.blks = nn.Sequential()
#         for i in range(num_layers):
#             self.blks.add_module("block" + str(i),
#                                  DecoderBlock(key_size, query_size, value_size,
#                                               num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
#                                               dropout, i))
#             self.dense = nn.Linear(num_hiddens, vocab_size)
#
#     def init_state(self, enc_outputs, enc_valid_lens, *args):
#         return [enc_outputs, enc_valid_lens, [None] * self.num_layers]
#
#     def forward(self, X, state):
#         X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
#         self._attention_weights = [[None] * len(self.blks) for _ in range(2)]
#         for i, blk in enumerate(self.blks):
#             X, state = blk(X, state)
#             self._attention_weights[0][i] = blk.attention1.attention.attention_weights
#             self._attention_weights[1][i] = blk.attention2.attention.attention_weights
#         return self.dense(X), state
#
#     def attention_weights(self):
#         return self._attention_weights
#
#
# class Transformer(nn.Module):
#     def __init__(self, encoder, decoder):
#         super().__init__()
#         self.encoder = encoder
#         self.decoder = decoder
#
#     def forward(self, enc_X, dec_X, *args):
#         enc_all_outputs = self.encoder(enc_X, *args)
#         dec_state = self.decoder.init_state(enc_all_outputs, *args)
#         # Return decoder output only
#         return self.decoder(dec_X, dec_state)[0]
#
# def get_device(i=0):
#     if torch.cuda.device_count() >= i+1:
#         return torch.device(f'cuda:{i}')
#     else:
#         return torch.device('cpu')
# device = get_device()
# num_hiddens, num_layers, dropout, num_steps = 32, 2, 0.1, 10
# ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4
# key_size, query_size, value_size = 32, 32, 32
# norm_shape = [32]
# encoder = TransformerEncoder(len(src_vocab), key_size, query_size, value_size, num_hiddens,norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,num_layers, dropout)
# decoder = TransformerDecoder(len(tgt_vocab), key_size, query_size, value_size, num_hiddens,norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,num_layers, dropout)
# net = Transformer(encoder, decoder)
# # nn.init.xavier_uniform_(net.weight) # for initialising the weights of the fully connected layers in the model
# for param in net.parameters():
#     if param.dim() > 1:
#         nn.init.xavier_uniform_(param)
#
#
#
# def grad_clipping(net, theta):
#     if isinstance(net, nn.Module):
#         params = [p for p in net.parameters() if p.requires_grad]
#     else:
#         params = net.params
#     norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
#     if norm > theta:
#         for param in params:
#             param.grad[:] *= theta / norm
#
# class Accumulator:
#     def __init__(self, n):
#         self.data = [0.0] * n
#
#     def add(self, *args):
#         self.data = [a + float(b) for a, b in zip(self.data, args)]
#
#     def reset(self):
#         self.data = [0.0] * len(self.data)
#
#     def __getitem__(self, idx):
#         return self.data[idx]
# #-----------------------------------------------------------------------------------
#
# #--------------------------------------loss--------------------------------------
#
# class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):
#     # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)
#     # `label` shape: (`batch_size`, `num_steps`)
#     # `valid_len` shape: (`batch_size`,)
#     def forward(self, pred, label, valid_len):
#         weights = torch.ones_like(label)
#         weights = sequence_mask(weights, valid_len)
#         self.reduction='none'
#         unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)
#         weighted_loss = (unweighted_loss * weights).mean(dim=1)
#         return weighted_loss
# #-------------------------------------------------------------------------------------
# #-------------------------------training-----------------------------------------------
# def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):
#     net.to(device)
#     optimizer = torch.optim.Adam(net.parameters(), lr=lr)
#     loss = MaskedSoftmaxCELoss()
#     net.train()
#     for epoch in range(num_epochs):
#         metric = Accumulator(2)  # Sum of training loss, no. of tokens
#         for batch in data_iter:
#             optimizer.zero_grad()
#             X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]
#             bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],device=device).reshape(-1, 1)
#             dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing
#             Y_hat = net(X, dec_input, X_valid_len)
#             l = loss(Y_hat, Y, Y_valid_len)
#             l.sum().backward()  # Make the loss scalar for `backward`
#             grad_clipping(net, 1)
#             num_tokens = Y_valid_len.sum()
#             optimizer.step()
#             with torch.no_grad():
#                 metric.add(l.sum(), num_tokens)
#         print(f"Done with epoch number: {epoch+1}") # optional step
#     print(f'loss {metric[0] / metric[1]:.3f} on {str(device)}')
# #----------------------------------------------------------------------------------------------
# lr = 0.005
# num_epochs =200
# train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device)
# #
# #
#
# def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps, device, save_attention_weights=False):
#     # Set `net` to eval mode for inference
#     net.eval()
#     src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]
#     enc_valid_len = torch.tensor([len(src_tokens)], device=device)
#     src_tokens = truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])
#     print(src_tokens)
#     enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)
#     enc_outputs = net.encoder(enc_X, enc_valid_len)
#     dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)
#     # Add the batch axis to the decoder now
#     dec_X = torch.unsqueeze(torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)
#     output_seq, attention_weight_seq = [], []
#     for _ in range(num_steps):
#         Y = net.decoder(dec_X, dec_state)[0]
#         # We use the token with the highest prediction likelihood as the input
#         # of the decoder at the next time step
#         dec_X = Y.argmax(dim=2)
#         pred = dec_X.squeeze(dim=0).type(torch.int32).item()
#         # Save attention weights
#         if save_attention_weights:
#             attention_weight_seq.append(net.decoder.attention_weights)
#             # Once the end-of-sequence token is predicted, the generation of the output sequence is complete
#         if pred == tgt_vocab['<eos>']:
#             break
#         output_seq.append(pred)
#     if len(output_seq) < 2:
#
#         if len(output_seq) == 1:
#             return ''.join(tgt_vocab.to_tokens(output_seq[0])), attention_weight_seq
#         else:
#
#             return "No output!", attention_weight_seq
#     else:
#         return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq
#
#
# # #
# # s='How is practicing gratitude helpful'
# # pred_sum, _ = predict_seq2seq(net, s, src_vocab, tgt_vocab, 30, device)
# # print("PREDICTED : {} ::=>".format(pred_sum), end='\t')
# # sample = x_test[:10]
# # actual = y_test[:10]
# # for s, a in zip(sample, actual):
# #     pred_sum, _ = predict_seq2seq(net, s, src_vocab, tgt_vocab, 10, device)
# #     print("PREDICTED : {} ::=>".format(pred_sum), end='\t')
# #     print("ACTUAL : {}".format(a))
# s='RJD MP Manoj Jha said that PM Narendra Modi must have called the special session of Parliament on the recommendation of some astrologer. "The PM believes in all thatâ€¦Don't say that you have no agenda. The agenda is very clear. We would like to see what the other agendas are. But there is nothing special about it he told ANI.'
# pred_sum, _ = predict_seq2seq(net, s, src_vocab, tgt_vocab, 30, device)
# print("PREDICTED : {} ::=>".format(pred_sum), end='\t')
# # sample = x_test[:10]
# # actual = y_test[:10]
# # for s, a in zip(sample, actual):
# #     pred_sum, _ = predict_seq2seq(net, s, src_vocab, tgt_vocab, 10, device)
# #     print("PREDICTED : {} ::=>".format(pred_sum), end='\t')
# #     print("ACTUAL : {}".format(a))

#--------------------------------third way------------------------------------------------------

#
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# import pandas as pd
# from collections import defaultdict
# import string
# import tensorflow as tf
# import re
# import os
# import time
# from tensorflow import keras
# from tensorflow.keras.layers import Dense, Input
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.models import Model
# from tensorflow.keras.callbacks import ModelCheckpoint
# from sklearn.preprocessing import OneHotEncoder
# from sklearn.model_selection import train_test_split
#
# # ENCODER_LEN = 200
# # DECODER_LEN = 70
# # BATCH_SIZE = 36
# # BUFFER_SIZE = BATCH_SIZE*8
# ENCODER_LEN = 200
# DECODER_LEN = 100
# BATCH_SIZE = 128
# BUFFER_SIZE = BATCH_SIZE*8
# # news = pd.read_csv("samples1.csv")
# #----------------------------------one way--------------------------------------------------
# # article = news['Question']
# # summary = news['Answer']
# # # news_subset = news.head(20)
# # #
# # # # Access the 'Headline' and 'Content' columns from the subset
# # # article = news_subset['Headline']
# # # summary = news_subset['Content']
# # article = article.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')
# # summary = summary.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')
# # # def preprocess(text):
# # #     text = re.sub(r"&.[1-9]+;₹"," ",text)
# # #     return text
# #
# # def preprocess(text):
# #     # Remove HTML entities like "&1234;", and the symbol ₹
# #     text = re.sub(r"&[1-9]+;|₹", " ", text)  # Updated regex for both patterns
# #     # Remove single quotes (')
# #     # text = re.sub(r"'", "", text)
# #     text = re.sub(r"'", "", text)
# #     # Remove single double quotes (") within words
# #     text = re.sub(r'(\w)"(\w)', r'\1\2', text)
# #     return text
# #
# # # def preprocess_text(text):
# # #     # Remove special characters (e.g., ₹) from words
# # #     return re.sub(r"[₹]", "", text)
# #
# # article = article.apply(lambda x: preprocess(x))
# # summary = summary.apply(lambda x: preprocess(x))
# #
# #
# #
# #
# # filters = '₹!"#$%&()*+,-./:;=?@[\\]^_`{|}~\t\n'
# # oov_token = '<unk>'
# # article_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)
# # summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)
# #
# # article_tokenizer.fit_on_texts(article)
# # summary_tokenizer.fit_on_texts(summary)
# #
# #
# # # Print the word indices
# # # print("Article Tokenizer Word Index:", article_tokenizer.word_index)
# # # print("Summary Tokenizer Word Index:", summary_tokenizer.word_index)
# #
# # def filter_nan_token(tokenizer):
# #   """Filters out the 'nan' token (if present) from the tokenizer."""
# #
# #   if 'nan' in tokenizer.word_index:  # Check if 'nan' is in the vocabulary
# #       nan_token_id = tokenizer.word_index['nan']
# #
# #       # If nan_token_id is 3, proceed to filter it out
# #       if nan_token_id == 3:
# #           # Adjust word_index, index_word, and word_counts
# #           del tokenizer.word_index['nan']
# #           tokenizer.index_word = {
# #               index: word for index, word in tokenizer.index_word.items()
# #               if index != nan_token_id
# #           }
# #           del tokenizer.word_counts['nan']
# #
# #           # Re-assign token IDs (shift down by 1 for tokens > nan_token_id)
# #           for word, index in list(tokenizer.word_index.items()):
# #               if index > nan_token_id:
# #                   tokenizer.word_index[word] = index - 1
# #
# #           tokenizer.index_word = {
# #               index: word for word, index in tokenizer.word_index.items()
# #           }
# #
# # # Filter 'nan' token from both tokenizers
# # filter_nan_token(article_tokenizer)
# # filter_nan_token(summary_tokenizer)
# # inputs = article_tokenizer.texts_to_sequences(article)
# # targets = summary_tokenizer.texts_to_sequences(summary)
# #
# #
# # ENCODER_VOCAB = len(article_tokenizer.word_index) + 1
# # DECODER_VOCAB = len(summary_tokenizer.word_index) + 1
# # # print(ENCODER_VOCAB, DECODER_VOCAB)
# #
# # inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')
# # targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')
# #
# # print("Article Tokenizer Word Index (Word to Token):")
# # print(article_tokenizer.word_index)
# #
# # print("\nArticle Tokenizer Index Word (Token to Word):")
# # print(article_tokenizer.index_word)
# # inputs = tf.cast(inputs, dtype=tf.int64)
# # targets = tf.cast(targets, dtype=tf.int64)
# #
# # dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
# #----------------------------------------------------------------------------------------
#
# #--------------------------------------proccessing numbers-----------------------------------
#
#
# news = pd.read_csv("average_training_dataset_modified.csv")  # Replace with your file path
# article = news['Input']
# summary = news['Output']
#
# # Add SOS and EOS tokens
# article = article.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')
# summary = summary.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')
# #
# #
# # # Preprocessing function to separate numbers and operators
# # def preprocess(text):
# #     # Add spaces around operators for proper tokenization
# #     text = re.sub(r"(\d+)([+])(\d+)", r"\1 \2 \3", text)  # e.g., "12+14" → "12 + 14"
# #     return text
#
#
# def preprocess(text):
#     # Tokenize numbers and separate them into digits
#     text = re.sub(r'(\d+)', lambda match: ' '.join(list(match.group(0))), text)  # e.g., "64" → "6 4"
#
#     # Add spaces around operators for proper tokenization
#     text = re.sub(r"(\+|\-|\*|/|=)", r" \1 ", text)  # Add spaces around operators
#
#     # Tokenize spaces between words
#     text = re.sub(r'\s+', ' ', text).strip()  # Normalize spaces
#
#     # Make text lowercase
#     text = text.lower()
#
#     return text
#
#
# # Apply the preprocessing function to the input and output text
# # article = article.apply(preprocess)
# # summary = summary.apply(preprocess)
#
# # Output the processed text for inspection
#
#
# # Apply preprocessing
# article = article.apply(preprocess)
# summary = summary.apply(preprocess)
#
# print("Processed Article:", article.head())
# print("Processed Summary:", summary.head())
# # print(article)
# # print(summary)
#
# # Tokenization
# filters = '₹!"#$%&(),-.:;?@[\\]^_`{|}~\t\n'
# # filters = '₹!"#$%&(),-./:;=?@[\\]^_`{|}~\t\n'
#
# oov_token = '<unk>'
#
# article_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)
# summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)
#
# # Fit tokenizers on preprocessed data
# article_tokenizer.fit_on_texts(article)
# summary_tokenizer.fit_on_texts(summary)
#
# # Convert text to sequences
# inputs = article_tokenizer.texts_to_sequences(article)
# targets = summary_tokenizer.texts_to_sequences(summary)
# # print(inputs)
#
# # Vocabulary sizes
# ENCODER_VOCAB = len(article_tokenizer.word_index) + 1
# DECODER_VOCAB = len(summary_tokenizer.word_index) + 1
#
# # Pad sequences
# ENCODER_LEN = 70  # Maximum length of input sequences
# DECODER_LEN = 40   # Maximum length of output sequences
#
# inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')
# targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')
# input_texts_padded = [' '.join([article_tokenizer.index_word.get(token, '') for token in seq]) for seq in inputs]
# target_texts_padded = [' '.join([summary_tokenizer.index_word.get(token, '') for token in seq]) for seq in targets]
#
# # Print tokenizers' word index
# print("Article Tokenizer Word Index (Word to Token):")
# print(article_tokenizer.word_index)
#
# print("\nSummary Tokenizer Word Index (Word to Token):")
# print(summary_tokenizer.word_index)
#
# # print("\nSample Input Sequence:", inputs[0])
# # print("\nSample Input Sequence:", inputs[1])
# #
# # print("Sample Output Sequence:", targets[0])
# # print("Sample Output Sequence:", targets[1])
#
# word_43 = article_tokenizer.index_word.get(43, "Not found")
# word_35 = article_tokenizer.index_word.get(35, "Not found")
# word_72 = summary_tokenizer.index_word.get(35, "Not found")
#
# # print("Word at index 43:", word_43)
# # print("Word at index 35:", word_35)
#
# inputs = tf.cast(inputs, dtype=tf.int64)
#
# targets = tf.cast(targets, dtype=tf.int64)
# dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
#
#
# #-------------------------------------------------------------------------------------------
#
#
# #--------------------------transformer---------------------------------------
#
#
# def get_angles(position, i, d_model):
#     angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
#     return position * angle_rates
#
# def positional_encoding(position, d_model):
#     angle_rads = get_angles(
#         np.arange(position)[:, np.newaxis],
#         np.arange(d_model)[np.newaxis, :],
#         d_model
#     )
#
#     angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
#
#     angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
#
#     pos_encoding = angle_rads[np.newaxis, ...]
#
#     return tf.cast(pos_encoding, dtype=tf.float32)
#
# def create_padding_mask(seq):
#     seq = tf.cast(tf.math.equal(seq, 0), tf.float32)
#     return seq[:, tf.newaxis, tf.newaxis, :]
#
# def create_look_ahead_mask(size):
#     mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
#     return mask
#
# def scaled_dot_product_attention(q, k, v, mask):
#     matmul_qk = tf.matmul(q, k, transpose_b=True)
#
#     dk = tf.cast(tf.shape(k)[-1], tf.float32)
#     scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
#     if mask is not None:
#         scaled_attention_logits += (mask * -1e9)
#
#     attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
#
#     output = tf.matmul(attention_weights, v)
#     return output, attention_weights
#
#
# class MultiHeadAttention(tf.keras.layers.Layer):
#     def __init__(self, d_model, num_heads):
#         super(MultiHeadAttention, self).__init__()
#         self.num_heads = num_heads
#         self.d_model = d_model
#
#         assert d_model % self.num_heads == 0
#
#         self.depth = d_model // self.num_heads
#
#         self.wq = tf.keras.layers.Dense(d_model)
#         self.wk = tf.keras.layers.Dense(d_model)
#         self.wv = tf.keras.layers.Dense(d_model)
#
#         self.dense = tf.keras.layers.Dense(d_model)
#
#     def split_heads(self, x, batch_size):
#         x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
#         return tf.transpose(x, perm=[0, 2, 1, 3])
#
#     def call(self, v, k, q, mask):
#         batch_size = tf.shape(q)[0]
#
#         q = self.wq(q)
#         k = self.wk(k)
#         v = self.wv(v)
#
#         q = self.split_heads(q, batch_size)
#         k = self.split_heads(k, batch_size)
#         v = self.split_heads(v, batch_size)
#
#         scaled_attention, attention_weights = scaled_dot_product_attention(
#             q, k, v, mask)
#
#         # tf.print("Attention weights matrix structure (example 1, head 1):", attention_weights[2,3], summarize=-1)
#
#         scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])
#
#         concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))
#         output = self.dense(concat_attention)
#         # print('output')
#         # print(output)
#         # tf.print("ouuput matrix structure (example 1, head 1):", output)
#
#
#         return output, attention_weights
#
#
# def point_wise_feed_forward_network(d_model, dff):
#     return tf.keras.Sequential([
#         tf.keras.layers.Dense(dff, activation='relu'),
#         tf.keras.layers.Dense(d_model)
#     ])
#
#
# class EncoderLayer(tf.keras.layers.Layer):
#     def __init__(self, d_model, num_heads, dff, rate=0.1):
#         super(EncoderLayer, self).__init__()
#
#         self.mha = MultiHeadAttention(d_model, num_heads)
#         self.ffn = point_wise_feed_forward_network(d_model, dff)
#
#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#         self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#
#         self.dropout1 = tf.keras.layers.Dropout(rate)
#         self.dropout2 = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, training, mask):
#         attn_output, _ = self.mha(x, x, x, mask)
#         attn_output = self.dropout1(attn_output, training=training)
#         out1 = self.layernorm1(x + attn_output)
#
#         ffn_output = self.ffn(out1)
#         ffn_output = self.dropout2(ffn_output, training=training)
#         out2 = self.layernorm2(out1 + ffn_output)
#
#         return out2
#
#
# class DecoderLayer(tf.keras.layers.Layer):
#     def __init__(self, d_model, num_heads, dff, rate=0.1):
#         super(DecoderLayer, self).__init__()
#
#         self.mha1 = MultiHeadAttention(d_model, num_heads)
#         self.mha2 = MultiHeadAttention(d_model, num_heads)
#
#         self.ffn = point_wise_feed_forward_network(d_model, dff)
#
#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#         self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#         self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
#
#         self.dropout1 = tf.keras.layers.Dropout(rate)
#         self.dropout2 = tf.keras.layers.Dropout(rate)
#         self.dropout3 = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
#         attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)
#         attn1 = self.dropout1(attn1, training=training)
#         out1 = self.layernorm1(attn1 + x)
#
#         attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)
#         attn2 = self.dropout2(attn2, training=training)
#         out2 = self.layernorm2(attn2 + out1)
#
#         ffn_output = self.ffn(out2)
#         ffn_output = self.dropout3(ffn_output, training=training)
#         out3 = self.layernorm3(ffn_output + out2)
#
#         return out3, attn_weights_block1, attn_weights_block2
#
#
# class Encoder(tf.keras.layers.Layer):
#     def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):
#         super(Encoder, self).__init__()
#
#         self.d_model = d_model
#         self.num_layers = num_layers
#
#         self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
#         self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)
#
#         self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]
#
#         self.dropout = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, training, mask):
#         seq_len = tf.shape(x)[1]
#
#         x = self.embedding(x)
#         x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
#         x += self.pos_encoding[:, :seq_len, :]
#
#         x = self.dropout(x, training=training)
#
#         for i in range(self.num_layers):
#             x = self.enc_layers[i](x, training=training, mask=mask)
#
#         return x
#
#
# class Decoder(tf.keras.layers.Layer):
#
#     def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):
#         super(Decoder, self).__init__()
#
#         self.d_model = d_model
#         self.num_layers = num_layers
#
#         self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)
#         self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)
#
#         self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]
#         self.dropout = tf.keras.layers.Dropout(rate)
#
#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
#         seq_len = tf.shape(x)[1]
#         attention_weights = {}
#
#         x = self.embedding(x)
#         x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
#         x += self.pos_encoding[:, :seq_len, :]
#
#         x = self.dropout(x, training=training)
#
#         for i in range(self.num_layers):
#             x, block1, block2 = self.dec_layers[i](x, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)
#
#             attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1
#             attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2
#
#         return x, attention_weights
#
# class Transformer(tf.keras.Model):
#     def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input,
#                      pe_target, rate=0.1):
#         super(Transformer, self).__init__()
#
#         self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)
#
#         self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)
#
#         self.final_layer = tf.keras.layers.Dense(target_vocab_size)
#
#     def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):
#         enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)
#
#         dec_output, attention_weights = self.decoder(tar, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)
#
#         final_output = self.final_layer(dec_output)
#
#         return final_output, attention_weights
#
# # num_layers = 6
# # d_model = 128
# # dff = 512
# # num_heads = 8
# # dropout_rate = 0.3
# # EPOCHS = 90
#
# num_layers = 6
# # d_model = 128
# d_model=128
# dff = 512
# num_heads = 8
# dropout_rate = 0.2
# EPOCHS = 10
#
# class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
#     def __init__(self, d_model, warmup_steps=4000):
#         super(CustomSchedule, self).__init__()
#
#         self.d_model = d_model
#         self.d_model = tf.cast(self.d_model, tf.float32)
#
#         self.warmup_steps = warmup_steps
#
#
#     def __call__(self, step):
#         step = tf.cast(step, dtype=tf.float32)
#         arg1 = tf.math.rsqrt(step)
#         arg2 = step * (self.warmup_steps ** -1.5)
#
#         return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)
#
#
# learning_rate = CustomSchedule(d_model)
# #learning_rate=.01
#
# optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)
# temp_learning_rate_schedule = CustomSchedule(d_model)
#
# plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))
# plt.ylabel("Learning Rate")
# plt.xlabel("Train Step")
#
#
# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
# def loss_function(real, pred):
#     mask = tf.math.logical_not(tf.math.equal(real, 0))
#     loss_ = loss_object(real, pred)
#
#     mask = tf.cast(mask, dtype=loss_.dtype)
#     loss_ *= mask
#
#     return tf.reduce_sum(loss_)/tf.reduce_sum(mask)
#
#
# def accuracy_function(real, pred):
#     accuracies = tf.equal(real, tf.argmax(pred, axis=2))
#     accuracies = tf.cast(accuracies, dtype= tf.float32)
#     mask = tf.math.logical_not(tf.math.equal(real, 0))
#     # accuracies = tf.math.logical_and(mask, accuracies)
#     accuracies = tf.cast(accuracies, dtype=tf.float32)
#     mask = tf.cast(mask, dtype=tf.float32)
#     return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)
#
# train_loss = tf.keras.metrics.Mean(name='train_loss')
# train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')
#
# transformer = Transformer(
#     num_layers=num_layers,
#     d_model=d_model,
#     num_heads=num_heads,
#     dff=dff,
#     input_vocab_size=ENCODER_VOCAB,
#     target_vocab_size=DECODER_VOCAB,
#     pe_input=1000,
#     pe_target=1000,
#     rate=dropout_rate)
#
#
# def create_masks(inp, tar):
#     enc_padding_mask = create_padding_mask(inp)
#     dec_padding_mask = create_padding_mask(inp)
#
#     look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])
#     dec_target_padding_mask = create_padding_mask(tar)
#     combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)
#
#     return enc_padding_mask, combined_mask, dec_padding_mask
#
# # checkpoint_path = "/content/drive/MyDrive/Dataset/checkpoints"
# checkpoint_path = "checkpoints"
# ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)
# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)
#
# if ckpt_manager.latest_checkpoint:
#     ckpt.restore(ckpt_manager.latest_checkpoint)
#     print ('Latest checkpoint restored!!')
#
# @tf.function
# def train_step(inp, tar):
#     tar_inp = tar[:, :-1]
#     tar_real = tar[:, 1:]
#
#     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)
#
#     with tf.GradientTape() as tape:
#         predictions, _ = transformer(
#             inp,
#             tar_inp,
#             training=True,
#             enc_padding_mask=enc_padding_mask,
#             look_ahead_mask=combined_mask,
#             dec_padding_mask=dec_padding_mask
#         )
#         loss = loss_function(tar_real, predictions)
#
#     gradients = tape.gradient(loss, transformer.trainable_variables)
#     optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
#     train_loss(loss)
#     train_accuracy(accuracy_function(tar_real, predictions))
#
#
# for epoch in range(EPOCHS):
#     start = time.time()
#
#     train_loss.reset_states()
#
#     for (batch, (inp, tar)) in enumerate(dataset):
#         train_step(inp, tar)
#
#         if batch % 100 == 0:
#             print(
#                 f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')
#
#     if (epoch + 1) % 5 == 0:
#         ckpt_save_path = ckpt_manager.save()
#         print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))
# #
# #     print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')
# #     print('Time taken for 1 epoch: {} secs\n'.format(time.time() - start))
# # creating embedding solving the problem of unseen number token
#
# # class NumericEmbedding(nn.Module):
# #     def __init__(self, embedding_dim=32):
# #         super().__init__()
# #         self.embedding = nn.Embedding(10, embedding_dim)  # 0-9 digits
# #         self.linear = nn.Linear(embedding_dim * 5, embedding_dim)  # 5-digit numbers
# #
# #     def forward(self, number):
# #         digits = [int(d) for d in str(number).zfill(5)]  # Pad number to 5 digits
# #         embeddings = self.embedding(torch.tensor(digits))  # Get embeddings
# #         return self.linear(embeddings.view(1, -1))  # Flatten & process
# #
# # num_encoder = NumericEmbedding()
# # print(num_encoder(34))  # Outputs a co
#
# def evaluate(input_article):
#     # Tokenize and pad the input article
#     print(input_article)
#     input_article = re.sub(r'(\d)', r'\1 ', input_article)  # Adds space after each digit
#     input_article = input_article.replace("  ", " ").strip()
#
#     input_article = tf.convert_to_tensor(input_article, dtype=tf.string)  # Ensure it's a TensorFlow string
#     input_article = input_article.numpy().decode("utf-8")
#         # inputs=inputArr.apply(preprocess(input_article))
#         # inputs = input_series.apply(lambda x: preprocess(x).split())
#         #
#     input_article = article_tokenizer.texts_to_sequences([input_article])
#
#
#     # inputs=input_article.apply(preprocess_and_split_numbers(input_article))
#
#     # input_article = article_tokenizer.texts_to_sequences([inputs])
#     # input_article=[[3,4,5,6,7,8,9,10,14,15,18,19,11,15,16,17,12]]
#     # input_article = [[2,3,4,9,10,5,13,14,6]]
#     print("tokens")
#     print(input_article)
#
#     input_article = tf.keras.preprocessing.sequence.pad_sequences(
#         input_article, maxlen=ENCODER_LEN, padding='post', truncating='post'
#     )
#     print("Inputs")
#     print(input_article)
#     encoder_input = tf.expand_dims(input_article[0], 0)
#     print (encoder_input)
#     decoder_input = [summary_tokenizer.word_index['<sos>']]
#     output = tf.expand_dims(decoder_input, 0)
#
#     for i in range(DECODER_LEN):
#         # Create masks
#         enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)
#
#         # Get predictions from the transformer
#         predictions, attention_weights = transformer(
#             encoder_input,
#             output,
#             training=False,
#             enc_padding_mask=enc_padding_mask,
#             look_ahead_mask=combined_mask,
#             dec_padding_mask=dec_padding_mask
#         )
#         predictions = predictions[:, -1:, :]  # Take the last predicted token
#         predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)
#         if predicted_id == summary_tokenizer.word_index['<eos>']:
#             return tf.squeeze(output, axis=0), attention_weights
#         # Append the predicted token to the output
#         output = tf.concat([output, predicted_id], axis=-1)
#     return tf.squeeze(output, axis=0), attention_weights
#
# def summarize(input_article):
#     summarized = evaluate(input_article=input_article)[0].numpy()
#     summarized = np.expand_dims(summarized[1:], 0)  # Remove the <sos> token
#     return summary_tokenizer.sequences_to_texts(summarized)[0]
#
# # Test the function
# print('Output:')
# print(summarize(
#     '<SOS> Find the average of 489 and 328 <EOS>'))
# # def preprocess_and_split_numbers(input_text):
# #     # Split numbers into their individual digits
# #     # This regular expression finds numbers and splits them into individual characters
# #     input_text = re.sub(r'(\d)', r' \1 ', input_text)  # Add space around each digit
# #     return input_text



#------------------------------------embedding  -----========================================

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import defaultdict
import string
import tensorflow as tf
import re
import os
import time
from tensorflow import keras
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split

# ENCODER_LEN = 200
# DECODER_LEN = 70
# BATCH_SIZE = 36
# BUFFER_SIZE = BATCH_SIZE*8
ENCODER_LEN = 200
DECODER_LEN = 100
BATCH_SIZE = 128
BUFFER_SIZE = BATCH_SIZE*8
# news = pd.read_csv("samples1.csv")
# #----------------------------------one way--------------------------------------------------
#

news = pd.read_csv("addition_dataset_v2.csv")  # Replace with yfiltered_positive_average_samplesour file path
article = news['Input']
summary = news['Output']

# Add SOS and EOS tokens

#---------------------------------------invalid for addition-----------------------------
article = article.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')
summary = summary.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')


def preprocess(text):
    # Add spaces around operators for correct tokenization
    text = re.sub(r"(\d+|\+|\-|\*|/|=)", r" \1 ", text)
    text = re.sub(r'\s+', ' ', text).strip()  # Normalize spaces
    return text

# Example usage:
# text = "90+88"
# tokens = preprocess(text).split()  # Tokenize by splitting on spaces
# Apply preprocessing
article = article.apply(lambda x: preprocess(x).split())
summary = summary.apply(lambda x: preprocess(x).split())



# Tokenization
filters = '₹!"#$%&()*,-.:;?@[\\]^_`{|}~\t\n'
oov_token = '<unk>'

article_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)
summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)

# Fit tokenizers on preprocessed data
article_tokenizer.fit_on_texts(article)
summary_tokenizer.fit_on_texts(summary)

# Convert text to sequences
inputs = article_tokenizer.texts_to_sequences(article)
targets = summary_tokenizer.texts_to_sequences(summary)
# print(inputs)
print("Processed Article:", inputs)
print("Processed Summary:", targets)
# Vocabulary sizes
ENCODER_VOCAB = len(article_tokenizer.word_index) + 1
DECODER_VOCAB = len(summary_tokenizer.word_index) + 1

# Pad sequences
ENCODER_LEN = 70  # Maximum length of input sequences
DECODER_LEN = 40   # Maximum length of output sequences

inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')
targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')
input_texts_padded = [' '.join([article_tokenizer.index_word.get(token, '') for token in seq]) for seq in inputs]
target_texts_padded = [' '.join([summary_tokenizer.index_word.get(token, '') for token in seq]) for seq in targets]

# Print tokenizers' word index
print("Article Tokenizer Word Index (Word to Token):")
print(article_tokenizer.word_index)

print("\nSummary Tokenizer Word Index (Word to Token):")
print(summary_tokenizer.word_index)

word_43 = article_tokenizer.index_word.get(43, "Not found")
word_35 = article_tokenizer.index_word.get(35, "Not found")
word_72 = summary_tokenizer.index_word.get(35, "Not found")


inputs = tf.cast(inputs, dtype=tf.int64)

targets = tf.cast(targets, dtype=tf.int64)
#---------------------------------------------------------------------------------------------

dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

#--------------------------transformer---------------------------------------


def get_angles(position, i, d_model):
    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
    return position * angle_rates

def positional_encoding(position, d_model):
    angle_rads = get_angles(
        np.arange(position)[:, np.newaxis],
        np.arange(d_model)[np.newaxis, :],
        d_model
    )

    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])

    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])

    pos_encoding = angle_rads[np.newaxis, ...]

    return tf.cast(pos_encoding, dtype=tf.float32)

def create_padding_mask(seq):
    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)
    return seq[:, tf.newaxis, tf.newaxis, :]

def create_look_ahead_mask(size):
    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
    return mask

def scaled_dot_product_attention(q, k, v, mask):
    matmul_qk = tf.matmul(q, k, transpose_b=True)

    dk = tf.cast(tf.shape(k)[-1], tf.float32)
    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
    # print('loggits')
    # tf.print(scaled_attention_logits)

    if mask is not None:
        scaled_attention_logits += (mask * -1e9)

    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)

    output = tf.matmul(attention_weights, v)
    return output, attention_weights


class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model

        assert d_model % self.num_heads == 0

        self.depth = d_model // self.num_heads

        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)

        self.dense = tf.keras.layers.Dense(d_model)

    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, v, k, q, mask):
        batch_size = tf.shape(q)[0]

        q = self.wq(q)
        k = self.wk(k)
        v = self.wv(v)

        q = self.split_heads(q, batch_size)
        k = self.split_heads(k, batch_size)
        v = self.split_heads(v, batch_size)

        scaled_attention, attention_weights = scaled_dot_product_attention(
            q, k, v, mask)

        # tf.print("Attention weights matrix structure (example 1, head 1):", attention_weights[2,3], summarize=-1)

        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])

        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))
        output = self.dense(concat_attention)
        return output, attention_weights


def point_wise_feed_forward_network(d_model, dff):
    return tf.keras.Sequential([
        tf.keras.layers.Dense(dff, activation='relu'),
        tf.keras.layers.Dense(d_model)
    ])


class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(EncoderLayer, self).__init__()

        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ffn = point_wise_feed_forward_network(d_model, dff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, x, training, mask):
        attn_output, _ = self.mha(x, x, x, mask)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)

        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)

        return out2

# class PositionalLogEmbedding(tf.keras.layers.Layer):
#     def __init__(self, vocab_size, embedding_dim, max_length=100):
#         super(PositionalLogEmbedding, self).__init__()
#         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
#         self.positional_encoding = self.add_weight(
#             shape=(max_length, embedding_dim),
#             initializer="random_normal",
#             trainable=True
#         )
#         self.norm = tf.keras.layers.LayerNormalization()
#
#     def call(self, inputs):
#         embedded = self.embedding(inputs)
#         log_embedded = tf.math.log1p(embedded)
#
#         positions = tf.range(tf.shape(inputs)[-1])  # Token positions
#         position_encoding = tf.gather(self.positional_encoding, positions)
#
#         return self.norm(log_embedded + position_encoding)

# class ExpEmbedding(tf.keras.layers.Layer):
#     def __init__(self, vocab_size, embedding_dim):
#         super(ExpEmbedding, self).__init__()
#         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
#         self.norm = tf.keras.layers.LayerNormalization()
#
#     def call(self, inputs):
#         embedded = self.embedding(inputs)
#         log_embedded = tf.math.log1p(embedded)  # Apply log(1 + x) to prevent log(0) issues
#         return self.norm(log_embedded)

#
    # class ExpEmbedding(tf.keras.layers.Layer):
    #     def __init__(self, vocab_size, embedding_dim, token_to_word_map):
    #         super(ExpEmbedding, self).__init__()
    #
    #         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    #         self.norm = tf.keras.layers.LayerNormalization()
    #         self.token_to_word_map = token_to_word_map  # Dictionary mapping tokens to words
    #
    #     def call(self, inputs):
    #         # Convert token IDs back to words
    #         words = tf.gather(self.token_to_word_map, inputs)
    #
    #         # Identify numeric tokens
    #         is_numeric = tf.map_fn(lambda x: tf.strings.regex_full_match(x, "\\d+"), words, dtype=tf.bool)
    #
    #         # Standard embedding for words
    #         word_embedded = self.embedding(inputs)
    #
    #         # Exponential embedding for numbers
    #         number_embedded = tf.exp(self.embedding(inputs))

            # Select appropriate embeddings
            # embedded = tf.where(tf.expand_dims(is_numeric, -1), number_embedded, word_embedded)
            #
            # return self.norm(embedded)


class ExpEmbedding(tf.keras.layers.Layer):
    def __init__(self, vocab_size, embedding_dim, token_to_word_map):
        super(ExpEmbedding, self).__init__()
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.norm = tf.keras.layers.LayerNormalization()

        # Convert token_to_word_map dictionary to a TensorFlow lookup table
        # self.token_lookup = tf.lookup.StaticHashTable(
        #     tf.lookup.KeyValueTensorInitializer(
        #         keys=tf.constant(list(token_to_word_map.keys()), dtype=tf.int32),
        #         values=tf.constant(list(token_to_word_map.values()), dtype=tf.string)
        #     ),
        self.token_lookup = tf.lookup.StaticHashTable(
            tf.lookup.KeyValueTensorInitializer(
                keys=tf.constant(list(token_to_word_map.keys()), dtype=tf.int64),  # Changed to int64
                values=tf.constant(list(token_to_word_map.values()), dtype=tf.string)
            ),

            default_value=tf.constant("", dtype=tf.string)  # Default empty string if key is not found
        )

    def call(self, inputs):
        # Convert token IDs back to words
        print('Inputtttttttttttttt')
        tensor_int64 = tf.cast(inputs, tf.int64)
        print(inputs)

        words = self.token_lookup.lookup(tensor_int64)

        # Identify numeric tokens (only numbers should get the exponential embedding)
        # is_numeric = tf.strings.regex_full_match(words, r"\d+")
        #
        # # Standard embedding for word tokens
        # word_embedded = self.embedding(inputs)
        #
        # # Exponential embedding for number tokens
        # number_embedded = tf.where(is_numeric, tf.exp(word_embedded), word_embedded)
        #
        # return self.norm(number_embedded)
        is_numeric = tf.strings.regex_full_match(words, r"\d+")

        # Expand dims for broadcasting
        is_numeric = tf.expand_dims(is_numeric, axis=-1)  # Shape: [batch_size, seq_len, 1]

        # Standard embedding for word tokens
        word_embedded = self.embedding(tensor_int64)  # Shape: [batch_size, seq_len, embedding_dim]
        #         log_embedded = tf.math.log1p(embedded)  # Apply log(1 + x) to prevent log(0) issues

        # Exponential embedding for number tokens
        # number_embedded = tf.where(is_numeric, tf.exp(word_embedded), word_embedded)
        number_embedded = tf.where(is_numeric,tf.exp(word_embedded), word_embedded)

        return self.norm(number_embedded)

# class ExpEmbedding(tf.keras.layers.Layer):
#     def __init__(self, vocab_size, embedding_dim, token_to_word_map, **kwargs):
#         super(ExpEmbedding, self).__init__(**kwargs)
#         self.vocab_size = vocab_size
#         self.embedding_dim = embedding_dim
#         self.token_to_word_map = token_to_word_map  # Store for reconstruction
#         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
#         self.norm = tf.keras.layers.LayerNormalization()
#
#         self.token_lookup = tf.lookup.StaticHashTable(
#             tf.lookup.KeyValueTensorInitializer(
#                 keys=tf.constant(list(token_to_word_map.keys()), dtype=tf.int64),
#                 values=tf.constant(list(token_to_word_map.values()), dtype=tf.string)
#             ),
#             default_value=tf.constant("", dtype=tf.string)
#         )
#
#     def call(self, inputs):
#         tensor_int64 = tf.cast(inputs, tf.int64)
#         words = self.token_lookup.lookup(tensor_int64)
#         # print(words)
#
#
#         is_numeric = tf.strings.regex_full_match(words, r"\d+")
#         is_numeric = tf.expand_dims(is_numeric, axis=-1)
#         word_embedded = self.embedding(tensor_int64)
#         number_embedded = tf.where(is_numeric, tf.math.log1p(word_embedded), word_embedded)
#         return self.norm(number_embedded)
#
#     def get_config(self):
#         config = super().get_config()
#         config.update({
#             "vocab_size": self.vocab_size,
#             "embedding_dim": self.embedding_dim,
#             "token_to_word_map": self.token_to_word_map  # Include for reconstruction
#         })
#         return config
#
#     @classmethod
#     def from_config(cls, config):
#         return cls(**config)

class DecoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(DecoderLayer, self).__init__()

        self.mha1 = MultiHeadAttention(d_model, num_heads)
        self.mha2 = MultiHeadAttention(d_model, num_heads)

        self.ffn = point_wise_feed_forward_network(d_model, dff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)
        self.dropout3 = tf.keras.layers.Dropout(rate)

    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)
        attn1 = self.dropout1(attn1, training=training)
        out1 = self.layernorm1(attn1 + x)

        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)
        attn2 = self.dropout2(attn2, training=training)
        out2 = self.layernorm2(attn2 + out1)

        ffn_output = self.ffn(out2)
        ffn_output = self.dropout3(ffn_output, training=training)
        out3 = self.layernorm3(ffn_output + out2)

        return out3, attn_weights_block1, attn_weights_block2


class Encoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):
        super(Encoder, self).__init__()
        self.d_model = d_model
        self.num_layers = num_layers

        # self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
        self.embedding = ExpEmbedding(input_vocab_size, d_model,article_tokenizer.index_word)  # Pass vocab_size and d_model

        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)

        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]

        self.dropout = tf.keras.layers.Dropout(rate)

    def call(self, x, training, mask):
        seq_len = tf.shape(x)[1]

        x = self.embedding(x)
        # print("encoder embedding")
        # print(x)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]

        x = self.dropout(x, training=training)

        for i in range(self.num_layers):
            x = self.enc_layers[i](x, training=training, mask=mask)

        return x


class Decoder(tf.keras.layers.Layer):

    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):
        super(Decoder, self).__init__()
        self.d_model = d_model
        self.num_layers = num_layers
        # self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)
        self.embedding = ExpEmbedding(target_vocab_size, d_model,summary_tokenizer.index_word)  # Pass vocab_size and d_model

        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)
        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(rate)

    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
        seq_len = tf.shape(x)[1]
        attention_weights = {}
        x = self.embedding(x)
        # print("deocder embedding")
        # print(x)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]

        x = self.dropout(x, training=training)

        for i in range(self.num_layers):
            x, block1, block2 = self.dec_layers[i](x, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)

            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1
            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2

        return x, attention_weights

class Transformer(tf.keras.Model):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input,
                     pe_target, rate=0.1):
        super(Transformer, self).__init__()

        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)

        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)

        self.final_layer = tf.keras.layers.Dense(target_vocab_size)

    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):
        # encoder_weights = self.encoder.embedding.embedding.get_weights()
        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)

        dec_output, attention_weights = self.decoder(tar, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)

        final_output = self.final_layer(dec_output)

        return final_output, attention_weights

# num_layers = 6
# d_model = 128
# dff = 512
# num_heads = 8
# dropout_rate = 0.3
# EPOCHS = 90

num_layers = 3
d_model = 256
# d_model=32
dff = 512
num_heads = 8
dropout_rate = 0.2
EPOCHS =10

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, d_model, warmup_steps=4000):
        super(CustomSchedule, self).__init__()

        self.d_model = d_model
        self.d_model = tf.cast(self.d_model, tf.float32)

        self.warmup_steps = warmup_steps


    def __call__(self, step):
        step = tf.cast(step, dtype=tf.float32)
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)
        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

# learning_rate = CustomSchedule(d_model)
learning_rate=.01

optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)
temp_learning_rate_schedule = CustomSchedule(d_model)

plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))
plt.ylabel("Learning Rate")
plt.xlabel("Train Step")


loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
def loss_function(real, pred):
    # mask = tf.math.logical_not(tf.math.equal(real, 0))
    # loss_ = loss_object(real, pred)
    #
    # mask = tf.cast(mask, dtype=loss_.dtype)
    # loss_ *= mask
    #
    # return tf.reduce_sum(loss_)/tf.reduce_sum(mask)
    loss_ = loss_object(real, pred)  # Computes loss for all tokens
    mask = tf.math.not_equal(real, 0)  # Mask out padding tokens (0)
    mask = tf.cast(mask, dtype=loss_.dtype)  # Convert mask to correct type
    loss_ *= mask  # Apply the mask to ignore padding losses
    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)  # Normalize


def accuracy_function(real, pred):
    # print('real')
    # print(real)
    # print('predicted')
    # print(pred)
    accuracies = tf.equal(real, tf.argmax(pred, axis=2))
    accuracies = tf.cast(accuracies, dtype= tf.float32)
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    # accuracies = tf.math.logical_and(mask, accuracies)
    accuracies = tf.cast(accuracies, dtype=tf.float32)
    mask = tf.cast(mask, dtype=tf.float32)
    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')

transformer = Transformer(
    num_layers=num_layers,
    d_model=d_model,
    num_heads=num_heads,
    dff=dff,
    input_vocab_size=ENCODER_VOCAB,
    target_vocab_size=DECODER_VOCAB,
    pe_input=1000,
    pe_target=1000,
    rate=dropout_rate)




def create_masks(inp, tar):
    enc_padding_mask = create_padding_mask(inp)
    dec_padding_mask = create_padding_mask(inp)
    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])
    dec_target_padding_mask = create_padding_mask(tar)
    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)

    return enc_padding_mask, combined_mask, dec_padding_mask

# checkpoint_path = "/content/drive/MyDrive/Dataset/checkpoints"
checkpoint_path = "checkpoints"
ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)
ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)

if ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)

@tf.function
def train_step(inp, tar):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]

    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)

    with tf.GradientTape() as tape:
        predictions, _ = transformer(
            inp,
            tar_inp,
            training=True,
            enc_padding_mask=enc_padding_mask,
            look_ahead_mask=combined_mask,
            dec_padding_mask=dec_padding_mask
        )

        loss = loss_function(tar_real, predictions)

    gradients = tape.gradient(loss, transformer.trainable_variables)
    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
    train_loss(loss)
    train_accuracy(accuracy_function(tar_real, predictions))


# for epoch in range(EPOCHS):
#     start = time.time()
#
#     train_loss.reset_states()
#
#     for (batch, (inp, tar)) in enumerate(dataset):
#         train_step(inp, tar)
#
#         if batch % 100 == 0:
#             print(
#                 f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')
#
#     if (epoch + 1) % 5 == 0:
#         ckpt_save_path = ckpt_manager.save()
#         print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))

    # print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')
    # print('Time taken for 1 epoch: {} secs\n'.format(time.time() - start))
# creating embedding solving the problem of unseen number token

def evaluate(input_article):
    # Tokenize and pad the input article
    print(input_article)
    # if ckpt_manager.latest_checkpoint:
    #     print(f"Restoring from checkpoint: {ckpt_manager.latest_checkpoint}")
    #     ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()
    #     # pretrained_embedding_weights = transformer.encoder.embedding.embedding.get_weights()[0]
    #     # print("Shape of Encoder Embedding Weights:", pretrained_embedding_weights.shape)
    #     #
    #     # pretrained_embedding_weights_decoder = transformer.decoder.embedding.embedding.get_weights()[0]
    #     # print("Shape of Decoder Embedding Weights:", pretrained_embedding_weights_decoder.shape)
    # else:
    #     print("No checkpoint found. Using randomly initialized weights.")

    input_article = tf.convert_to_tensor(input_article, dtype=tf.string)  # Ensure it's a TensorFlow string
    input_article = input_article.numpy().decode("utf-8")
    # inputs=inputArr.apply(preprocess(input_article))
    # inputs = input_series.apply(lambda x: preprocess(x).split())
    #
    input_article = article_tokenizer.texts_to_sequences([input_article])

    # input_article = article_tokenizer.texts_to_sequences([inputs])
    # input_article=[[3,4,5,6,7,8,9,10,14,15,18,19,11,15,16,17,12]]
    # input_article = [[4,5,6,7,8,11,12,9]]
    input_article = tf.cast(input_article, dtype=tf.int64)

    print("tokens")
    print(input_article)

    input_article = tf.keras.preprocessing.sequence.pad_sequences(
        input_article, maxlen=ENCODER_LEN, padding='post', truncating='post'
    )

    encoder_input = tf.expand_dims(input_article[0], 0)
    print (encoder_input)
    decoder_input = [summary_tokenizer.word_index['<sos>']]
    output = tf.expand_dims(decoder_input, 0)
    # print("99999999999999999")
    # print(output)

    for i in range(DECODER_LEN):
        # Create masks
        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)

        # Get predictions from the transformer
        predictions, attention_weights = transformer(
            encoder_input,
            output,
            training=False,
            enc_padding_mask=enc_padding_mask,
            look_ahead_mask=combined_mask,
            dec_padding_mask=dec_padding_mask
        )
        # print(transformer.summary())
        # for layer in transformer.layers:
        #     print(layer.name)
        # for var in transformer.variables:
        #     print(var.name)
        # for var in transformer.encoder.trainable_variables:
        #     print(var.name, var.shape)


        # transformer.encoder.embedding.embedding.set_weights(pretrained_embedding_weights)
        # transformer.decoder.embedding.embedding.set_weights(pretrained_embedding_weights)

        predictions = predictions[:, -1:, :]  # Take the last predicted token
        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)
        if predicted_id == summary_tokenizer.word_index['<eos>']:
            return tf.squeeze(output, axis=0), attention_weights
        # Append the predicted token to the output
        output = tf.concat([output, predicted_id], axis=-1)
        # print(output)
    return tf.squeeze(output, axis=0), attention_weights

def summarize(input_article):
    summarized = evaluate(input_article=input_article)[0].numpy()
    summarized = np.expand_dims(summarized[1:], 0)  # Remove the <sos> token
    return summary_tokenizer.sequences_to_texts(summarized)[0]

# Test the function
print('Output:')
print(summarize(
    "<SOS> 457 + 1116 <EOS>"))

def preprocess_and_split_numbers(input_text):
    # Split numbers into their individual digits
    # This regular expression finds numbers and splits them into individual characters
    input_text = re.sub(r'(\d)', r' \1 ', input_text)  # Add space around each digit
    return input_text


