# import numpy as np
# import pandas as pd
# import random
# import torch
# import torch.nn as nn
# import torch.optim as optim

# # Load data
# data = pd.read_csv("/kaggle/input/userdata-agent/user_data.csv")

# states = data['state'].unique().tolist()
# actions = data['action'].unique().tolist()
# num_states = len(states)
# num_actions = len(actions)

# # One-hot encode states
# def state_to_tensor(state_name):
#     vec = torch.zeros(num_states)
#     vec[states.index(state_name)] = 1.0
#     return vec

# # Compute reward probabilities
# reward_probs = {}
# for state in states:
#     reward_probs[state] = {}
#     for action in actions:
#         row = data[(data['state'] == state) & (data['action'] == action)]
#         if not row.empty:
#             prob = (row['clicks'].values[0] + row['purchases'].values[0]) / row['views'].values[0]
#             reward_probs[state][action] = prob
#         else:
#             reward_probs[state][action] = 0.1

# # Reward function
# def get_reward(state, action):
#     prob = reward_probs[state][action]
#     r = random.random()
#     if r < prob * 0.3:
#         return -5
#     elif r < prob * 0.6:
#         return 0
#     elif r < prob * 0.9:
#         return 5
#     else:
#         return 10

# # Define PyTorch Q-network
# class QNetwork(nn.Module):
#     def __init__(self, input_size, output_size):
#         super(QNetwork, self).__init__()
#         self.fc = nn.Linear(input_size, output_size)
        
#     def forward(self, x):
#         return self.fc(x)

# model = QNetwork(num_states, num_actions)
# optimizer = optim.SGD(model.parameters(), lr=0.1)
# loss_fn = nn.MSELoss()

# # Hyperparameters
# gamma = 0.9
# epsilon = 0.2
# episodes = 2000

# # Training loop
# for episode in range(episodes):
#     state_name = random.choice(states)
#     state_tensor = state_to_tensor(state_name)
    
#     if random.random() < epsilon:
#         action_idx = random.choice(range(num_actions))  # explore
#     else:
#         with torch.no_grad():
#             q_values = model(state_tensor)
#             action_idx = torch.argmax(q_values).item()    # exploit

#     action_name = actions[action_idx]
#     reward = get_reward(state_name, action_name)
    
#     # Next state
#     next_state_name = random.choice(states)
#     next_state_tensor = state_to_tensor(next_state_name)
    
#     # Compute target Q-value
#     with torch.no_grad():
#         next_q_values = model(next_state_tensor)
#         target_q = reward + gamma * torch.max(next_q_values).item()
    
#     # Current Q-value
#     q_values = model(state_tensor)
#     current_q = q_values[action_idx]
    
#     # Compute loss and update
#     loss = loss_fn(current_q, torch.tensor(target_q))
#     optimizer.zero_grad()
#     loss.backward()
#     optimizer.step()

# # Save the model
# torch.save(model.state_dict(), "q_network_model.pth")

# # Load model for prediction
# loaded_model = QNetwork(num_states, num_actions)
# loaded_model.load_state_dict(torch.load("q_network_model.pth"))
# loaded_model.eval()

# # Function to predict best action
# def predict_best_action(state_name):
#     state_tensor = state_to_tensor(state_name)
#     with torch.no_grad():
#         q_values = loaded_model(state_tensor)
#     best_action_idx = torch.argmax(q_values).item()
#     return actions[best_action_idx]

# # Example
# # for state in states:
# print(f"Best action for home_page: {predict_best_action('home_page')}")


# print("Weights (Q-values) for each action per state:")
# print(model.fc.weight)   # shape: [num_actions, num_states]

# print("\nBiases for each action:")
# print(model.fc.bias)     # shape: [num_actions]

# # If you want a more readable format: state vs action
# for i, state in enumerate(states):
#     print(f"\nState: {state}")
#     for j, action in enumerate(actions):
#         # Weight from input neuron i (state) to output neuron j (action)
#         q_value = model.fc.weight[j, i].item() + model.fc.bias[j].item()
#         print(f"  Action: {action:25s} ‚Üí Q-value: {q_value:.2f}")



#--------------------------------------------------------------------------


import pandas as pd
import random
import torch
import torch.nn as nn
import torch.optim as optim

# Define nested states
pages = ['home_page', 'search_page', 'product_page', 'cart_page', 'checkout_page']
user_types = ['new_user', 'returning_user']
time_of_day = ['morning', 'afternoon', 'evening']

# Generate all possible nested states
nested_states = [(p, u, t) for p in pages for u in user_types for t in time_of_day]
actions = ['show_ad', 'recommend_product', 'offer_discount', 'send_email', 'push_notification']

num_states = len(nested_states)
num_actions = len(actions)

# One-hot encode nested state
def state_to_tensor(state_tuple):
    vec = torch.zeros(num_states)
    idx = nested_states.index(state_tuple)
    vec[idx] = 1.0
    return vec

# Generate a sample CSV dataset for nested states
data_rows = []
for state in nested_states:
    for action in actions:
        views = random.randint(50, 500)
        clicks = random.randint(0, views)
        purchases = random.randint(0, clicks)
        data_rows.append({
            'page': state[0],
            'user_type': state[1],
            'time_of_day': state[2],
            'action': action,
            'views': views,
            'clicks': clicks,
            'purchases': purchases
        })

df = pd.DataFrame(data_rows)
df.to_csv('nested_user_data.csv', index=False)

# Compute reward probabilities
reward_probs = {}
for state in nested_states:
    reward_probs[state] = {}
    for action in actions:
        row = df[
            (df['page'] == state[0]) &
            (df['user_type'] == state[1]) &
            (df['time_of_day'] == state[2]) &
            (df['action'] == action)
        ]
        if not row.empty:
            prob = (row['clicks'].values[0] + row['purchases'].values[0]) / row['views'].values[0]
            reward_probs[state][action] = prob
        else:
            reward_probs[state][action] = 0.1

# Reward function
def get_reward(state, action):
    prob = reward_probs[state][action]
    r = random.random()
    if r < prob * 0.3:
        return -5
    elif r < prob * 0.6:
        return 0
    elif r < prob * 0.9:
        return 5
    else:
        return 10

# PyTorch Q-network
class QNetwork(nn.Module):
    def __init__(self, input_size, output_size):
        super(QNetwork, self).__init__()
        self.fc = nn.Linear(input_size, output_size)
        
    def forward(self, x):
        return self.fc(x)

model = QNetwork(num_states, num_actions)
optimizer = optim.SGD(model.parameters(), lr=0.1)
loss_fn = nn.MSELoss()

# Training hyperparameters
gamma = 0.9
epsilon = 0.2
episodes = 2000

# Training loop
for _ in range(episodes):
    state = random.choice(nested_states)
    state_tensor = state_to_tensor(state)
    
    if random.random() < epsilon:
        action_idx = random.choice(range(num_actions))  # explore
    else:
        with torch.no_grad():
            q_values = model(state_tensor)
            action_idx = torch.argmax(q_values).item()    # exploit

    action = actions[action_idx]
    reward = get_reward(state, action)
    
    # Next state
    next_state = random.choice(nested_states)
    next_state_tensor = state_to_tensor(next_state)
    
    with torch.no_grad():
        next_q_values = model(next_state_tensor)
        target_q = reward + gamma * torch.max(next_q_values).item()
    
    q_values = model(state_tensor)
    current_q = q_values[action_idx]
    
    loss = loss_fn(current_q, torch.tensor(target_q))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Save the model
torch.save(model.state_dict(), "nested_q_network_model.pth")

# Predict best action for a nested state
def predict_best_action(state):
    state_tensor = state_to_tensor(state)
    with torch.no_grad():
        q_values = model(state_tensor)
    best_action_idx = torch.argmax(q_values).item()
    return actions[best_action_idx]

# Example prediction
example_state = ('home_page', 'new_user', 'morning')
print(f"Best action for {example_state}: {predict_best_action(example_state)}")



/////////////////////////////actor -crttic//////////////////////


Absolutely! Let‚Äôs write a simplified Python simulation of Actor-Critic for a content recommendation scenario using PyTorch. This will illustrate how the Actor chooses actions, the Critic evaluates, and both get updated.

import torch
import torch.nn as nn
import torch.optim as optim
import random

# --------- Setup: States, Actions, Rewards ----------
user_states = ['morning_user', 'afternoon_user', 'evening_user']
shows = ['Show_A', 'Show_B', 'Show_C']  # actions
num_states = len(user_states)
num_actions = len(shows)

# Reward probabilities for demo purposes
reward_probs = {
    'morning_user': [0.5, 0.3, 0.2],
    'afternoon_user': [0.2, 0.5, 0.3],
    'evening_user': [0.1, 0.3, 0.6]
}

# One-hot encoding for states
def state_to_tensor(state):
    vec = torch.zeros(num_states)
    vec[user_states.index(state)] = 1.0
    return vec

# --------- Actor Network: outputs action probabilities ----------
class Actor(nn.Module):
    def __init__(self, input_size, output_size):
        super(Actor, self).__init__()
        self.fc = nn.Linear(input_size, output_size)
        self.softmax = nn.Softmax(dim=-1)
    
    def forward(self, x):
        return self.softmax(self.fc(x))

# --------- Critic Network: outputs state value ----------
class Critic(nn.Module):
    def __init__(self, input_size):
        super(Critic, self).__init__()
        self.fc = nn.Linear(input_size, 1)
    
    def forward(self, x):
        return self.fc(x)

# Initialize networks and optimizers
actor = Actor(num_states, num_actions)
critic = Critic(num_states)

actor_optimizer = optim.Adam(actor.parameters(), lr=0.01)
critic_optimizer = optim.Adam(critic.parameters(), lr=0.01)

# Hyperparameters
gamma = 0.9
episodes = 1000

# --------- Training Loop ----------
for _ in range(episodes):
    state = random.choice(user_states)
    state_tensor = state_to_tensor(state)

    # Actor chooses action
    action_probs = actor(state_tensor)
    action_dist = torch.distributions.Categorical(action_probs)
    action_idx = action_dist.sample()
    action = shows[action_idx]

    # Environment gives reward
    reward = 1 if random.random() < reward_probs[state][action_idx] else 0

    # Critic estimates value
    value = critic(state_tensor)
    
    # Next state (for simplicity, random user state)
    next_state = random.choice(user_states)
    next_state_tensor = state_to_tensor(next_state)
    next_value = critic(next_state_tensor).detach()

    # Compute advantage
    target = reward + gamma * next_value
    advantage = target - value

    # --------- Update Critic ----------
    critic_loss = advantage.pow(2)  # MSE
    critic_optimizer.zero_grad()
    critic_loss.backward()
    critic_optimizer.step()

    # --------- Update Actor ----------
    actor_loss = -action_dist.log_prob(action_idx) * advantage.detach()
    actor_optimizer.zero_grad()
    actor_loss.backward()
    actor_optimizer.step()

# --------- Test: Predict best show for a state ----------
def recommend_show(state):
    with torch.no_grad():
        probs = actor(state_to_tensor(state))
    best_idx = torch.argmax(probs).item()
    return shows[best_idx]

# Example
test_state = 'morning_user'
print(f"Recommended show for {test_state}: {recommend_show(test_state)}")


Explanation


Actor outputs probabilities for each show given the user state.


Critic estimates how good the current state is.


Reward is sampled based on the predefined probabilities (reward_probs).


Advantage is calculated: advantage = (reward + gamma * next_value) - current_value.


Actor update: Increases probability of actions that gave positive advantage.


Critic update: Trains to predict state values accurately.


After training, the Actor learns to recommend shows based on the state of the user.

If you want, I can also visualize the probability distribution of recommended shows per user state to show how the Actor adapts over time. This is useful for understanding the learning process.
Do you want me to add that visualization?




////////////////////////////////Experience -replay/////////////////////


Sure! Let‚Äôs take a realistic industry-style example using Deep Q-Networks (DQN) with Experience Replay. I‚Äôll keep it illustrative but realistic.

Scenario: E-commerce Recommendation

Problem: Decide which product to show a user on the homepage to maximize clicks/purchases.

State: User attributes (new/returning, time of day, last page visited).

Actions: Recommend one of several products.

Reward: 1 if the user clicks/purchases, 0 otherwise.

We‚Äôll use DQN and Experience Replay to learn the best recommendation policy.

import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque

# ----- Setup -----
user_states = ['new_morning', 'new_evening', 'returning_morning', 'returning_evening']
actions = ['prod_A', 'prod_B', 'prod_C', 'prod_D']
num_states = len(user_states)
num_actions = len(actions)

# Reward probabilities (simulated)
reward_probs = {
    'new_morning': [0.1, 0.3, 0.2, 0.4],
    'new_evening': [0.3, 0.1, 0.4, 0.2],
    'returning_morning': [0.5, 0.2, 0.3, 0.1],
    'returning_evening': [0.2, 0.5, 0.1, 0.3]
}

# One-hot encoding of states
def state_to_tensor(state):
    vec = torch.zeros(num_states)
    vec[user_states.index(state)] = 1.0
    return vec.unsqueeze(0)  # batch dimension

# ----- DQN Model -----
class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_size, 32),
            nn.ReLU(),
            nn.Linear(32, output_size)
        )
    def forward(self, x):
        return self.fc(x)

# ----- Experience Replay -----
class ReplayBuffer:
    def __init__(self, capacity=1000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state):
        self.buffer.append((state, action, reward, next_state))
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states = zip(*batch)
        return states, actions, rewards, next_states
    
    def __len__(self):
        return len(self.buffer)

# ----- Hyperparameters -----
gamma = 0.9
epsilon = 0.2
lr = 0.01
episodes = 500
batch_size = 16

model = DQN(num_states, len(actions))
optimizer = optim.Adam(model.parameters(), lr=lr)
loss_fn = nn.MSELoss()
replay_buffer = ReplayBuffer()

# ----- Training Loop -----
for ep in range(episodes):
    state = random.choice(user_states)
    state_tensor = state_to_tensor(state)
    
    # Epsilon-greedy
    if random.random() < epsilon:
        action_idx = random.choice(range(len(actions)))
    else:
        with torch.no_grad():
            q_values = model(state_tensor)
            action_idx = torch.argmax(q_values).item()
    
    # Simulated reward
    reward = 1 if random.random() < reward_probs[state][action_idx] else 0
    
    # Next state (random for simplicity)
    next_state = random.choice(user_states)
    next_state_tensor = state_to_tensor(next_state)
    
    # Save to replay buffer
    replay_buffer.push(state_tensor, action_idx, reward, next_state_tensor)
    
    # Sample from replay buffer
    if len(replay_buffer) >= batch_size:
        states_b, actions_b, rewards_b, next_states_b = replay_buffer.sample(batch_size)
        
        states_b = torch.cat(states_b)
        next_states_b = torch.cat(next_states_b)
        actions_b = torch.tensor(actions_b)
        rewards_b = torch.tensor(rewards_b, dtype=torch.float32)
        
        q_values = model(states_b)
        next_q_values = model(next_states_b).detach()
        
        # Compute target
        target_q = q_values.clone()
        for i in range(batch_size):
            target_q[i, actions_b[i]] = rewards_b[i] + gamma * torch.max(next_q_values[i])
        
        # Optimize
        loss = loss_fn(q_values, target_q)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# ----- Testing -----
def recommend_product(state):
    with torch.no_grad():
        q_vals = model(state_to_tensor(state))
        best_idx = torch.argmax(q_vals).item()
    return actions[best_idx]

# Example
test_state = 'new_morning'
print(f"Recommended product for {test_state}: {recommend_product(test_state)}")

‚úÖ Key Points in Industry Usage

DQN: Uses a neural network to approximate Q-values when states are too large to handle with tables.

Experience Replay: Stores past interactions to improve stability and break correlations between sequential samples.

Epsilon-greedy exploration: Helps the system discover better recommendations.

Reward simulation: In real applications, the reward would come from actual user clicks or purchases.

If you want, I can also extend this code to include a ‚ÄúTarget Network‚Äù for even more stable training, which is commonly used in production DQN systems.

Do you want me to do that?


Explainnation:

Great ‚Äî let me break this entire training loop step-by-step, line-by-line, and concept-by-concept, so you understand exactly what is happening and why it is done this way in real-world DQN systems.

‚úÖ FULL EXPLANATION OF THE DQN + EXPERIENCE REPLAY TRAINING LOOP
üîµ 1. Start Training Loop
for ep in range(episodes):

This loops through many "training episodes".
Each episode is one interaction between the agent and the environment.

üîµ 2. Pick a State
state = random.choice(user_states)
state_tensor = state_to_tensor(state)



In a real app (website, robot, game), the environment gives the state.


Here we randomly pick a user type like new_morning, returning_evening, etc.


Convert state into a tensor (neural network input).



üîµ 3. Choose Action Using Œµ-Greedy Strategy
if random.random() < epsilon:
    action_idx = random.choice(range(len(actions)))
else:
    with torch.no_grad():
        q_values = model(state_tensor)
        action_idx = torch.argmax(q_values).item()

What is happening?
‚úî 3.1 ‚ÄúExploration‚Äù (Random)


With probability Œµ (say 0.2),


The agent chooses a random action.


This helps the model explore new actions.


‚úî 3.2 ‚ÄúExploitation‚Äù (Model uses its knowledge)


With probability (1 - Œµ),


Pick the action with highest Q-value from the neural network.


This means the agent chooses what it believes gives the highest long-term reward.


üî• Why do we use this?
Without exploration, the model would get stuck recommending the same product forever, even if others might be better.

üîµ 4. Get Reward
reward = 1 if random.random() < reward_probs[state][action_idx] else 0

Here we simulate the user‚Äôs response:


If the user clicks ‚Üí reward = 1


If no click ‚Üí reward = 0


In real e-commerce:


Click = +1


Buy = +5


Ignore = 0



üîµ 5. Sample Next State
next_state = random.choice(user_states)
next_state_tensor = state_to_tensor(next_state)

In real environments:


The next state depends on the action.


But for simplicity, we choose a random next state.


This is okay because the goal is to teach Q-network how to correlate actions with expected success, even if state transitions are random.

üîµ 6. Store the Experience (Replay Buffer)
replay_buffer.push(state_tensor, action_idx, reward, next_state_tensor)

We save the tuple:
(state, action, reward, next_state)

This is called a transition.
Why store experiences?
Experience Replay is used in every industrial DQN system because:


It prevents the model from only learning from the most recent event.


It breaks correlation between sequential data.


It allows multiple training passes over old data ‚Üí improves sample efficiency.


This is ESSENTIAL for stable training.

üîµ 7. Train From a Random Batch of Stored Experiences
if len(replay_buffer) >= batch_size:

We only train if the replay buffer contains enough samples.

üîµ 8. Sample a Batch
states_b, actions_b, rewards_b, next_states_b = replay_buffer.sample(batch_size)

This gives random past transitions:
state1, action1, reward1, next_state1
state2, action2, reward2, next_state2
...
stateN, actionN, rewardN, next_stateN

Then convert to tensors:
states_b = torch.cat(states_b)
next_states_b = torch.cat(next_states_b)
actions_b = torch.tensor(actions_b)
rewards_b = torch.tensor(rewards_b, dtype=torch.float32)


üîµ 9. Compute Q-values from the Model
Current state Q-values:
q_values = model(states_b)

This outputs something like:
state batchprod_Aprod_Bprod_Cprod_Drow 10.40.90.10.2row 20.30.20.80.4...............

Next state Q-values:
next_q_values = model(next_states_b).detach()

.detach() prevents backprop through the next state network (important for stability).

üîµ 10. Compute the DQN Target
For each sample:
target = reward + Œ≥ * max Q(next_state)

In code:
target_q = q_values.clone()

for i in range(batch_size):
    target_q[i, actions_b[i]] = rewards_b[i] + gamma * torch.max(next_q_values[i])

So DQN learns:

‚ÄúThe value of the action = immediate reward + future value‚Äù

This is the Bellman Equation.

üîµ 11. Compute Loss
loss = loss_fn(q_values, target_q)

Loss is:
(predicted Q-values) vs (target Q-values)

Only one action index is updated per training sample.

üîµ 12. Gradient Descent Step
optimizer.zero_grad()
loss.backward()
optimizer.step()

This updates the neural network weights to make predictions more accurate.

üîµ 13. Use Trained Model to Recommend Product
def recommend_product(state):
    with torch.no_grad():
        q_vals = model(state_to_tensor(state))
    best_idx = torch.argmax(q_vals).item()
    return actions[best_idx]

Now the AI can take in a user state and recommend the best product.

üü© FINAL SUMMARY (INDUSTRY LEVEL)
Here‚Äôs the full cycle happening:


Agent picks action (Œµ-greedy).


Simulated environment gives reward.


Store experience in replay buffer.


Sample random old experiences.


Compute DQN target using Bellman equation.


Train the network to reduce difference between predicted Q-values and target Q-values.


After training, the agent chooses highest-Q action for any state.


This is exactly how Netflix, Amazon Ads, TikTok recommendation models train RL agents.

Want me to add a Target Network version (real industry DQN)?
It will make the training more stable ‚Äî the version used in real production systems.
I can write it fully for you if you want.
