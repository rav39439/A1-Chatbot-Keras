
# install: pip install pyautogen


# from autogen_ext.models.ollama import OllamaChatCompletionClient
# ollamamodel_client = OllamaChatCompletionClient(model="llama3.2")

# from autogen_agentchat.messages import TextMessage
# message = TextMessage(content="I'd like to go to London", source="user")
# from autogen_agentchat.messages import TextMessage, MultiModalMessage


# from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
# from autogen_core import CancellationToken

# from autogen_ext.models.ollama import OllamaChatCompletionClient
# from autogen_core.models import UserMessage
# from autogen_agentchat.conditions import  TextMentionTermination

# from langchain_community.utilities import GoogleSerperAPIWrapper
# from langchain_core.tools import Tool
# from autogen_ext.tools.langchain import LangChainToolAdapter
# from dotenv import load_dotenv
# from autogen_agentchat.teams import RoundRobinGroupChat
# import os
# load_dotenv(override=True)

# print("SERPER_API_KEY (raw):", os.getenv("SERPER_API_KEY"))

# os.environ["SERPER_API_KEY"]= os.getenv("SERPER_API_KEY")
# serper = GoogleSerperAPIWrapper()
# langchain_serper =Tool(name="internet_search", func=serper.run, description="useful for when you need to search the internet")
# autogen_serper = LangChainToolAdapter(langchain_serper)
# prompt = """Find a one-way non-stop flight from Bangalore to Mumbai in November 2025."""


# ollama_client = OllamaChatCompletionClient(
#         model="llama3.2",  # Specify the model you pulled with Ollama
# )

# primary_agent = AssistantAgent(
#     "primary",
#     model_client=ollama_client,
#     tools=[autogen_serper],
#     system_message="You are a helpful AI research assistant who looks for promising deals on flights. Incorporate any feedback you receive.",
# )

# evaluation_agent = AssistantAgent(
#     "evaluator",
#     model_client=ollama_client,
#     system_message="Provide constructive feedback. Respond with 'APPROVE' when your feedback is addressed.",
# )

# text_termination = TextMentionTermination("APPROVE")
#     # Example usage
# team = RoundRobinGroupChat([primary_agent, evaluation_agent], termination_condition=text_termination, max_turns=20)

#     # Run the async function (e.g., in an async context or with asyncio.run())

# # Agent 1: Parse ONTAP logs
# LogParserAgent = AssistantAgent(
#     name="LogParserAgent",
#     system_message=(
#         "You are an expert in parsing error logs. Extract relevant information from ONTAP log for classifying the defect and convert it into a single paragraph "
    
#     ),
#     model_client=ollama_client,
# )

# # Agent 2: Classify defect type
# DefectClassifierAgent = AssistantAgent(
#     name="DefectClassifierAgent",
#     system_message=(
#         "You are a defect classifier. Given parsed ONTAP log data, classify the "
#         "defect into one of: cluster-config defect, client-side defect, test-script-defect, System defect."
#         "If defect does not come under any of the above then use your own logic to classify"
#     ),
#     model_client=ollama_client,
# )

# # Agent 3: Summarize
# SummaryAgent = AssistantAgent(
#     name="SummaryAgent",
#     system_message=(
#         "You are a summarizer. Combine parsed log data and defect classification "
#         "into a concise, readable report for the user."
#     ),
#     model_client=ollama_client,

# )

# # User proxy
# user = UserProxyAgent(name="User")

# # --------------------------
# # Message Passing Workflow
# # --------------------------
# async def process_log(log_text):
#     # Step 1: LogParserAgent parses
#     # multi_modal_message = MultiModalMessage(content=[f"Parse this ONTAP log:\n{log_text}"], source="User")
#     # response = await LogParserAgent.on_messages([multi_modal_message], cancellation_token=CancellationToken())
#     # reply = response.chat_message.content

#     # print(reply)  # type: ignore
#     # multi_modal_message = MultiModalMessage(content=[f"Classify this defect:\n{reply}"], source="User")

#     # # Step 2: DefectClassifierAgent classifies
#     # classified = await DefectClassifierAgent.on_messages([multi_modal_message], cancellation_token=CancellationToken())
#     # resultr = classified.chat_message.content

#     # # Step 3: SummaryAgent summarizes
#     # multi_modal_message = MultiModalMessage(content=[f"Summarize this:\nParsed Info: {reply}\nClassification: {resultr}"], source="User")

#     # summarized = await SummaryAgent.on_messages([multi_modal_message], cancellation_token=CancellationToken())
#     # info = summarized.chat_message.content
#     result = await team.run(task=prompt)
#     for message in result.messages:
#        print(f"{message.source}:\n{message.content}\n\n")

    

#     # Step 4: Send to user
# # Example usage:
# ontap_log = """
# [2025-11-04 10:45:21] INFO  :: NATE::TestManager -> Initializing test session for ONTAP cluster 10.25.44.16
# [2025-11-04 10:45:21] DEBUG :: Loading test configuration from /etc/nate/config/ontap_cluster.conf
# [2025-11-04 10:45:21] INFO  :: Connecting to ONTAP management LIF 10.25.44.16 via HTTPS (port 443)
# [2025-11-04 10:45:22] DEBUG :: Using Perl SDK module NaServer.pm for API calls
# [2025-11-04 10:45:22] INFO  :: Preparing test harness for test cases: aggregate_resize_validation, qtree_clone_validation
# [2025-11-04 10:45:22] INFO  :: Starting test case 'aggregate_resize_validation'
# [2025-11-04 10:45:22] DEBUG :: Constructing XML request for aggregate_resize_validation
# <netapp xmlns="http://www.netapp.com/filer/admin" version="1.140">
#    <aggr-resize></aggr-resize>
# </netapp>
# [2025-11-04 10:45:22] ERROR :: NATE::TestManager -> Test script error: Missing mandatory parameter 'target_aggregate' in 'aggregate_resize_validation'
# [2025-11-04 10:45:23] WARN  :: Execution halted for 'aggregate_resize_validation' due to missing input parameter
# [2025-11-04 10:45:23] INFO  :: Logging defect: test-script-defect, subtype: missing-parameter
# [2025-11-04 10:45:23] DEBUG :: Skipping dependent subtests as aggregate_resize_validation failed
# [2025-11-04 10:45:24] INFO  :: Starting subtest 'qtree_clone_validation'
# [2025-11-04 10:45:24] DEBUG :: Validating test configuration for qtree_clone_validation
# [2025-11-04 10:45:25] ERROR :: NATE::TestManager -> Subtest failed: Missing 'source_qtree' parameter in test script
# [2025-11-04 10:45:25] WARN  :: Subtest 'qtree_clone_validation' skipped due to configuration error
# [2025-11-04 10:45:25] INFO  :: Logging defect: test-script-defect, subtype: configuration-error
# [2025-11-04 10:45:26] DEBUG :: Capturing XML request for qtree_clone_validation
# <netapp xmlns="http://www.netapp.com/filer/admin" version="1.140">
#    <qtree-clone-create></qtree-clone-create>
# </netapp>
# [2025-11-04 10:45:26] DEBUG :: XML Response indicates test skipped: status="failed" reason="Configuration error"
# [2025-11-04 10:45:27] INFO  :: Test session summary:
#    Total Test Cases: 2
#    Passed: 0
#    Failed: 2
#    Skipped: 1
# [2025-11-04 10:45:27] INFO  :: Defects recorded:
#    1. aggregate_resize_validation - Missing mandatory parameter 'target_aggregate'
#    2. qtree_clone_validation - Configuration error (missing 'source_qtree' parameter)
# [2025-11-04 10:45:28] INFO  :: NATE Test session completed successfully
# [2025-11-04 10:45:28] INFO  :: Session log saved to /var/log/nate/session_20251104_testscript_2.log
# """

# import asyncio
# asyncio.run(process_log(ontap_log))







#-------------------------------autogen-intro----------------------------------

# from autogen_ext.models.ollama import OllamaChatCompletionClient
# from autogen_core.models import UserMessage

# ollama_client = OllamaChatCompletionClient(
#         model="llama3.2",  # Specify the model you pulled with Ollama
# )

#     # Example usage
# async def chat_with_ollama():
#         result = await ollama_client.create([UserMessage(content="What is the capital of France?", source="user")])
#         print(result)

#     # Run the async function (e.g., in an async context or with asyncio.run())
# import asyncio
# asyncio.run(chat_with_ollama())



# from autogen_ext.models.ollama import OllamaChatCompletionClient
# ollamamodel_client = OllamaChatCompletionClient(model="llama3.2")

# from autogen_agentchat.messages import TextMessage
# message = TextMessage(content="I'd like to go to London", source="user")


# from autogen_agentchat.agents import AssistantAgent

# agent = AssistantAgent(
#     name="airline_agent",
#     model_client=ollamamodel_client,
#     system_message="You are a helpful assistant for an airline. You give short, humorous answers.",
#     model_client_stream=True
# )


# from autogen_core import CancellationToken



# async def chat_with_ollama():
#     response = await smart_agent.on_messages([message], cancellation_token=CancellationToken())
#     for inner_message in response.inner_messages:
#       print(inner_message.content)
#     print(response.chat_message.content)
        
# import os
# import sqlite3

# # Delete existing database file if it exists
# if os.path.exists("tickets.db"):
#     os.remove("tickets.db")

# # Create the database and the table
# conn = sqlite3.connect("tickets.db")
# c = conn.cursor()
# c.execute("CREATE TABLE cities (city_name TEXT PRIMARY KEY, round_trip_price REAL)")
# conn.commit()
# conn.close()

# def save_city_price(city_name, round_trip_price):
#     conn = sqlite3.connect("tickets.db")
#     c = conn.cursor()
#     c.execute("REPLACE INTO cities (city_name, round_trip_price) VALUES (?, ?)", (city_name.lower(), round_trip_price))
#     conn.commit()
#     conn.close()

# # Some cities!
# save_city_price("London", 299)
# save_city_price("Paris", 399)
# save_city_price("Rome", 499)
# save_city_price("Madrid", 550)
# save_city_price("Barcelona", 580)
# save_city_price("Berlin", 525)

# def get_city_price(city_name: str) -> float | None:
#     """ Get the roundtrip ticket price to travel to the city """
#     conn = sqlite3.connect("tickets.db")
#     c = conn.cursor()
#     c.execute("SELECT round_trip_price FROM cities WHERE city_name = ?", (city_name.lower(),))
#     result = c.fetchone()
#     conn.close()
#     return result[0] if result else None

# from autogen_agentchat.agents import AssistantAgent

# smart_agent = AssistantAgent(
#     name="smart_airline_agent",
#     model_client=ollamamodel_client,
#     system_message="You are a helpful assistant for an airline. You give short, humorous answers, including the price of a roundtrip ticket.",
#     model_client_stream=True,
#     tools=[get_city_price],
#     reflect_on_tool_use=True
# )
#     # Run the async function (e.g., in an async context or with asyncio.run())
# import asyncio
# asyncio.run(chat_with_ollama())
