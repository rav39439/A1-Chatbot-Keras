
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import defaultdict
import string
import tensorflow as tf
import re
import os
import time
from tensorflow import keras
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split

# ENCODER_LEN = 200
# DECODER_LEN = 70
# BATCH_SIZE = 36
# BUFFER_SIZE = BATCH_SIZE*8
ENCODER_LEN = 200
DECODER_LEN = 100
BATCH_SIZE = 128
BUFFER_SIZE = BATCH_SIZE*8
# news = pd.read_csv("samples1.csv")
#----------------------------------one way--------------------------------------------------


news = pd.read_csv("multiplication4.csv")  # Replace with your file path
article = news['Input']
summary = news['Output']

# Add SOS and EOS tokens

#---------------------------------------invalid for addition-----------------------------
article = article.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')
summary = summary.apply(lambda x: '<SOS> ' + str(x) + ' <EOS>')


def preprocess(text):
    # Add spaces around operators for correct tokenization
    text = re.sub(r"(\d+|\+|\-|\*|/|=)", r" \1 ", text)
    text = re.sub(r'\s+', ' ', text).strip()  # Normalize spaces
    return text

# Example usage:
# text = "90+88"
# tokens = preprocess(text).split()  # Tokenize by splitting on spaces
# Apply preprocessing
article = article.apply(lambda x: preprocess(x).split())
summary = summary.apply(lambda x: preprocess(x).split())

print("Processed Article:", article.head())
print("Processed Summary:", summary.head())

# Tokenization
filters = 'â‚¹!"#$%&()*,-./:;=?@[\\]^_`{|}~\t\n'
oov_token = '<unk>'

article_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)
summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)

# Fit tokenizers on preprocessed data
article_tokenizer.fit_on_texts(article)
summary_tokenizer.fit_on_texts(summary)

# Convert text to sequences
inputs = article_tokenizer.texts_to_sequences(article)
targets = summary_tokenizer.texts_to_sequences(summary)
# print(inputs)

# Vocabulary sizes
ENCODER_VOCAB = len(article_tokenizer.word_index) + 1
DECODER_VOCAB = len(summary_tokenizer.word_index) + 1

# Pad sequences
ENCODER_LEN = 70  # Maximum length of input sequences
DECODER_LEN = 40   # Maximum length of output sequences

inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')
targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')
input_texts_padded = [' '.join([article_tokenizer.index_word.get(token, '') for token in seq]) for seq in inputs]
target_texts_padded = [' '.join([summary_tokenizer.index_word.get(token, '') for token in seq]) for seq in targets]

# Print tokenizers' word index
print("Article Tokenizer Word Index (Word to Token):")
print(article_tokenizer.word_index)

print("\nSummary Tokenizer Word Index (Word to Token):")
print(summary_tokenizer.word_index)

word_43 = article_tokenizer.index_word.get(43, "Not found")
word_35 = article_tokenizer.index_word.get(35, "Not found")
word_72 = summary_tokenizer.index_word.get(35, "Not found")


inputs = tf.cast(inputs, dtype=tf.int64)

targets = tf.cast(targets, dtype=tf.int64)
#---------------------------------------------------------------------------------------------

dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

#--------------------------transformer---------------------------------------


def get_angles(position, i, d_model):
    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
    return position * angle_rates

def positional_encoding(position, d_model):
    angle_rads = get_angles(
        np.arange(position)[:, np.newaxis],
        np.arange(d_model)[np.newaxis, :],
        d_model
    )

    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])

    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])

    pos_encoding = angle_rads[np.newaxis, ...]

    return tf.cast(pos_encoding, dtype=tf.float32)

def create_padding_mask(seq):
    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)
    return seq[:, tf.newaxis, tf.newaxis, :]

def create_look_ahead_mask(size):
    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
    return mask

def scaled_dot_product_attention(q, k, v, mask):
    matmul_qk = tf.matmul(q, k, transpose_b=True)

    dk = tf.cast(tf.shape(k)[-1], tf.float32)
    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
    # print('loggits')
    # tf.print(scaled_attention_logits)

    if mask is not None:
        scaled_attention_logits += (mask * -1e9)

    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)

    output = tf.matmul(attention_weights, v)
    return output, attention_weights


class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model

        assert d_model % self.num_heads == 0

        self.depth = d_model // self.num_heads

        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)

        self.dense = tf.keras.layers.Dense(d_model)

    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, v, k, q, mask):
        batch_size = tf.shape(q)[0]

        q = self.wq(q)
        k = self.wk(k)
        v = self.wv(v)

        q = self.split_heads(q, batch_size)
        k = self.split_heads(k, batch_size)
        v = self.split_heads(v, batch_size)

        scaled_attention, attention_weights = scaled_dot_product_attention(
            q, k, v, mask)

        # tf.print("Attention weights matrix structure (example 1, head 1):", attention_weights[2,3], summarize=-1)

        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])

        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))
        output = self.dense(concat_attention)
        return output, attention_weights


def point_wise_feed_forward_network(d_model, dff):
    return tf.keras.Sequential([
        tf.keras.layers.Dense(dff, activation='relu'),
        tf.keras.layers.Dense(d_model)
    ])


class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(EncoderLayer, self).__init__()

        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ffn = point_wise_feed_forward_network(d_model, dff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, x, training, mask):
        attn_output, _ = self.mha(x, x, x, mask)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)

        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)

        return out2

# class ExpEmbedding(tf.keras.layers.Layer):
#     def __init__(self, vocab_size, d_model):
#         super(ExpEmbedding, self).__init__()
#         self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)
#
#     def call(self, x):
#         x = tf.cast(x, tf.float32)
#         x = tf.exp(x / tf.reduce_max(x))  # Apply exponential scaling
#         return self.embedding(x)
class PositionalLogEmbedding(tf.keras.layers.Layer):
    def __init__(self, vocab_size, embedding_dim, max_length=100):
        super(PositionalLogEmbedding, self).__init__()
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.positional_encoding = self.add_weight(
            shape=(max_length, embedding_dim),
            initializer="random_normal",
            trainable=True
        )
        self.norm = tf.keras.layers.LayerNormalization()

    def call(self, inputs):
        embedded = self.embedding(inputs)
        log_embedded = tf.math.log1p(embedded)

        positions = tf.range(tf.shape(inputs)[-1])  # Token positions
        position_encoding = tf.gather(self.positional_encoding, positions)

        return self.norm(log_embedded + position_encoding)

class ExpEmbedding(tf.keras.layers.Layer):
    # def __init__(self, vocab_size, embedding_dim):
    #     super(ExpEmbedding, self).__init__()
    #     self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    #     self.norm = tf.keras.layers.LayerNormalization()
    #
    # def call(self, inputs):
    #     embedded = self.embedding(inputs)
    #     return self.norm(tf.exp(embedded))
    def __init__(self, vocab_size, embedding_dim):
        super(ExpEmbedding, self).__init__()
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.norm = tf.keras.layers.LayerNormalization()

    def call(self, inputs):
        embedded = self.embedding(inputs)
        log_embedded = tf.math.log1p(embedded)  # Apply log(1 + x) to prevent log(0) issues
        return self.norm(log_embedded)
        # embedded = self.embedding(inputs)
        # log_embedded = tf.where(embedded > 400, tf.math.log1p(embedded), embedded)
        # return self.norm(log_embedded)

class DirectNumericalEmbedding(tf.keras.layers.Layer):
    def __init__(self, vocab_size, embedding_dim):
        super(DirectNumericalEmbedding, self).__init__()
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.norm = tf.keras.layers.LayerNormalization()

    def call(self, inputs):
        embedded = self.embedding(inputs)
        max_value = tf.reduce_max(embedded) + 1e-6  # Prevent division by zero
        normalized = embedded / max_value  # Normalize values
        return self.norm(normalized)

#
#     class ExpEmbedding(tf.keras.layers.Layer):
#         def __init__(self, vocab_size, embedding_dim, token_to_word_map):
#             super(ExpEmbedding, self).__init__()
#             self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
#             self.norm = tf.keras.layers.LayerNormalization()
#             self.token_to_word_map = token_to_word_map  # Dictionary mapping tokens to words
#
#         def call(self, inputs):
#             # Convert token IDs back to words
#             words = tf.gather(self.token_to_word_map, inputs)
#
#             # Identify numeric tokens
#             is_numeric = tf.map_fn(lambda x: tf.strings.regex_full_match(x, "\\d+"), words, dtype=tf.bool)
#
#             # Standard embedding for words
#             word_embedded = self.embedding(inputs)
#
#             # Exponential embedding for numbers
#             number_embedded = tf.exp(self.embedding(inputs))
#
#             # Select appropriate embeddings
#             embedded = tf.where(tf.expand_dims(is_numeric, -1), number_embedded, word_embedded)
#
#             return self.norm(embedded)

class DecoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(DecoderLayer, self).__init__()

        self.mha1 = MultiHeadAttention(d_model, num_heads)
        self.mha2 = MultiHeadAttention(d_model, num_heads)

        self.ffn = point_wise_feed_forward_network(d_model, dff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)
        self.dropout3 = tf.keras.layers.Dropout(rate)

    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)
        attn1 = self.dropout1(attn1, training=training)
        out1 = self.layernorm1(attn1 + x)

        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)
        attn2 = self.dropout2(attn2, training=training)
        out2 = self.layernorm2(attn2 + out1)

        ffn_output = self.ffn(out2)
        ffn_output = self.dropout3(ffn_output, training=training)
        out3 = self.layernorm3(ffn_output + out2)

        return out3, attn_weights_block1, attn_weights_block2


class Encoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):
        super(Encoder, self).__init__()
        self.d_model = d_model
        self.num_layers = num_layers

        # self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
        self.embedding = ExpEmbedding(input_vocab_size, d_model)  # Pass vocab_size and d_model

        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)

        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]

        self.dropout = tf.keras.layers.Dropout(rate)

    def call(self, x, training, mask):
        seq_len = tf.shape(x)[1]

        x = self.embedding(x)
        print("encoder embedding")
        print(x)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]

        x = self.dropout(x, training=training)

        for i in range(self.num_layers):
            x = self.enc_layers[i](x, training=training, mask=mask)

        return x


class Decoder(tf.keras.layers.Layer):

    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):
        super(Decoder, self).__init__()
        self.d_model = d_model
        self.num_layers = num_layers
        # self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)
        self.embedding = ExpEmbedding(target_vocab_size, d_model)  # Pass vocab_size and d_model

        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)
        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(rate)

    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
        seq_len = tf.shape(x)[1]
        attention_weights = {}
        x = self.embedding(x)
        print("deocder embedding")
        print(x)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]

        x = self.dropout(x, training=training)

        for i in range(self.num_layers):
            x, block1, block2 = self.dec_layers[i](x, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)

            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1
            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2

        return x, attention_weights

class Transformer(tf.keras.Model):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input,
                     pe_target, rate=0.1):
        super(Transformer, self).__init__()

        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)

        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)

        self.final_layer = tf.keras.layers.Dense(target_vocab_size)

    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):
        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)

        dec_output, attention_weights = self.decoder(tar, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)

        final_output = self.final_layer(dec_output)

        return final_output, attention_weights

# num_layers = 6
# d_model = 128
# dff = 512
# num_heads = 8
# dropout_rate = 0.3
# EPOCHS = 90

num_layers = 6
d_model = 128
# d_model=32
dff = 512
num_heads = 8
dropout_rate = 0.2
EPOCHS =20

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, d_model, warmup_steps=4000):
        super(CustomSchedule, self).__init__()

        self.d_model = d_model
        self.d_model = tf.cast(self.d_model, tf.float32)

        self.warmup_steps = warmup_steps


    def __call__(self, step):
        step = tf.cast(step, dtype=tf.float32)
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)
        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

learning_rate = CustomSchedule(d_model)
# learning_rate=.0001

optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)
temp_learning_rate_schedule = CustomSchedule(d_model)

plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))
plt.ylabel("Learning Rate")
plt.xlabel("Train Step")


loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
def loss_function(real, pred):
    # mask = tf.math.logical_not(tf.math.equal(real, 0))
    # loss_ = loss_object(real, pred)
    #
    # mask = tf.cast(mask, dtype=loss_.dtype)
    # loss_ *= mask
    #
    # return tf.reduce_sum(loss_)/tf.reduce_sum(mask)
    loss_ = loss_object(real, pred)  # Computes loss for all tokens
    mask = tf.math.not_equal(real, 0)  # Mask out padding tokens (0)
    mask = tf.cast(mask, dtype=loss_.dtype)  # Convert mask to correct type
    loss_ *= mask  # Apply the mask to ignore padding losses
    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)  # Normalize


def accuracy_function(real, pred):
    print('real')
    print(real)
    print('predicted')
    print(pred)
    accuracies = tf.equal(real, tf.argmax(pred, axis=2))
    accuracies = tf.cast(accuracies, dtype= tf.float32)
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    # accuracies = tf.math.logical_and(mask, accuracies)
    accuracies = tf.cast(accuracies, dtype=tf.float32)
    mask = tf.cast(mask, dtype=tf.float32)
    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')

transformer = Transformer(
    num_layers=num_layers,
    d_model=d_model,
    num_heads=num_heads,
    dff=dff,
    input_vocab_size=ENCODER_VOCAB,
    target_vocab_size=DECODER_VOCAB,
    pe_input=1000,
    pe_target=1000,
    rate=dropout_rate)




def create_masks(inp, tar):
    enc_padding_mask = create_padding_mask(inp)
    dec_padding_mask = create_padding_mask(inp)
    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])
    dec_target_padding_mask = create_padding_mask(tar)
    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)

    return enc_padding_mask, combined_mask, dec_padding_mask

# checkpoint_path = "/content/drive/MyDrive/Dataset/checkpoints"
checkpoint_path = "checkpoints"
ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)
ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)

if ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)
    print ('Latest checkpoint restored!!')

@tf.function
def train_step(inp, tar):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]

    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)

    with tf.GradientTape() as tape:
        predictions, _ = transformer(
            inp,
            tar_inp,
            training=True,
            enc_padding_mask=enc_padding_mask,
            look_ahead_mask=combined_mask,
            dec_padding_mask=dec_padding_mask
        )
        loss = loss_function(tar_real, predictions)

    gradients = tape.gradient(loss, transformer.trainable_variables)
    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
    train_loss(loss)
    train_accuracy(accuracy_function(tar_real, predictions))


# for epoch in range(EPOCHS):
#     start = time.time()
#
#     train_loss.reset_states()
#
#     for (batch, (inp, tar)) in enumerate(dataset):
#         train_step(inp, tar)
#
#         if batch % 100 == 0:
#             print(
#                 f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')
#
#     if (epoch + 1) % 5 == 0:
#         ckpt_save_path = ckpt_manager.save()
#         print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))
#
#     print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')
#     print('Time taken for 1 epoch: {} secs\n'.format(time.time() - start))
# creating embedding solving the problem of unseen number token

def evaluate(input_article):
    # Tokenize and pad the input article
    print(input_article)
    # inputs=input_article.apply(preprocess_and_split_numbers(input_article))

    # input_article = article_tokenizer.texts_to_sequences([inputs])
    # input_article=[[3,4,5,6,7,8,9,10,14,15,18,19,11,15,16,17,12]]
    input_article = [[2,5,3,14,4]]
    print("tokens")
    print(input_article)

    input_article = tf.keras.preprocessing.sequence.pad_sequences(
        input_article, maxlen=ENCODER_LEN, padding='post', truncating='post'
    )
    print("Inputs")
    print(input_article)
    encoder_input = tf.expand_dims(input_article[0], 0)
    print (encoder_input)
    decoder_input = [summary_tokenizer.word_index['<sos>']]
    output = tf.expand_dims(decoder_input, 0)
    print("99999999999999999")
    print(output)

    for i in range(DECODER_LEN):
        # Create masks
        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)

        # Get predictions from the transformer
        predictions, attention_weights = transformer(
            encoder_input,
            output,
            training=False,
            enc_padding_mask=enc_padding_mask,
            look_ahead_mask=combined_mask,
            dec_padding_mask=dec_padding_mask
        )
        predictions = predictions[:, -1:, :]  # Take the last predicted token
        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)
        if predicted_id == summary_tokenizer.word_index['<eos>']:
            return tf.squeeze(output, axis=0), attention_weights
        # Append the predicted token to the output
        output = tf.concat([output, predicted_id], axis=-1)
        print(output)
    return tf.squeeze(output, axis=0), attention_weights

def summarize(input_article):
    summarized = evaluate(input_article=input_article)[0].numpy()
    summarized = np.expand_dims(summarized[1:], 0)  # Remove the <sos> token
    return summary_tokenizer.sequences_to_texts(summarized)[0]

# Test the function
print('Output:')
print(summarize(
    '<SOS>67+99<EOS>'))

def preprocess_and_split_numbers(input_text):
    # Split numbers into their individual digits
    # This regular expression finds numbers and splits them into individual characters
    input_text = re.sub(r'(\d)', r' \1 ', input_text)  # Add space around each digit
    return input_text


