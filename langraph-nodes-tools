



# from langchain_ollama import ChatOllama
# from langchain_core.tools import tool
# from langchain_core.messages import HumanMessage, AIMessage
# from langgraph.prebuilt import ToolNode, tools_condition
# from langgraph.graph import StateGraph, START, END
# from pydantic import BaseModel, Field
# from typing import List
# from langgraph.graph.message import add_messages
# from typing import TypedDict,Annotated

# # 1Ô∏è‚É£ Define a simple tool
# @tool
# def get_weather(city: str) -> str:
#     """Get the weather in a given city."""
#     fake_data = {"Delhi": "Hot, 33¬∞C", "Mumbai": "Humid, 29¬∞C"}
#     return fake_data.get(city, "Weather data not found.")

# # 2Ô∏è‚É£ Define the graph state using Pydantic
# class State(TypedDict):
    
#     messages: Annotated[list, add_messages]

# # 3Ô∏è‚É£ Initialize LLM and bind the tool
# llm = ChatOllama(model="llama3.2")

# llm_with_tools = llm.bind_tools([get_weather])

# # 4Ô∏è‚É£ Define a function that runs the LLM
# def llm_step(state: State) -> State:
#     response = llm_with_tools.invoke(state["messages"])
#     state["messages"].append(response)
#     return state

# # 5Ô∏è‚É£ Create ToolNode
# tool_node = ToolNode([get_weather])

# # 6Ô∏è‚É£ Build the graph
# graph_builder = StateGraph(State)
# graph_builder.add_node("llm", llm_step)
# graph_builder.add_node("tools", tool_node)
# graph_builder.add_conditional_edges( "llm", tools_condition, "tools")
# graph_builder.add_edge("tools", "llm")
# graph_builder.add_edge(START, "llm")


# # 7Ô∏è‚É£ Compile the graph
# app = graph_builder.compile()

# # 8Ô∏è‚É£ Initialize and run
# import asyncio
# async def main():
#     user_input = "What's the weather in Delhi?"
#     config = {"recursion_limit": 20}  # optional, can omit

#     # üëá The async equivalent of invoke()
#     result = await app.ainvoke(
#         {"messages": [HumanMessage(content=user_input)]}    )

#     print("\n--- FINAL OUTPUT ---")
#     print(result["messages"][-1].content)

# # 9Ô∏è‚É£ Run it
# if __name__ == "__main__":
#     asyncio.run(main())

# 9Ô∏è‚É£ Print results


#-------------------------------------------------------------------------------------



from langchain_ollama import ChatOllama
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel, Field
from typing import List
from langgraph.graph.message import add_messages
from typing import TypedDict,Annotated
from langchain_core.tools import BaseTool

# 1Ô∏è‚É£ Define a simple tool
@tool
def get_weather(city: str) -> str:
    """Get the weather in a given city."""
    fake_data = {"Delhi": "Hot, 33¬∞C", "Mumbai": "Humid, 29¬∞C"}
    return fake_data.get(city, "Weather data not found.")

# 2Ô∏è‚É£ Define the graph state using Pydantic
class State(TypedDict):
    
    messages: Annotated[list, add_messages]

# 3Ô∏è‚É£ Initialize LLM and bind the tool
llm = ChatOllama(model="llama3.2")

llm_with_tools = llm.bind_tools([get_weather])

# 4Ô∏è‚É£ Define a function that runs the LLM
def llm_step(state: State) -> State:
    response = llm_with_tools.invoke(state["messages"])
    state["messages"].append(response)
    return state


class CityClimateTool(BaseTool):
    name: str = "get_city_climate"
    description: str = (
        "Fetch climate and weather details for a given city, "
        "including temperature, humidity, and season."
    )

    def _run(self, city: str) -> str:
        """Synchronous version used by the LLM."""
        data = {
            "Delhi": {
                "temperature": "33¬∞C",
                "humidity": "45%",
                "season": "Autumn",
                "climate": "Hot and dry",
            },
            "Mumbai": {
                "temperature": "29¬∞C",
                "humidity": "80%",
                "season": "Monsoon",
                "climate": "Humid and coastal",
            },
            "Shimla": {
                "temperature": "18¬∞C",
                "humidity": "50%",
                "season": "Spring",
                "climate": "Cool and pleasant",
            },
        }

        info = data.get(city)
        if not info:
            return f"No climate data found for {city}."

        return (
            f"{city} Climate Report:\n"
            f"- Temperature: {info['temperature']}\n"
            f"- Humidity: {info['humidity']}\n"
            f"- Season: {info['season']}\n"
            f"- Climate: {info['climate']}"
        )

    async def _arun(self, city: str) -> str:
        """Async version."""
        return self._run(city)
        
llm = ChatOllama(model="llama3.2")

climate_tool = CityClimateTool()
system_prompt = (
    "You are a concise assistant. "
    "When you call tools, return the final answer directly. "
    "Do not explain that you are using a tool. "
    "Only output the final facts in plain text."
)

llm = ChatOllama(model="llama3.2", system=system_prompt).bind_tools([CityClimateTool()])

# 4Ô∏è‚É£ Define a function that runs the LLM
def llm_step(state: State) -> State:
    response = llm_with_tools.invoke(state["messages"])
    state["messages"].append(response)
    return state

# 5Ô∏è‚É£ Create ToolNode
tool_node = ToolNode([get_weather])

# 6Ô∏è‚É£ Build the graph
graph_builder = StateGraph(State)
graph_builder.add_node("llm", llm_step)
graph_builder.add_node("tools", tool_node)
graph_builder.add_conditional_edges( "llm", tools_condition, "tools")
graph_builder.add_edge("tools", "llm")
graph_builder.add_edge(START, "llm")


# 7Ô∏è‚É£ Compile the graph
app = graph_builder.compile()

# 8Ô∏è‚É£ Initialize and run
import asyncio
async def main():
    user_input = "What's the weather,humidity and climate in Delhi?"
    config = {"recursion_limit": 20}  # optional, can omit

    # üëá The async equivalent of invoke()
    result = await app.ainvoke(
        {"messages": [HumanMessage(content=user_input)]}    )

    print("\n--- FINAL OUTPUT ---")
    print(result["messages"][-1].content)

# 9Ô∏è‚É£ Run it
if __name__ == "__main__":
    asyncio.run(main())








-----------------------


Defect_agent

from langchain_ollama import ChatOllama
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage,SystemMessage,ToolCall
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel, Field
from typing import List
from langgraph.graph.message import add_messages
from typing import TypedDict,Annotated
import json
from langchain.tools import tool

@tool
def analyze_client_side_defect(log_summary: str) -> str:
    """
    Specialized ONTAP log analysis tool for **client-side defects** only.
    Client-side defects typically originate from issues on the client system
    that interacts with ONTAP, such as invalid requests, timeouts, network
    disconnects, or misconfigured client parameters. These problems usually
    occur before the request reaches the ONTAP cluster for processing.
    Use this tool only when the log or defect description suggests that the
    failure originated from the client side rather than from ONTAP itself.
    The tool internally classifies the defect type before proceeding.
    """
    print("------------analyze_client_side_defect invoked--------------------------")
    explanation_prompt = f"""
        The following ONTAP log represents a client-side defect.
        Only summarize the root cause of the defect in detail focusing on {log_summary}
        """
    return (
            f"Defect Type: Client-side\n"
            f"Analysis: {explanation_prompt}"
        )
        
def get_prompt_message():
    """
    Returns a single summarized paragraph for ONTAP log parsing.
    """
    return {
        "role": "system",
        "content": (
           "You are an expert in analyzing NetApp ONTAP NATE test framework and Perl stack logs. "
        "Your task is to extract and summarize key diagnostic information from NATE or Perl stack traces. "
        "Focus on capturing the following fields when present: "
        "‚Ä¢ error_message (text following FAIL, ERROR, or WARNING) "
        "‚Ä¢ error_code or return_code (numeric or symbolic if available) "
        "‚Ä¢ root_cause (concise description inferred from the message or function) "
        "‚Ä¢ status (e.g., FAILED, ABORTED, TIMEOUT, WARNING) "
        "‚Ä¢ event_or_module (such as EventMan::EventModules::ControllerEvents::CMode::SFOEvents) "
        "‚Ä¢ timeout_value (if mentioned in seconds) "
        "‚Ä¢ command_or_function (e.g., detect_sfo_giveback, sfo_giveback_generic) "
        "‚Ä¢ subtest_or_runid (values after -runid, Subtest::new, or Tharn::subtest) "
        "‚Ä¢ file_path_and_line (the Perl module path and line number) "
        "If any field is missing, omit it gracefully. "
        "Summarize the extracted information in a single, clear paragraph written in natural language, "
        "stating what happened, in which module, and the probable defect type if inferable "
        
        )
    }


def ontap_log_parse(log:str) -> str:
    print("started")
    prompt = get_prompt_message()
    last_msg =log
    user_message =log
    # Invoke LLM properly
    response = llm.invoke([
        SystemMessage(content=prompt["content"]),
        HumanMessage(content=user_message)
    ])
    return response.content
    

@tool
def analyze_cluster_config_defect(log_summary: str) -> str:
    """
    Cluster configuration defects typically involve issues with inter-node
communication, cluster setup, quorum formation, or synchronization between
clustered nodes. These can result from misconfigured network interfaces,
authentication mismatches, or cluster metadata inconsistencies.
Use this tool only when the log or defect description suggests such
cluster-related setup or connectivity problems.
The tool internally classifies the defect type before proceeding.
    """
    classification_prompt = f"""
    You are an ONTAP defect classification assistant.
    Determine whether the issue in the following log is:
    - "cluster configuration"
    - "server-side"
    - "unclear"

    Log Summary:
    {log_summary}

    Respond with only one label.
    """

    print("------------analyze_cluster_config_defect invoked--------------------------")
    defect_type = defect_analyser_llm.invoke(classification_prompt).content.strip().lower()

    # Execute only if classification == cluster configuration
    if "cluster" in defect_type:
        explanation_prompt = f"""
        The following ONTAP log represents a cluster configuration defect.
        Summarize in 2‚Äì3 lines what the likely root cause is,
        focusing on node communication, interconnects, or cluster setup.

        Log Summary:
        {log_summary}
        """
        explanation = llm.invoke(explanation_prompt).content.strip()
        return (
            f"Defect Type: Cluster Configuration\n"
            f"Analysis: {explanation}"
        )

    return (
        f"Defect Type: {defect_type.capitalize()}\n"
        f"Analysis: Not a cluster configuration issue. "
        f"Suggest using another tool specialized for {defect_type} defects."
    )

class State(TypedDict):
    messages: Annotated[list, add_messages]
llm = ChatOllama(model="llama3.2")
llm_with_tools = llm.bind_tools([analyze_client_side_defect,analyze_cluster_config_defect])
tool_node = ToolNode([analyze_client_side_defect,analyze_cluster_config_defect])
def llm_step(state: State) -> State:
    response = llm_with_tools.invoke(state["messages"])
    state["messages"].append(response)
    return 

system_prompt = """
You are an ONTAP defect analysis assistant operating in a restricted tool environment.

Rules:
1. Use only the provided tools to handle user input.
2. Do not reason, summarize, or answer directly.
3. If a tool is applicable, invoke it.
4. If no tool applies, output exactly:
   "defect cannot be categorized by using the provided tools"
5. Never create or assume information outside the available tools.
"""

# 6Ô∏è‚É£ Build the graph
graph_builder = StateGraph(State)
graph_builder.add_node("llm", llm_step)
graph_builder.add_node("tools", tool_node)
graph_builder.add_conditional_edges( "llm", tools_condition, "tools")
graph_builder.add_edge("tools", "llm")
graph_builder.add_edge(START, "llm")


# 7Ô∏è‚É£ Compile the graph
app = graph_builder.compile()

# 8Ô∏è‚É£ Initialize and run
import asyncio
async def main():
    user_input = """[2025-10-31 11:32:42] [ERROR] [ONTAP::Client::IO] Read operation failed:
Timeout while waiting for server response.
Request: GET_VOL_INFO volume1
Client IP: 10.42.18.75
Server IP: 10.42.18.10
Error: Connection timed out after 30000 ms
at /usr/lib/perl5/ONTAP/Client/IO.pm line 241.

[2025-10-31 11:32:42] [WARN] [ONTAP::Client::Retry] Retrying request due to network timeout (attempt 2/3)
at /usr/lib/perl5/ONTAP/Client/Retry.pm line 112.

[2025-10-31 11:32:45] [ERROR] [ONTAP::Client::Session] Client failed to establish a stable session.
Possible causes: client NIC misconfiguration or packet loss on client network path.
Exiting with status code 408."""

    config = {"recursion_limit": 20}  # optional, can omit
    ontpparsed = ontap_log_parse(user_input)
    print(ontpparsed)
    # üëá The async equivalent of invoke()
    result = await app.ainvoke(
        {"messages": [HumanMessage(content=user_input)]}    )
    result = await app.ainvoke({
    "messages": [
        SystemMessage(content="You are an ONTAP defect classifier. Use the correct tool for each log."),
        HumanMessage(content=ontpparsed)        # The user‚Äôs ONTAP log input
    ]
})

    print("\n--- FINAL OUTPUT ---")
    # print(result["messages"])
    print(result["messages"][-1].content)


# 9Ô∏è‚É£ Run it
if __name__ == "__main__":
    asyncio.run(main())

    
