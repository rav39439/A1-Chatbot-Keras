



# from langchain_ollama import ChatOllama
# from langchain_core.tools import tool
# from langchain_core.messages import HumanMessage, AIMessage
# from langgraph.prebuilt import ToolNode, tools_condition
# from langgraph.graph import StateGraph, START, END
# from pydantic import BaseModel, Field
# from typing import List
# from langgraph.graph.message import add_messages
# from typing import TypedDict,Annotated

# # 1ï¸âƒ£ Define a simple tool
# @tool
# def get_weather(city: str) -> str:
#     """Get the weather in a given city."""
#     fake_data = {"Delhi": "Hot, 33Â°C", "Mumbai": "Humid, 29Â°C"}
#     return fake_data.get(city, "Weather data not found.")

# # 2ï¸âƒ£ Define the graph state using Pydantic
# class State(TypedDict):
    
#     messages: Annotated[list, add_messages]

# # 3ï¸âƒ£ Initialize LLM and bind the tool
# llm = ChatOllama(model="llama3.2")

# llm_with_tools = llm.bind_tools([get_weather])

# # 4ï¸âƒ£ Define a function that runs the LLM
# def llm_step(state: State) -> State:
#     response = llm_with_tools.invoke(state["messages"])
#     state["messages"].append(response)
#     return state

# # 5ï¸âƒ£ Create ToolNode
# tool_node = ToolNode([get_weather])

# # 6ï¸âƒ£ Build the graph
# graph_builder = StateGraph(State)
# graph_builder.add_node("llm", llm_step)
# graph_builder.add_node("tools", tool_node)
# graph_builder.add_conditional_edges( "llm", tools_condition, "tools")
# graph_builder.add_edge("tools", "llm")
# graph_builder.add_edge(START, "llm")


# # 7ï¸âƒ£ Compile the graph
# app = graph_builder.compile()

# # 8ï¸âƒ£ Initialize and run
# import asyncio
# async def main():
#     user_input = "What's the weather in Delhi?"
#     config = {"recursion_limit": 20}  # optional, can omit

#     # ðŸ‘‡ The async equivalent of invoke()
#     result = await app.ainvoke(
#         {"messages": [HumanMessage(content=user_input)]}    )

#     print("\n--- FINAL OUTPUT ---")
#     print(result["messages"][-1].content)

# # 9ï¸âƒ£ Run it
# if __name__ == "__main__":
#     asyncio.run(main())

# 9ï¸âƒ£ Print results


#-------------------------------------------------------------------------------------



from langchain_ollama import ChatOllama
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel, Field
from typing import List
from langgraph.graph.message import add_messages
from typing import TypedDict,Annotated
from langchain_core.tools import BaseTool

# 1ï¸âƒ£ Define a simple tool
@tool
def get_weather(city: str) -> str:
    """Get the weather in a given city."""
    fake_data = {"Delhi": "Hot, 33Â°C", "Mumbai": "Humid, 29Â°C"}
    return fake_data.get(city, "Weather data not found.")

# 2ï¸âƒ£ Define the graph state using Pydantic
class State(TypedDict):
    
    messages: Annotated[list, add_messages]

# 3ï¸âƒ£ Initialize LLM and bind the tool
llm = ChatOllama(model="llama3.2")

llm_with_tools = llm.bind_tools([get_weather])

# 4ï¸âƒ£ Define a function that runs the LLM
def llm_step(state: State) -> State:
    response = llm_with_tools.invoke(state["messages"])
    state["messages"].append(response)
    return state


class CityClimateTool(BaseTool):
    name: str = "get_city_climate"
    description: str = (
        "Fetch climate and weather details for a given city, "
        "including temperature, humidity, and season."
    )

    def _run(self, city: str) -> str:
        """Synchronous version used by the LLM."""
        data = {
            "Delhi": {
                "temperature": "33Â°C",
                "humidity": "45%",
                "season": "Autumn",
                "climate": "Hot and dry",
            },
            "Mumbai": {
                "temperature": "29Â°C",
                "humidity": "80%",
                "season": "Monsoon",
                "climate": "Humid and coastal",
            },
            "Shimla": {
                "temperature": "18Â°C",
                "humidity": "50%",
                "season": "Spring",
                "climate": "Cool and pleasant",
            },
        }

        info = data.get(city)
        if not info:
            return f"No climate data found for {city}."

        return (
            f"{city} Climate Report:\n"
            f"- Temperature: {info['temperature']}\n"
            f"- Humidity: {info['humidity']}\n"
            f"- Season: {info['season']}\n"
            f"- Climate: {info['climate']}"
        )

    async def _arun(self, city: str) -> str:
        """Async version."""
        return self._run(city)
        
llm = ChatOllama(model="llama3.2")

climate_tool = CityClimateTool()
system_prompt = (
    "You are a concise assistant. "
    "When you call tools, return the final answer directly. "
    "Do not explain that you are using a tool. "
    "Only output the final facts in plain text."
)

llm = ChatOllama(model="llama3.2", system=system_prompt).bind_tools([CityClimateTool()])

# 4ï¸âƒ£ Define a function that runs the LLM
def llm_step(state: State) -> State:
    response = llm_with_tools.invoke(state["messages"])
    state["messages"].append(response)
    return state

# 5ï¸âƒ£ Create ToolNode
tool_node = ToolNode([get_weather])

# 6ï¸âƒ£ Build the graph
graph_builder = StateGraph(State)
graph_builder.add_node("llm", llm_step)
graph_builder.add_node("tools", tool_node)
graph_builder.add_conditional_edges( "llm", tools_condition, "tools")
graph_builder.add_edge("tools", "llm")
graph_builder.add_edge(START, "llm")


# 7ï¸âƒ£ Compile the graph
app = graph_builder.compile()

# 8ï¸âƒ£ Initialize and run
import asyncio
async def main():
    user_input = "What's the weather,humidity and climate in Delhi?"
    config = {"recursion_limit": 20}  # optional, can omit

    # ðŸ‘‡ The async equivalent of invoke()
    result = await app.ainvoke(
        {"messages": [HumanMessage(content=user_input)]}    )

    print("\n--- FINAL OUTPUT ---")
    print(result["messages"][-1].content)

# 9ï¸âƒ£ Run it
if __name__ == "__main__":
    asyncio.run(main())








-----------------------


parse _agent

@lru_cache(maxsize=1)
def get_prompt_message():
    """
    Returns a static system message dict for ONTAP log parsing.
    """
    return {
        "role": "system",
        "content": (
           "You are an expert in analyzing NetApp ONTAP NATE test framework and Perl stack logs. "
        "Your task is to extract and summarize key diagnostic information from NATE or Perl stack traces. "
        "Focus on capturing the following fields when present: "
        "â€¢ error_message (text following FAIL, ERROR, or WARNING) "
        "â€¢ error_code or return_code (numeric or symbolic if available) "
        "â€¢ root_cause (concise description inferred from the message or function) "
        "â€¢ status (e.g., FAILED, ABORTED, TIMEOUT, WARNING) "
        "â€¢ event_or_module (such as EventMan::EventModules::ControllerEvents::CMode::SFOEvents) "
        "â€¢ timeout_value (if mentioned in seconds) "
        "â€¢ command_or_function (e.g., detect_sfo_giveback, sfo_giveback_generic) "
        "â€¢ subtest_or_runid (values after -runid, Subtest::new, or Tharn::subtest) "
        "â€¢ file_path_and_line (the Perl module path and line number) "
        "If any field is missing, omit it gracefully. "
        "Summarize the extracted information in a single, clear paragraph written in natural language, "
        "stating what happened, in which module, and the probable defect type if inferable "
        "(e.g., timeout, system failure, or test harness issue)."
        )
    }





def ontap_log_parse(log:str) -> str:
    print("started")
    prompt = get_prompt_message()
    last_msg =log
    user_message = get_message_content(last_msg)

    # Invoke LLM properly
    response = summary_llm.invoke([
        SystemMessage(content=prompt["content"]),
        HumanMessage(content=user_message)
    ])

    return response.content
    
