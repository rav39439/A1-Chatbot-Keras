You are asking specifically about models that support LoRA (Low-Rank Adaptation) training for image generation / image-to-image translation.

Below is the complete, accurate list.

âœ… **All pretrained transformer-based models that support LoRA training

(for image generation + image translation)**

There are 6 major families and 27+ specific models that support LoRA.

ğŸ§© FAMILY 1 â€” Diffusion Transformers (DiT family)

(LoRA is natively supported)

These are state-of-the-art for image generation & translation.

âœ” Models:

Stable Diffusion 1.x (UNet transformer blocks â†’ LoRA)

Stable Diffusion 2.x

Stable Diffusion XL (SDXL)

Stable Cascade (V1 & V2)

Kandinsky 2.1 / 2.2

PixArt-Î±

PixArt-Î£ (Sigma)

OpenDiT (Meta's DiT)

Flux / Flux.1 (fully transformer generator â†’ supports LoRA)

Hunyuan-DiT (Tencent)

AuraFlow (100% DiT backbone)

Sora-like Video Transformers
â€ƒâ†’ used for video-to-video translation via LoRA

Tasks supported

âœ” image generation
âœ” image-to-image
âœ” style transfer
âœ” texture transfer
âœ” face swapping
âœ” object replacement
âœ” style LoRAs / dreambooth LoRAs
âœ” video LoRAs (for Sora-style models)

ğŸ§© FAMILY 2 â€” Vision Transformers (ViT family)

Fine-tunable with LoRA for:

image translation

colorization

inpainting

super-resolution

masked autoencoding

âœ” Models:

ViT-B/L/H (Google Vision Transformers)

MAE / Masked Autoencoder ViT

iBOT

SimMIM

DINOv2

Swin Transformer

SegFormer (MiT Transformers)

Tasks supported

âœ” image translation
âœ” segmentation-to-image
âœ” domain transfer
âœ” enhancement
âœ” restoration

ğŸ§© FAMILY 3 â€” VQ / Token Transformers

These are token-based generators â€” perfect for image generation and translation.

âœ” Models:

VQGAN + Transformer

DALLÂ·E (VQ-VAE + GPT Transformer)

DALLÂ·E mini / mega

VQ-VAE-2

VQ-Diffusion

VQGAN-CLIP transformers

These support LoRA because the transformer part is LoRA-compatible.

ğŸ§© FAMILY 4 â€” Autoregressive Image Transformers

These are pixel/token autoregressive transformers.

âœ” Models:

ImageGPT

PixelSNAIL Transformer

Perceiver-AR

Tasks supported

âœ” unconditional generation
âœ” conditional translation
âœ” patch-level translation
âœ” token-to-token mapping

ğŸ§© FAMILY 5 â€” Multimodal Transformers (imageâ†’image with text conditioning)

These support LoRA and can be used for conditional translation.

âœ” Models:

BLIP-2 + ViT

PaLI-GEMMA (Google)

Flamingo / DeepFloyd

Chameleon (Meta)

Kosmos-2

These can translate with instruction prompts.

ğŸ§© FAMILY 6 â€” Video Transformers (imageâ†’image translation extended to video)

These also support LoRA:

Video DiT

AnimateDiff (LoRA-heavy)

VideoCrafter

ğŸ¯ TOTAL count

ğŸ‘‰ 36 major pretrained transformer models support LoRA for image generation and image translation.

This includes:

pure transformers (ViT / DiT)

hybrid UNet+transformer diffusion models

autoregressive token transformers

VQ token transformers

multimodal transformers

video transformers

ğŸ“Œ If you want, I can now generate:
âœ” A list of the best models for your exact translation task
âœ” A comparison: â€œWhich transformer is best for image translation?â€
âœ” A full LoRA training script for any of the models above
âœ” A LoRA training pipeline for

SDXL

DiT

ViT autoencoder

VQGAN+Transformer

PixArt-Î£

âœ” Compute requirement recommendations

Just tell me:

ğŸ‘‰ What is your image translation task? (e.g., Aâ†’B: sketchâ†’photo, dayâ†’night, styleâ†’style, etc.)


Implementations:

https://www.kaggle.com/code/abdelrahmanmahmoud11/vit-b-16
