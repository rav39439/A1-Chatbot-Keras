import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import defaultdict
import string
import tensorflow as tf
import re
import os
import time
from tensorflow import keras
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split

ENCODER_LEN = 200
DECODER_LEN = 70
# BATCH_SIZE = 
# BUFFER_SIZE = BATCH_SIZE*8
# ENCODER_LEN = 140
# DECODER_LEN = 100
# BATCH_SIZE = 16
# BUFFER_SIZE = BATCH_SIZE * 8
# news = pd.read_csv("/kaggle/input/summerization-sample/summarization_100000.csv")
# news = news.head(20000)
# print(news
# print(news.columns)


import tensorflow as tf
import numpy as np
import re

# -------------------------
# Parameters
# -------------------------
# seq_len = 20  # max length of input sequences
# BATCH_SIZE = 32
# BUFFER_SIZE = 10000
# EMBED_DIM = 128
# NUM_HEADS = 4
# DFF = 512
# NUM_LAYERS = 3
# DROPOUT = 0.1
# EPOCHS = 10
SEQ_LEN = 128     # number of tokens in decoder input (window size for model in
WINDOW = SEQ_LEN + 1    # we create windows of length SEQ_LEN+1 then split to input/target
BATCH_SIZE = 4
BUFFER_SIZE = 20000
EMBED_DIM = 256
NUM_HEADS = 8
DFF = 1024
NUM_LAYERS = 8
DROPOUT = 0.1
EPOCHS = 4
LR = 3e-4

import tensorflow as tf
import numpy as np
import math

@tf.function
def longformer_sliding_window_attention(Q, K, V, alibi_bias=None, mask_future=True):
    """
    Implements Longformer's sliding window attention using a fixed-size local window.

    This function iterates over the sequence length L using a TensorArray,
    which is necessary for dynamic slicing/gathering within a TensorFlow graph.
    It assumes symmetric windows, where the total window_size includes the query token itself.

    Args:
        Q (tf.Tensor): Query tensor (B, H, L, D).
        K (tf.Tensor): Key tensor (B, H, L, D).
        V (tf.Tensor): Value tensor (B, H, L, D).
        window_size (int): The full size of the local attention window (e.g., 512).
                           This must be an even number (or +1 for the query).
        alibi_bias (tf.Tensor): Pre-calculated ALiBi bias tensor (H, L, L).
                                This represents the relative position bias.

    Returns:
        tf.Tensor: Output tensor with applied windowed attention (B, H, L, D).
    """
    window_size=64
    B, H, L, D = tf.unstack(tf.shape(Q))
    # D_k is the head dimension, used for scaling
    D_k = tf.cast(D, Q.dtype)
    scale = tf.math.rsqrt(D_k)
    print(scale)

    # Calculate half-window size (context tokens on one side).
    # We take half the window size and ensure it's at least 1 for proper indexing.
    W = tf.cast(tf.maximum(1, window_size // 2), tf.int32)

    # TensorArray for collecting the output tokens (B, H, D) for each position
    output_ta = tf.TensorArray(
        dtype=Q.dtype,
        size=L,
        dynamic_size=False,
        element_shape=[Q.shape[0], Q.shape[1], Q.shape[3]]

    )
    NEG_INF = tf.constant(-1e9, dtype=Q.dtype)
    lower = tf.linalg.band_part(tf.ones((L, L), dtype=Q.dtype), -1, 0)
    causal_mask = 1.0 - lower   # future tokens = 1



    # --- Start Iterative Window Attention --
    for i in tf.range(L):
        # 1. Calculate window start and end indices
        # start = max(0, i - W)
        # end = min(L, i + W + 1) -> +1 because Python slicing is exclusive
        start_index = tf.maximum(0, i - W)
        end_index = tf.minimum(L, i + W + 1)

        # 2. Extract current query token Q_i (B, H, 1, D)
        Q_i = Q[:, :, i:i+1, :]

        # 3. Slice K, V, and ALiBi bias for the current window
        # K_window, V_window shape: (B, H, WindowLength, D)
        K_window = K[:, :, start_index:end_index, :]
        V_window = V[:, :, start_index:end_index, :]

        # Alibi_window shape: (H, 1, WindowLength)
        # We slice the L x L alibi matrix: all heads, row 'i', columns 'start' to 'end'
        Alibi_window = alibi_bias[:, i:i+1, start_index:end_index]

        # 4. Compute Attention Scores (B, H, 1, WindowLength)
        # S = (Q_i @ K_window^T) * scale
        Attention_Scores = tf.matmul(Q_i, K_window, transpose_b=True) * scale

        # 5. Apply ALiBi Bias
        # The bias needs to be broadcastable to the scores (B, H, 1, WindowLength)
        # Alibi_window is (H, 1, WindowLength). It broadcasts across B and the Q sequence length (which is 1).
        Attention_Scores += Alibi_window

        causal_mask_i = causal_mask[i:i+1, start_index:end_index]

        # reshape to broadcast → (1, 1, 1, W_len)
        causal_mask_i = tf.reshape(causal_mask_i, (1, 1, 1, -1))

        # apply padding mask

        Attention_Scores += causal_mask_i * NEG_INF

        

        # 6. Apply Softmax (normalize across the WindowLength dimension)
        Attention_Weights = tf.nn.softmax(Attention_Scores, axis=-1)

        # 7. Compute Weighted Sum (B, H, 1, D)
        # Output_i = Attention_Weights @ V_window
        Output_i = tf.matmul(Attention_Weights, V_window)

        # 8. Write the result to TensorArray
        # Squeeze the sequence dimension (axis=2) to match element_shape [B, H, D]
        output_ta = output_ta.write(i, tf.squeeze(Output_i, axis=2))

    # --- End Iterative Window Attention ---

    # 9. Gather from TensorArray and Concatenate
    # Stack creates (L, B, H, D)
    out = output_ta.stack()

    # 10. Reorder dimensions back to (B, H, L, D)
    out = tf.transpose(out, perm=[1, 2, 0, 3])
    return out

#------------------alternative--------------------

# @tf.function
# def longformer_sliding_window_attention(Q, K, V, alibi_bias, mask_future=True):
#     """
#     Longformer-style sliding window attention WITH internal CAUSAL MASK.
#     Blocks future tokens inside the window: j > i → masked.
#     """

#     B, H, L, D = tf.unstack(tf.shape(Q))

#     # scaling
#     D_k = tf.cast(D, Q.dtype)
#     scale = tf.math.rsqrt(D_k)

#     # window size must come from model attributes (you are passing only mask_future here)
#     window_size = 64  # <-- or self.window_size if inside a class
#     W = tf.cast(window_size // 2, tf.int32)

#     NEG_INF = tf.constant(-1e9, dtype=Q.dtype)

#     # -------- Causal Mask (L, L) --------
#     if mask_future:
#         lower = tf.linalg.band_part(tf.ones((L, L), dtype=Q.dtype), -1, 0)
#         causal_mask = 1.0 - lower   # 1 for future → masked
#     else:
#         causal_mask = tf.zeros((L, L), dtype=Q.dtype)

#     # TensorArray to store each output token
#     output_ta = tf.TensorArray(
#         dtype=Q.dtype,
#         size=L,
#         element_shape=[Q.shape[0], Q.shape[1], Q.shape[3]]
#     )

#     # -------- Sliding Window Loop --------
#     for i in tf.range(L):

#         start_index = tf.maximum(0, i - W)
#         end_index = tf.minimum(L, i + W + 1)

#         Q_i = Q[:, :, i:i+1, :]
#         K_window = K[:, :, start_index:end_index, :]
#         V_window = V[:, :, start_index:end_index, :]

#         Alibi_window = alibi_bias[:, i:i+1, start_index:end_index]

#         # -------- Scores --------
#         Attention_Scores = tf.matmul(Q_i, K_window, transpose_b=True) * scale
#         Attention_Scores += Alibi_window

#         # -------- Apply Causal Mask --------
#         if mask_future:
#             causal_mask_i = causal_mask[i:i+1, start_index:end_index]  # (1, W_len)
#             causal_mask_i = tf.reshape(causal_mask_i, (1, 1, 1, -1))
#             Attention_Scores += causal_mask_i * NEG_INF

#         # -------- Softmax --------
#         Attention_Weights = tf.nn.softmax(Attention_Scores, axis=-1)

#         # -------- Weighted sum --------
#         Output_i = tf.matmul(Attention_Weights, V_window)

#         # Write final (B, H, D)
#         output_ta = output_ta.write(i, tf.squeeze(Output_i, axis=2))

#     # (L,B,H,D)
#     out = output_ta.stack()

#     # (B,H,L,D)
#     return tf.transpose(out, perm=[1, 2, 0, 3])




#-------------------------------------------------

def get_angles(pos, i, d_model):
    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
    return pos * angle_rates

def positional_encoding(position, d_model):
    angle_rads = get_angles(np.arange(position)[:, np.newaxis],
                            np.arange(d_model)[np.newaxis, :],
                            d_model)
    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
    pos_encoding = angle_rads[np.newaxis, ...].astype(np.float32)  # (1, position, d_model)
    return tf.constant(pos_encoding)  # shape (1, position, d_model)

def create_look_ahead_mask(size):
    # 1's for future positions (to be masked), 0's for allowed positions
    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
    return mask  # (seq_len, seq_len)

# ----------------------
# ALiBi helpers
# ----------------------
def get_alibi_slopes(n_heads: int):
    def get_slopes_power_of_2(n):
        start = 2 ** (-2 ** -(math.log2(n) - 3))
        ratio = start
        return [start * (ratio ** i) for i in range(n)]
    if math.log2(n_heads).is_integer():
        slopes = get_slopes_power_of_2(n_heads)
    else:
        m = 1 << (math.ceil(math.log2(n_heads)))
        slopes = get_slopes_power_of_2(m)
        slopes = slopes[:n_heads]
    return tf.constant(slopes, dtype=tf.float32)  # (n_heads,)

def build_alibi_bias(n_heads: int, seq_len: int):
    slopes = get_alibi_slopes(n_heads)  # (n_heads,)
    idxs = tf.range(seq_len, dtype=tf.int32)
    dist = tf.cast(tf.expand_dims(idxs, 1) - tf.expand_dims(idxs, 0), tf.float32)  # (seq_len, seq_len)
    bias = -tf.reshape(slopes, (n_heads, 1, 1)) * tf.reshape(dist, (1, seq_len, seq_len))  # (n_heads, seq_len, seq_len)
    return bias

# ----------------------
# Naive causal self-attention (TensorFlow)
# Q,K,V shapes: (batch, heads, seq_len, head_dim)
# ----------------------
def causal_self_attention_naive(Q, K, V, alibi_bias=None, mask_future=True):
    d = tf.cast(tf.shape(Q)[-1], tf.float32)  # head dimension
    scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(d)  # (B, H, L, L)

    if alibi_bias is not None:
        # alibi_bias: (H, L, L) -> expand to (1, H, L, L)
        scores += tf.expand_dims(alibi_bias, 0)

    if mask_future:
        L = tf.shape(Q)[2]
        # lower triangular (including diagonal) -> 1 for allowed
        lower = tf.linalg.band_part(tf.ones((L, L)), -1, 0)
        mask = 1.0 - lower  # 1 for future tokens
        mask = tf.reshape(mask, (1, 1, L, L))  # (1,1,L,L)
        scores = scores - 1e9 * mask

    attn = tf.nn.softmax(scores, axis=-1)  # (B,H,L,L)
    out = tf.matmul(attn, V)               # (B,H,L,head_dim)
    return out

# ----------------------
# Multi-head layer with ALiBi - returns only attention output
# ----------------------
class CausalSelfAttentionWithALiBi(tf.keras.layers.Layer):
    def __init__(self, num_heads, head_dim, use_alibi=True, **kwargs):
        super().__init__(**kwargs)
        assert head_dim > 0 and num_heads > 0
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.use_alibi = use_alibi

    def build(self, input_shape):
        d_model = int(input_shape[-1])
        self.Wq = self.add_weight(name="Wq", shape=(d_model, self.num_heads * self.head_dim),
                          initializer="glorot_uniform")
        self.Wk = self.add_weight(name="Wk", shape=(d_model, self.num_heads * self.head_dim),
                                  initializer="glorot_uniform")
        self.Wv = self.add_weight(name="Wv", shape=(d_model, self.num_heads * self.head_dim),
                                  initializer="glorot_uniform")
        self.Wo = self.add_weight(name="Wo", shape=(self.num_heads * self.head_dim, d_model),
                                  initializer="glorot_uniform")

    def call(self, x, mask_future=True):
        # x: (B, L, d_model)
        B = tf.shape(x)[0]
        L = tf.shape(x)[1]
